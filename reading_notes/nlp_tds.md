### Some examples of applying BERT in specific domain](https://towardsdatascience.com/how-to-apply-bert-in-scientific-domain-2d9db0480bd9)

 SciBERT which is based on BERT to address the performance on scientific data. It uses a pre-trained model from BERT and fine-tune  contextualized embeddings by using scientific publications which  including 18% papers from computer science domain and 82% from the broad biomedical domain.

generic pretrained NLP model may not work very well in specific domain  data. Therefore, they fine-tuned BERT to be BioBERT and 0.51% ~ 9.61%  absolute improvement in biomedical‚Äôs NER, relation extraction and  question answering NLP tasks

Both SciBERT and BioBERT also introduce domain specific data for pre-training.

![img](https://miro.medium.com/max/560/1*MhNie2aPLIid3hq_uh0qqg.png)

Both SciBERT and BioBERT follow BERT model architecture which is **multi  bidirectional transformer** and learning text representation by predicting masked token and next sentence.

 A sequence of tokens will be transform to token embeddings, segment  embeddings and position embeddings. 

**Token embeddings** refers to  contextualized word embeddings;

**segment embeddings** only include 2  embeddings which are either 0 or 1 to represent first sentence and  second sentence

**position embeddings** stores the token position relative  to the sequence. 

### [How BERT leverage attention mechanism and transformer to learn word contextual relations](https://towardsdatascience.com/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb)

a new state-of-the-art NLP paper is released by Google. They call this  approach as BERT (Bidirectional Encoder Representations from  Transformers).

 **transformer** architecture to learn the text representations. 

BERT uses **bidirectional transformer** (both  left-to-right and right-to-left direction) rather than dictional  transformer (left-to-right direction). 

BERT uses **bidirectional language model** to learn the text representations 

BERT uses deep neural network

##### Input Representation

 3 embeddings to compute the input representations

1. token embeddings;  
   * general word embeddings
   * uses vector to represent token (or word) (see [here](https://towardsdatascience.com/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a) for detail)

2. segment embeddings 
   * sentence embeddings 
   * If input includes 2 sentence,  corresponding sentence embeddings will be assigned to particular words.  
   * If input only include 1 sentence, one and only one sentence embeddings will be used. 
   * learnt before computing BERT (see [here](https://towardsdatascience.com/transforming-text-to-sentence-embeddings-layer-via-some-thoughts-b77bed60822c) for detail)
3. position embeddings
   *  token sequence of input
   * even if there are 2 sentences, position will be accumulated.

‚ÄúCLS‚Äù is the reserved token to represent the start of sequence while ‚ÄúSEP‚Äù separate segment (or sentence)

##### Pre-Training Tasks

First pre-training task is **masked language model** while the second task is **predicting next sentence**.

*Masked language model* (masked LM)

bidirectional rather than traditional directional as a  pre-training objective

If using traditional approach to train a  bidirectional model, each word will able to see ‚Äúitself‚Äù indirectly

Masked Language Model (MLM) approach instead: by masking  some tokens randomly, using other token to predicted those masked token  to learn the representations

Unlike other approaches, predicts masked token rather than entire input.

the experiment picks 15% of token randomly to be replaced.

downsides:

1.  MASK token (actual  token will be replaced by this token) will never be seen in fine-tuning  stage and actual prediction. Therefore, Devlin et al, the selected token for masking will not alway be masked but
   * A: 80% of time, it will be replaced by [MASK] token
   * B: 10% of time, it will be replaced by other actual token
   * C: 10% of time, it will be keep as original.

2. only 15% token is masked (predicted) per batch, a longer time will take for training.

*Next Sentence Prediction*

 predict next sentence

overcome the issue of first task as it cannot learn the relationship between sentences

simple objective: only classifying whether second sentence is next sentence or not

##### Training the model

2 phases training: 1) using generic data set to perform first training; 2) fine tuning by providing domain specific data set.

*Pre-training phase*

sentences retrieved from BooksCorpus (800M words) and English Wikipedia (2500M words)

step done by Google research team and we can leverage this pre-trained model to further fine tuning model based on own data.

*Fine-tuning phase*

Only some model hyperparameters are changed such as batch size, learning rate and number of training epochs, most mode hyperparameters are kept as same in pre-training phase.

Fine-tuning procedure is different and it depends on downstream tasks.

* classification: 
  * [CLS] token feed as the final hidden state
  * Label  (C) probabilities computed with a softmax
  * fine-tuned to maximize the log-probability of the correct label.
* Named Entity Recognition: 
  * Final hidden representation of token feed into the  classification layer. 
  * Surrounding words considered on the  prediction. 
  * the classification only focus on the token  itself and no Conditional Random Field (CRF).

### [Transformers from scratch](http://peterbloem.nl/blog/transformers)

##### Self-attention

The fundamental operation of any transformer architecture is the *self-attention operation*.

Self-attention is a **sequence-to-sequence operation**: a sequence of vectors goes in, and a sequence of vectors comes out. 

To produce output vector ùê≤i, the self attention operation simply takes *a **weighted average over all the input vectors***

ùê≤i=‚àëjwijùê±j.

Where j indexes over the whole sequence and the weights sum to one over all j. The weight wij is not a parameter, as in a normal neural net, but it is *derived* from a function over ùê±i and ùê±j. The simplest option for this function is the dot product:

The dot product gives us a value anywhere between negative and positive infinity, so we apply a softmax to map the values to [0,1] and to ensure that they sum to 1 over the whole sequence:

wij=exp w‚Ä≤ij‚àëjexp w‚Ä≤ij

And that‚Äôs the basic operation of self attention.

this is **the only operation in the whole architecture that propagates information *between* vectors**.

Every other operation in the transformer is applied to each vector in the input sequence without interactions between vectors.

##### Understanding why self-attention works

example: customer movie features matching movie features, dot product to get the match between them; Annotating a database of millions of movies is very costly, and  annotating users with their likes and dislikes is pretty much impossible;  instead is that we make the movie features and user features ***parameters*** of the model. We then ask users for a small number of movies that they  like and **we optimize the user features and movie features so that their  dot product matches the known likes**

Even though we don‚Äôt tell the model what any of the features should mean, in practice, it turns out that after training the features do actually reflect meaningful semantics about the movie content

This is the basic principle at work in the self-attention

 Let‚Äôs say we are faced with a sequence of words. To apply self-attention, 

* **embedding layer**: 
  * assign each word t in our vocabulary an *embedding vector* ùêØt (the values of which we‚Äôll learn)
  * turns the word sequence into the vector sequence
* **self-attention layer**
  * input is the embedding vector, the output is another sequence of vectors 
  * which element of the output vector is a weighted sum over all the embedding vectors in the first sequence, weighted by their (normalized) dot-product with the element of the embedding vector

we are *learning* what the values in the embedding vector should be, how "related" two words are is entirely determined by the task.

The **dot product** expresses how related two vectors in the input sequence  are, with ‚Äú**related**‚Äù defined by the learning task, and the output vectors are **weighted sums over the whole input sequence**, with the **weights  determined by these dot products**.

following properties, which are unusual for a sequence-to-sequence operation

* There are **no parameters** 
* Self attention sees its input as a *set*, not a sequence. If we  permute the input sequence, the output sequence will be exactly the  same, except permuted also (i.e. **self-attention is *permutation  equivariant***); self-attention by itself actually ignores the sequential nature of the input

##### basic self-attention implementation

‚Äã			

```
import torch
import torch.nn.functional as F

# assume we have some tensor x with size (b, t, k)
x = ...

# The set of all raw dot products w‚Ä≤ij forms a matrix, which we can compute simply by multiplying ùêó by its transpose: 

raw_weights = torch.bmm(x, x.transpose(1, 2))
# - torch.bmm is a batched matrix multiplication. It 
#   applies matrix multiplication over batches of 
#   matrices

#  turn the raw weights w‚Ä≤ij into positive values that sum to one, we apply a *row-wise* softmax:
weights = F.softmax(raw_weights, dim=2)

# Finally, to compute the output sequence, we just multiply the weight matrix by ùêó. This results in a batch of output matrices ùêò of size `(b, t, k)` whose rows are weighted sums over the rows of ùêó.
y = torch.bmm(weights, x)
```

##### Additional tricks

The actual self-attention used in modern transformers relies on three additional tricks.

*1) Queries, keys and values*

Every input vector ùê±i is used in 3 different ways in the self attention operation:

1. compared to every other vector to establish the weights for its own output ùê≤i (**query**)
2. compared to every other vector to establish the weights for the output  of the j-th vector ùê≤j (**key**)
3. used as part of the weighted sum to compute each output vector once the weights have been established (**value**)

basic self-attention: each input vector must play all three roles

make easier: by deriving new  vectors for each role, by applying a linear transformation to the  original input vector. In other words, we add three k√ók weight matrices ùêñq, ùêñk,ùêñv and compute three linear transformations of each xi, for the three different parts of the self attention:

This gives the self-attention layer some controllable parameters, and allows it to modify the incoming vectors to suit the three roles they  must play.

*2) Scaling the dot product*

The softmax function can be sensitive to very large input values.  These kill the gradient, and slow down learning, or cause it to stop  altogether. Since the average value of the  dot product grows with the  embedding dimension k, it helps to scale the dot product back a little to stop the inputs to the softmax function from growing too large

3) Multi-head attention

account for the fact that a word can mean different things to different neighbours

ex: mary,gave,roses,to,susan -> different relations for gave. mary expresses who‚Äôs doing the giving, roses expresses what‚Äôs being given, and susan expresses who the recipient is.

In a single self-attention operation, all this information just gets summed together. If Susan gave Mary the roses instead, the output vector ùê≤gave would be the same, even though the meaning has changed.

give the self attention greater power of discrimination, by combining several self attention mechanisms (which we'll index with r), each with different matrices ùêñrq, ùêñrk,ùêñrv. These are called **attention heads**.

For input ùê±i each attention head produces a different output vector ùê≤ri. We **concatenate these, and pass them through a linear transformation** to reduce the dimension back to k.

<u>Efficient multi-head self-attention</u>. The simplest way to understand **multi-head self-attention** is to see it as a small number of copies of the self-attention mechanism applied in parallel, each with their own key, value and query transformation. This works well, but for R heads, the self-attention operation is R times as slow.

there is a way to implement multi-head self-attention so that it is roughly as fast as the single-head version, but we still get the benefit of having different attention matrices in parallel. To accomplish this, we **cut each incoming vector into chunks**: if the input vector has 256 dimensions, and we have 8 attention heads, we cut it into 8 chunks of 32 dimensions. For each chunk, we **generate keys, values and queries of 32 dimensions each**. This means that the matrices ùêñrq, ùêñrk,ùêñrv are all 32√ó32.

##### Building transformers

A transformer is not just a self-attention layer, it is an **architecture**. It‚Äôs not quite clear what does and doesn‚Äôt qualify as a transformer, but here we‚Äôll use the following definition:

a **transformer** = any architecture designed to **process a connected set of units**‚Äîsuch as the tokens in a sequence or the pixels in an image‚Äîwhere the only **interaction between units is through self-attention**. 

standard approach for how to build self-attention layers up into a larger network. 1st step: wrap the self-attention into a block that we can repeat.

*The transformer block*

some variations on how to build a basic transformer block, but most of them are structured roughly like this:

![img](http://peterbloem.nl/files/transformers/transformer-block.svg)

the block applies, in sequence: 

* a self attention layer
* layer normalization
* a feed forward layer (a single MLP applied independently to each vector)
* another layer normalization.
* residual connections are added around both, before the normalization. 

The order of the  various components is not set in stone; the important thing is to **combine self-attention with a local feedforward, and to add  normalization and residual connections**.

Normalization and residual connections are standard tricks used to help  deep neural networks train faster and more accurately. The layer  normalization is applied over the embedding dimension only.

*Classification transformer*

The simplest transformer = a sequence classifier. 

The heart of the architecture will simply be a large chain of transformer blocks. 

All we need to do is work out how to feed it the input sequences, and how to transform the final output sequence into a a single classification.



The most common way to build a **sequence classifier** out of  sequence-to-sequence layers, is to apply **global average pooling** to the  final output sequence, and to **map the result to a softmaxed class  vector**.

![img](http://peterbloem.nl/files/transformers/classifier.svg) Overview of a simple sequence classification transformer. **The output sequence is averaged to produce a single vector representing the whole sequence. This vector is projected down to a vector with one element per class and softmaxed  to produce probabilities**.



##### Input: using the positions

embedding layer to represent the words.

stacking permutation equivariant layers, and the final global average pooling is permutation *in*variant, so the network as a whole is also permutation invariant.

if we shuffle up the words in the sentence, we get the exact same classification, whatever weights we learn

but we want our state-of-the-art language model to have at least some sensitivity to word order,

solution: create a second vector of equal length, that represents the position of  the word in the current sentence, and add this to the word embedding; 2 options

1. **position embeddings**:  embed the positions like we did the words. 
   * drawback: we have to see sequences of every length during training, otherwise the relevant position embeddings don't get trained
   * benefit: works pretty well, and easy to implement
2. **position encodings**: work in the same way as embeddings, except that we don't *learn* the position vectors, we just choose some function to map the positions to real valued vectors, and let the network figure out how to interpret these encodings
   * benefit: for a well  chosen function, the network should be able to deal with sequences that  are longer than those it's seen during training (it's unlikely to  perform well on them, but at least we can check)
   * drawbacks: the choice of encoding function is a complicated hyperparameter, and it complicates the implementation a little.

##### Text generation transformer

try an **autoregressive model**; train a ***character* level transformer to predict the next character** in a sequence.

give the sequence-to-sequence model a sequence, and ask it to  predict the next character at each point in the sequence; the target output is the same sequence shifted one character to  the left:

![img](http://peterbloem.nl/files/transformers/generator.svg)

With RNNs this is all we need to do, since they cannot look forward into the input sequence: output i depends only on inputs 0 to i. 

**With a transformer, the output depends on the entire input sequence**, so prediction of the next character becomes vacuously easy, just retrieve  it from the input.

To use **self-attention as an autoregressive model**, need to  **ensure that it cannot look forward into the sequence**. We do this by  **applying a mask to the matrix of dot products**, before the softmax is  applied. This mask **disables all elements above the diagonal of the  matrix**.



##### Design considerations

The main point of the transformer was to overcome the problems of the previous state-of-the-art architecture, the RNN 

 big weakness here is the **recurrent connection**

*  this allows information to propagate along the sequence, 
* but means that we cannot compute the cell at time step i until we‚Äôve computed the cell at timestep i‚àí1.

contrasts with 1D convolution: 

* each vector can be computed in parallel with  every other output vector; much faster
* drawback: limited in modeling *long range dependencies*. In one convolution layer,  only words that are closer together than the kernel size can interact  with each other. For longer dependence we need to stack many  convolutions.

The transformer is an attempt to capture the best of both worlds.

* can model dependencies over the whole range of the input sequence  just as easily as they can for words that are next to each other (in  fact, without the position vectors, they can‚Äôt even tell the  difference)
* no recurrent connections, so the whole  model can be computed in a very efficient feedforward fashion.

The rest of the design of the transformer is based primarily on one  consideration: depth. Most choices follow from the desire to train big  stacks of transformer blocks. 

only  2  places in the transformer where non-linearities occur: 

1. the softmax in  the self-attention 
2. the ReLU in the feedforward layer. 

The rest of the model is entirely composed of linear transformations, which perfectly preserve the gradient.

layer normalization likely also nonlinear, but this nonlinearity helps to keep the gradient stable as it  propagates back down the network.

##### Why is it called self-*attention*?

Before self-attention, sequence models consisted  mostly of recurrent networks or convolutions stacked together. 

it was discovered that these models could be helped by adding ***attention mechanisms***: instead of feeding the output sequence of the previous layer directly  to the input of the next, **an intermediate mechanism was introduced, that decided which elements of the input were relevant for a particular word of the output**.

The general mechanism was as follows.

*  call the input the **values**
* some (trainable) mechanism assigns a **key** to each value
* then to each output, some other mechanism assigns a **query**.

These names derive from the datastructure of a **key-value store**. In  that case we expect only one item in our store to have a key that  matches the query, which is returned when the query is executed.  

Attention is a softened version of this: ***every* key in the store matches the query to some extent. All are returned, and we take a sum,  weighted by the extent to which each key matches the query**.

The great breakthrough of **self-attention** was that **attention by itself is a strong enough mechanism to do all the learning**; **The key, query and value are all the same vectors (with minor linear transformations).** They ***attend to themselves*** and **stacking such self-attention provides sufficient nonlinearity and  representational power** to learn very complicated functions.

##### BERT 

BERT was one of the first models to show that transformers could reach human-level performance on a variety of language based tasks: question answering, sentiment classification or classifying whether two sentences naturally follow one another.

BERT consists of a simple **stack of transformer blocks**

pre-trained on a large general-domain corpus consisting of 800M words from English books (modern work, from unpublished authors), and 2.5B words of text from English Wikipedia articles (without markup).

**Pretraining** is done through two tasks:

1. **Masking**
   * A certain number of  words in the input sequence are: masked out, replaced with a random word or kept as is. 
   * The model is then asked to predict, for these words, what the original words were
   * **the model doesn't need to predict the entire denoised sentence, just the modified words**
   * Since the model doesn't know which words it will be asked about, it learns a representation for every word in the sequence.
2. Next sequence classification
   * 2 sequences of about 256 words are sampled that either (a) follow each other directly in the corpus, or (b) are both taken from random places. 
   * The model must then predict whether a or b is the case.

BERT uses **WordPiece tokenization**

* somewhere in **between word-level and character level sequences.** 
* breaks words like walking up into the tokens walk and ##ing -> allows the model to make some inferences based on word structure: two verbs ending in -ing have similar grammatical functions, and two verbs starting with walk- have similar semantic function.

The input is **prepended with a special <cls> token**. **The output vector corresponding to this token is used as a sentence representation in sequence classification tasks** like the next sentence classification (as opposed to the global average pooling over all vectors that we used in our classification model above).

After pretraining, a **single task-specific layer** is placed after the body of transformer blocks, which maps the general purpose representation to a task specific output. 

For classification tasks, this simply maps the first output token to softmax probabilities over the classes. 

For more complex tasks, a final sequence-to-sequence layer is designed specifically for the task.

The whole model is then re-trained to finetune the model for the specific task at hand.

In an ablation experiment, the authors show that **the largest improvement as compared to previous models comes from the bidirectional nature of BERT**. 

* previous models like GPT used an **autoregressive mask**, which allowed attention only over previous tokens. 
* in BERT **all attention is over the whole sequence** is the main cause of the improved performance. This is why the B in BERT stands for "bidirectional".

The largest BERT model uses 24 transformer blocks, an embedding dimension of 1024 and 16 attention heads, resulting in 340M parameters.

##### Sparse transformers

 tackle the problem of quadratic memory use  head-on. I**nstead of computing a dense matrix of attention weights (which grows quadratically), they compute the self-attention only for  particular pairs of input tokens**, resulting in a ***sparse* attention matrix**, with only n*‚àön explicit elements.

* benefit: 
  * allows models with very large context sizes, for instance for  generative modeling over images, with large dependencies between pixels; allow to train transformers with very large  sequence lengths
  *  also allows a very elegant way  of designing an inductive bias. We take our input as a collection of  units (words, characters, pixels in an image, nodes in a graph) and we  specify, through the sparsity of the attention matrix, which units we  believe to be related. The rest is just a matter of building the  transformer up as deep as it will go and seeing if it trains.
* tradeoff: sparsity structure is not learned, so by the  choice of sparse matrix, we are disabling some interactions between  input tokens that might otherwise have been useful. However, two units  that are not directly related may still interact in higher layers of the transformer (similar to the way a convolutional net builds up a larger  receptive field with more



### [Fleuret NLP lectures](https://fleuret.org/dlc/)

A common word embedding is the Continuous Bag of Words (CBOW) version
of **word2vec** (Mikolov et al., 2013a).
In this model, the embedding vectors are chosen so that a word can be
[linearly] predicted from the sum of the embeddings of words around it.

An alternative algorithm is the **skip-gram model**, which optimizes the
embedding so that a word can be predicted by any individual word in its context
(Mikolov et al., 2013a).

The main benefit of word embeddings is that they are trained with unsupervised
corpora, hence possibly extremely large.

in all the operations we have seen such as fully connected layers, convolutions,
or poolings, **the contribution of a value in the input tensor to a value in the**
**output tensor is entirely driven by their [relative] locations [in the tensor].**

**Attention mechanisms** aggregate features with an importance score that

* depends on the feature themselves, not on their positions in the tensor,
* relax locality constraints

Attention mechanisms **modulate dynamically the weighting of different parts of**
**a signal and allow the representation and allocation of information channels to**
**be dependent on the activations themselves**.

While they were developed to equip deep-learning models with memory-like
modules (Graves et al., 2014), their main use now is to **provide long-term**
**dependency for sequence-to-sequence translation** (Vaswani et al., 2017).



##### attention mechanisms

the simplest form of attention is **content-based attention**.

which differs from **context attention**, 

the most classical version of attention is a **context-attention with a dot-product**
**for attention function**

 using the terminology of Graves et al. (2014), attention is an **averaging of**
**values associated to keys matching a query**. Hence the keys used for
computing attention and the values to average are different quantities

attention layer to equip the model with the **ability to combine**
**information from parts of the signal that it actively identifies as relevant**.



##### Transformer networks

Vaswani et al. (2017) proposed to go one step further: instead of using
attention mechanisms as a supplement to standard convolutional and recurrent
operations, they designed **a model combining only attention layers**.

They designed this ‚Äú**transformer**‚Äù for a sequence-to-sequence translation task,
but it is currently key to state-of-the-art approaches across NLP tasks

they first introduce a multi-head attention module

Their complete model is composed of:

* An **encoder** that combines N = 6 modules each composed of a **multi-head**
  **attention sub-module**, and a **[per-component] one hidden-layer MLP**, with
  residual pass-through and layer normalization.
* A **decoder** with a similar structure, but with **causal attention layers** to allow
  for regression training, and **additional attention layers** that attend to the
  layers of the encoder.

Positional information is provided through an **additive positional encoding**

The **Universal Transformer** (Dehghani et al., 2018) is a similar model where **all**
**the blocks are identical**, resulting in a recurrent model that iterates over
consecutive revisions of the representation instead of positions.
Additionally the **number of steps is modulated per position dynamically**.

transformer networks were introduced for translation, and trained with a
supervised procedure, from pairs of sentences

 they can be trained in a unsupervised
manner, for auto-regression or as denoising auto-encoders, from very large
data-sets, and fine-tuned on supervised tasks with small data-sets

BERT (Bidirectional Encoder Representation from Transformers) is a transformer pre-trained with:

* **Masked Language Model** (MLM), that consists in predicting [15% of]
  words which have been replaced with a ‚ÄúMASK‚Äù token.
* **Next Sentence Prediction** (NSP), which consists in predicting if a certain
  sentence follows the current one.

It is then fine-tuned on multiple NLP tasks.

##### Attention in computer vision

They insert ‚Äúnon-local blocks‚Äù in residual architectures and get improvements
on both video and images classification.

### [BERT Explained: A Complete Guide with Theory and Tutorial](https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/)

**pre-training** = raining general purpose language representation models using the  enormous piles of unannotated text on the web 

These general purpose pre-trained models can then be ***fine-tuned\*** on smaller task-specific datasets, e.g., when working with problems like question answering and sentiment analysis.

BERT is a recent addition to these techniques for NLP pre-training

we can either use the BERT models to extract high quality language  features from our text data, or we can fine-tune these models on a  specific task

In the pre-BERT world, **one-directional approach** works well for generating sentences ‚Äî we can  predict the next word, append that to the sequence, then predict the  next to next word until we have a complete sentence.

 BERT, a language model which is **bidirectionally trained** (this is also its key technical innovation). This means we can now have a  deeper sense of language context and flow compared to the  single-direction language models.

Instead of predicting the next word in a sequence, BERT makes use of a novel technique called **Masked LM** (MLM): it randomly masks words in the sentence and then it tries to predict them. 

**Masking**  means that the model looks in both directions and it uses the full  context of the sentence, both left and right surroundings, in order to  predict the masked word. 

Unlike the previous language models, it takes  **both the previous and next tokens into account** at the **same time.** 

The existing combined left-to-right and right-to-left LSTM based models  were missing this ‚Äúsame-time part‚Äù. (It might be more accurate to say  that BERT is non-directional though.)

Pre-trained language representations can either be

* **context-free** (e.g. word2vec)
  * generate a single word embedding representation (a vector of numbers) for each word in the vocabulary.
  * e.g.  ‚Äúbank‚Äù would have the same context-free representation in ‚Äúbank account‚Äù and ‚Äúbank of the river"
* **context-based**
  * generate a representation of each word that is based on the other words in the sentence
  * **unidirectional** 
    * e.g.  in  ‚ÄúI accessed the bank account,‚Äù represent ‚Äúbank‚Äù based on ‚ÄúI accessed the‚Äù 
  * *bidirectional ** (e.g. BERT)
    * e.g. represents ‚Äúbank‚Äù using both its previous and next context ‚Äî ‚ÄúI accessed the ‚Ä¶ account‚Äù

Moreover, BERT is based on the **[Transformer model architecture](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html),** instead of LSTMs. 

A **Transformer** works by performing a small, constant number of steps. 

* In  each step, it applies an **attention mechanism** to understand relationships between all words in a sentence, regardless of their respective  position. 
* For example, given the sentence, ‚ÄúI arrived at the bank after crossing the river‚Äù, to determine that the word ‚Äúbank‚Äù refers to the  shore of a river and not a financial institution, the Transformer can  **learn to immediately pay attention to the word ‚Äúriver‚Äù and make this  decision in just one step.**

BERT relies on a **Transformer** (the **attention mechanism that learns contextual relationships between words** in a text). A basic Transformer consists of an **encoder** to read the text input and a **decoder** to produce a prediction for the task. 

Since **BERT‚Äôs goal is to generate a language  representation model**, it only needs the **encoder** part. 

The **input** to the  encoder for BERT is a **sequence of tokens**, which are first converted into **vectors** and then processed in the neural network. But before processing can start, BERT needs the input to be massaged and decorated with some  extra metadata:

1.  **Token embeddings**:
   * A [CLS] token is added to the input word tokens **at the beginning of the first sentence** 
   * a [SEP] token is inserted at the end of each  sentence.
2. **Segment embeddings**: 
   * A marker **indicating Sentence A or Sentence B** is added to each token. This allows the encoder to distinguish between sentences.
3. **Positional embeddings**: 
   * A positional embedding is added to each token to **indicate its position in the sentence**.

Essentially, t**he Transformer stacks a layer that maps sequences to sequences**, so the output is also a sequence of vectors with a 1:1  correspondence between input and output tokens at the same index. 

BERT does not try to predict the next word in the  sentence. 

Training makes use of the following two strategies:

**1. Masked LM (MLM)**

Procedure:

* Randomly mask out 15% of the words in the  input ‚Äî replacing them with a [MASK] token 
* run the entire sequence  through the BERT attention based encoder and then predict only the  masked words, based on the context provided by the other non-masked  words in the sequence. 

problem with this naive  masking approach: the model only tries to predict when the [MASK]  token is present in the input, while we want the model to try to predict the correct tokens regardless of what token is present in the input. To deal with this issue, out of the 15% of the tokens selected for  masking:

- 80% of the tokens actually replaced with the token [MASK].
- 10% replaced with a random token.
- 10% left unchanged.

While training the BERT loss function **considers only the prediction of the  masked tokens** and ignores the prediction of the non-masked ones. This  results in a model that **converges much more slowly** than left-to-right or right-to-left models.

2. **Next Sentence Prediction (NSP)**

In order to **understand relationship between two sentences**, BERT training process also uses next sentence prediction. 

A pre-trained model with this kind of understanding is **relevant for tasks like question answering**. 

During training the model gets as i**nput pairs of sentences and it learns to predict if the second sentence is the next sentence in the original text** as well.

As we have seen earlier, BERT separates sentences with a special [SEP] token. During training the model is fed with two input sentences at a time such that:

* 50% of the time the second sentence comes after the first one.
* 50% of the time it is a a random sentence from the full corpus.

BERT is then required to **predict whether the second sentence is random or not**, with the assumption that the random sentence will be disconnected from the first sentence (predict label "IsNext" or "NotNext")

To predict if the second sentence is connected to the first one or not,  basically the complete input sequence goes through the Transformer based model, the output of the [CLS] token is transformed into a 2√ó1 shaped  vector using a simple classification layer, and the IsNext-Label is  assigned using softmax.

The model is **trained with both Masked LM and Next Sentence Prediction  together**. This is to **minimize the combined loss function** of the two  strategies ‚Äî *‚Äútogether is better‚Äù*.

##### Architecture

There are **four types of pre-trained versions of BERT** depending on the scale of the model architecture:

**`BERT-Base`**: 12-layer, 768-hidden-nodes, 12-attention-heads, 110M parameters
 **`BERT-Large`**: 24-layer, 1024-hidden-nodes, 16-attention-heads, 340M parameters


if we want to fine-tune the original model based on our own dataset, we  can do so by just adding a single layer on top of the core model.

For example, say we are creating **a question answering application**. In essence question answering is just a **prediction task** ‚Äî on receiving a question as input, the goal of the application is to identify the  right answer from some corpus. 

So, given a question and a context  paragraph, **the model predicts a start and an end token from the  paragraph that most likely answers the question** -> BERT can be trained by learning **two extra  vectors that mark the beginning and the end of the answer**.

Just like sentence pair tasks, **the question becomes the first  sentence and paragraph the second sentence in the input sequence**.  However, this time there are two new parameters learned during  fine-tuning: a **start vector** and an **end vector.**

in case we want to do fine-tuning, we need to **transform our input into  the specific format** that was used for pre-training the core BERT models, e.g., we would need to add special tokens to mark the beginning ([CLS]) and separation/end of sentences ([SEP]) and segment IDs used to  distinguish different sentences ‚Äî convert the data into features that  BERT uses.

, we can also do custom **fine tuning** by **creating a single new layer** **trained to adapt BERT** to our sentiment task (or any other task). 

we need to preprocess our data so that it matches the data  BERT was trained on. For this, we'll need to do a couple of things (but  don't worry--this is also included in the Python library):

* Lowercase our text (if we're using a BERT lowercase model)

* Tokenize it (i.e. "sally says hi" -> ["sally", "says", "hi"]) 
* Break words into WordPieces (i.e. "calling" -> ["call", "##ing"]) 
* Map our words to indexes using a vocab file that BERT provides 
* Add special "CLS" and "SEP" tokens (see the [readme](https://github.com/google-research/bert)) 
* Append "index" and "segment" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf)) 

First, it loads the BERT tf hub module again (this time to extract the computation graph). Next, it creates a single new layer that will be trained to adapt BERT to our sentiment task (i.e. classifying whether a movie review is positive or negative). This strategy of using a mostly trained model is called fine-tuning.

### [BERT : Le "Transformer model" qui s‚Äôentra√Æne et qui repr√©sente](https://lesdieuxducode.com/blog/2019/4/bert--le-transformer-model-qui-sentraine-et-qui-represente)

**BERT** c'est pour **B**idirectional **E**ncoder **R**epresentations from **T**ransformers. Il est sorti des labos [Google AI](https://ai.google/) fin 2018, et s'il est ce jour l'objet de notre attention c'est que son mod√®le est √† la fois :

- Plus performant que ses pr√©d√©cesseurs en terme de [r√©sultats](https://rajpurkar.github.io/SQuAD-explorer/).
- Plus performant que ses pr√©d√©cesseurs en terme de rapidit√© d'apprentissage.
- Une fois **pr√©-entra√Æn√©, de fa√ßon non supervis√©e** (initialement avec avec tout - absolument tout - le corpus anglophone de Wikipedia),  il poss√®de une "repr√©sentation" linguistique qui lui est propre. Il est  ensuite possible, sur la base de cette repr√©sentation initiale, de le  customiser pour une t√¢che particuli√®re. Il peut √™tre **entra√Æn√© en mode incr√©mental (de fa√ßon supervis√©e cette fois)** pour sp√©cialiser le mod√®le rapidement et avec peu de donn√©es.
- Enfin il peut fonctionner de fa√ßon **multi-mod√®le**, en prenant en entr√©e des  donn√©es de diff√©rents types comme des images ou/et du texte, moyennant  quelques manipulations.

Il a l'avantage par rapport √† ses concurrents Open AI GTP (GPT-2 est ici pour ceux que √ßa int√©resse) et ELMo d'√™tre **bidirectionnel**, il n'est pas oblig√© de ne regarder qu'en arri√®re comme OpenAI GPT ou de concat√©ner la vue "arri√®re" et la vue "avant" entra√Æn√©es ind√©pendamment comme pour ELMo.

Pour faire du "sequence to sequence", i.e. de la traduction simultan√©e, ou du text-to-speech, ou encore du speech-to-text, l'√©tat de l'art jusque ces derni√®res ann√©es, c'√©tait les **RNNs**, nourris avec des s√©quences de word-embeddings , et parfois quelques couches de convolution sur les s√©quences d'entr√©e pour en extraire des caract√©ristiques (features) plus ou moins fines (fonction du nombre de couches), afin d'acc√©l√©rer les calculs, avant de passer les infos au RNN.

Notez que **BERT utilise des embeddings sur des morceaux de mots**. Donc ni des embeddings niveau caract√®re, ni des embeddings au niveau de chaque mot, mais un interm√©diaire.


Contrairement aux r√©seaux de neurones "classiques" (FFN pour feed-forward neural networks), qui connectent des couches de neurones formels les unes √† la suite des autres, avec pour chaque couche sa propre matrice de poids "entra√Ænable" - c'est √† dire dont on peut modifier les poids petit √† petit lors de l'apprentissage - et qui prennent en entr√©e des fourn√©es (batchs) de donn√©es, **les RNNs traitent des s√©quences, qu'ils parcourent pas √† pas, avec une m√™me matrice de poids**. Pour cette raison (le pas-√†-pas), dans le cadre des RNNs, **on ne peut pas parall√©liser les calculs.**

Les s√©quences sont des objets dont chaque √©l√©ment poss√®de un ordre, une position, une inscription dans le temps. Par exemple dans une phrase, chaque mot vient dans un certain ordre, est prononc√© sur un intervalle de temps distinct de celui des autres.


Nous allons les repr√©senter de fa√ßon d√©roul√©e pour des raisons de lisibilit√©, mais en r√©alit√© il s'agit d'**it√©rer dans l'ordre sur chaque √©l√©ment d'une s√©quence**. Il n'y a qu'**une seule matrice de poids, ou "les poids sont partag√©s dans le temps"** si vous pr√©f√©rez. Ci-dessous la repr√©sentation r√©elle et la repr√©sentation d√©roul√©e.

Une seule matrice de poids aide √† traiter de **s√©quences de longueurs  diff√©rentes** 

2 s√©quences diff√©rentes peuvent avoir un  sens tr√®s similaire; dans ce cas une seule  matrice de poids permet de partager les m√™me param√®tres √† chaque √©tape  de traitement, et par cons√©quence de donner un r√©sultat global assez  similaire en sortie pour deux phrase ayant le m√™me sens, bien qu'ayant  une composition diff√©rente.

Les FFNs sont soumis √†  l'explosion ou √† l'√©vanouissement du gradient (**exploding/vanishing  gradient descent**), et ce ph√©nom√®ne est d'autant plus prononc√© qu'on  traite de longue s√©quences ou qu'on les approfondis. 

les RNNs y sont soumis quand on augmente le nombre d'√©tapes de  traitement, mais de fa√ßon aggrav√©e par rapport aux FFNs. En effet il  peut y avoir des dizaines/centaines de mots dans une phrase, et **rarement autant de couches dans un FFN**. De plus, **dans un FFN, chaque couche  poss√®de sa propre matrice de poids et ses propres fonctions  d'activation. Les matrices peuvent parfois se contre-balancer les unes  les autres et "compenser"** ce ph√©nom√®ne. **Dans un RNN avec une seule  matrice les probl√®mes de gradient sont plus prononc√©s.**

*Note* : Il peut y avoir plusieurs couches dans un RNN, auquel cas chaque  couche aura sa propre matrice de poids. Entendez par l√† qu'en sus du  nombre d'√©tapes de traitement, un RNN peut aussi √™tre un r√©seau  "profond". On peut empiler plusieurs couches de RNN connect√©es entre  elles.

Afin de conserver la m√©moire du contexte, et  d'att√©nuer les probl√®mes de descente de gradient, les RNNs sont  g√©n√©ralement compos√©s d'unit√©s un peu particuli√®res, les **LSTMs** (long  short term memory), ou d'une de leur variantes les **GRUs** (gated recurrent units). Il existe en sus d'autres techniques pour les soucis de  gradient dans les RNNs - gradient clipping quand √ßa explose, sauts de  connexion, i.e. additive/concatenative skip/residual connexion quand √ßa  "vanishe" etc... 

**les LSTMs sont construites pour m√©moriser une partie des autres √©l√©ments d'une s√©quence**, et sont donc bien adapt√©es √† des t√¢ches traitant d'objets **dont les  √©l√©ments ont des d√©pendances entre eux** √† plus ou moins long terme, comme la traduction simultan√©e, qui ne peut pas se faire mot √† mot, sans le  contexte de ce qui a √©t√© prononc√© avant.

 pour faire du "sequence to sequence", la topologie utilis√©e  classiquement a un encodeur, suivi d'un d√©codeur.

* La sortie de l'encodeur est une suite de chiffres qui repr√©sente le  sens de la phrase d'entr√©e
* Pass√©e au d√©codeur, celui-ci va g√©n√©rer un  mot apr√®s l'autre, jusqu'√† tomber sur un √©l√©ment qui signifie "fin de la phrase".

vecteur qui contient tout le sens de la phrase en sortie de  l'encodeur. Il est souvent appel√© "**context vector**", vecteur contexte.



##### Les m√©canismes d'attention

Les m√©canismes  d'attention = les moyens de **faire passer au d√©codeur l'information de quelles √©tapes de l'encodeur (i.e. quels mots de la s√©quence d'entr√©e)  sont les plus importantes au moment de g√©n√©rer un mot de sortie**. Quels  sont les mots de la s√©quence d'entr√©e qui se rapportent le plus au mot  qu'il est en train de g√©n√©rer en sortie, soit qu'ils s'y rapportent  comme contexte pour lui donner un sens, soit qu'ils s'y rapportent comme mots "cousins" de signification proche.

**Les m√©canismes d'auto-attention** (self-attention) sont similaires, sauf qu'**au lieu de s'op√©rer entre les √©l√©ments de l'encodeur et du d√©codeur**, ils **s'op√®rent sur les √©l√©ments de l'input entre eux** (le pr√©sent regarde le pass√© et le futur) **et de l'output entre eux** aussi (le pr√©sent regarde le pass√©, vu que le futur est encore √† g√©n√©rer).

Par exemple au moment de g√©n√©rer "feel", c'est en fait l'ensemble "avoir la p√™che" qui a du sens, il doit faire attention √† tous les mots, et idem  quand il g√©n√®re "great", car il s'agit de la traduction d'une expression idiomatique.

**convolution**: souvent int√©gr√©e aux RNNs; r√©pond - en partie - √† un objectif similaire, fournit un "contexte" √† une suite de mots. On peut en effet passer la s√©quence d'entr√©e dans une ou plusieurs couches de convolution, avant de la passer dans l'encodeur. 

Les produits de convolution vont alors extraire des caract√©ristiques contextuelles entre mots se trouvant √† proximit√© les uns des autres, exacerber le poids de certains mots, et att√©nuer le poids de certains autres mots, et ceci de fa√ßon tr√®s positionnelle. 

**La convolution permet couche apr√®s couche d'extraire des caract√©ristiques (features) spatiales (dans diverses zones de la phrase), de plus en plus fines au fur et √† mesure que l'on plonge plus profond dans le r√©seau**.

Par analogie avec la convolution sur les images, pour appliquer la convolution √† une phrase on peut par exemple √† mettre un mot par ligne, chaque ligne √©tant le vecteur d'embeddings du mot correspondant. Chaque case du tableau de chiffres que cela produit √©tant alors l'√©quivalent de l'intensit√© lumineuse d'un pixel pour une image.

Les CNN (convolutional neural networks) ont aussi l'avantage que les calculs **peuvent se faire en parall√®le** (O(n/k) vs. O(n) pour un RNN), qu'on peut les utiliser pour concentrer l'information, et par cons√©quent **diminuer les probl√®mes de d'explosion/√©vanouissement du gradient**, utile s'il s'agit, par exemple, de g√©n√©rer une texte de plus de 1000 mots. J(e.g. bons r√©sultats avec WaveNet et ByteNet)

Ce qui sortira du CNN sera une info du type : "les mots 8, 11 et 23 sont tr√®s importants pour donner le sens exact de cette phrase, de plus il faudra combiner ou corr√©ler 8 et 11, retiens √ßa au moment de d√©coder la phrase". √Ä chaque √©tape on d√©cide de conserver tel ou tel mot (concentrant ainsi l'information) qui est cens√© avoir de l'importance au moment de g√©n√©rer le mot suivant. Ceci-dit, **c'est tr√®s positionnel**. √áa d√©pend beaucoup de la position des mots dans la phrase et de leur position les uns par rapport aux autres pour cr√©er un contexte, plus que de leur similarit√© de contexte s√©mantique.

! diff√©rence fondamentale avec **l'attention, qui ne va pas regarder dans quelle position se trouvent les mots, mais plut√¥t quels sont les mots les plus "similaires" entre eux**. Comme la notion de "position" d'un √©l√©ment dans une s√©quence reste tr√®s importante, les m√©canismes d'attention nous obligeront √† passer cette notion par d'autres moyens, mais ils ont **l'avantage de pouvoir faire des liens entre des √©l√©ments tr√®s distants d'une s√©quence, de fa√ßon plus l√©g√®re qu'avec un produit de convolution,** et d'une fa√ßon plus naturelle (sans se baser sur la position mais plut√¥t **en comparant les similarit√©s entre les √©l√©ments**).


Les RNNs avec LSTMs ont en r√©alit√© d√©j√† leurs m√©canismes d'attention, il existe plusieurs fa√ßons de faire; 3 grandes familles 

1. **attention stricte** (hard attention), qui **focalise sur un seul √©l√©ment** du contexte  et qui est **stochastique** (pas de notion de d√©riv√©e, la s√©lection de  l'√©l√©ment "√©lu" et la r√©tro-propagation se font via des m√©thodes  statistiques, √©chantillonnages, distributions)

- **attention  locale** qui ne s√©lectionne que **quelques √©l√©ments proches** sur lesquels  porter l'attention; hybride entre la soft et la hard attention.
- **attention "molle"** (soft attention), la plus classique



**R√©solution de co-r√©f√©rences - Sch√©mas de Winograd**

li√© aux m√©canismes d'attention; aucun autre mod√®le que BERT ne donne de bonnes performances sur ce point

La **co-r√©f√©rence** c'est lorsque √©l√©ment en r√©f√©rence un autre mais de  fa√ßon suffisamment ambigu√´ pour qu'il faille une compr√©hension fine de  la phrase pour comprendre ce qu'il r√©f√©rence.

*Exemple* : Je ne peux pas garer ma voiture sur cette place parce-qu‚Äôelle est  trop petite. <- Ici le pronom personnel "elle" renvoie √† la place de  parking, pas √† la voiture. Il faut une compr√©hension assez fine de la  phrase pour le comprendre. BERT y arrive tr√®s bien.

avec l'**attention Multi-t√™tes** chaque t√™te "voit" des choses que les autres ne voient pas et  ce "coll√®ge" se compl√®te bien.



##### Architecture

BERT n'utilise qu'une partie de l'architecture Transformer. Comme son nom l'indique (Bidirectional Encoder Representations from Transformers) **Bert n'est compos√© que d'un empilement de blocs type "Encodeur" sans "D√©codeur"**. Il y a aussi des mod√®les comme GPT-2 compos√©s de couches "D√©codeur"  seulement et plus sp√©cialis√©s pour de la g√©n√©ration de texte. 

architecture pr√©sent√©e dans l'article original

- 6 **encodeurs** empil√©s (le Nx du sch√©ma), 
  - chaque encodeur prenant en entr√©e la sortie de l'encodeur pr√©c√©dent (sauf le premier qui prend  en entr√©e les embeddings),
- suivi de 6 **d√©codeurs** empil√©s
  - prenant en  entr√©e la sortie du d√©codeur pr√©c√©dent et la sortie du dernier encodeur  (sauf pour le premier d√©codeur qui ne prend en entr√©e que la sortie du  dernier d√©codeur

Les 12 blocs (ou 24 selon les versions de BERT) **ne partagent pas les m√™mes matrices de poids**.

Chaque **encodeur** 

* se compose de 2 sous-couches : 

1. une couche  d'**auto-attention "multi-t√™tes"** 
2. suivie d'un **FFN compl√®tement connect√© et position-wise** (i.e. chaque √©l√©ment du vecteur de sortie de la couche  pr√©c√©dente est connect√© √† un neurone formel de l'entr√©e du FFN, dans le  m√™me ordre qu'ils le sont dans le vecteur). 

* Chaque sous-couche poss√®de  en sortie **une couche qui ajoute, additionne, les sorties de la couche et du raccord √† une connexion dite r√©siduelle** (qui connecte directement  les valeurs d'entr√©e de la couche √† la sortie de la couche) et qui  **normalise** l'ensemble.

Chaque **d√©codeur** 

* se compose de 3 couches : 
  1. une couche d'**auto-attention "multi-t√™tes",** 
  2. suivie d'une couche d'**attention avec le dernier encodeur**, 
  3. puis un **FFN compl√®tement  connect√© et position-wise** (i.e. chaque √©l√©ment du vecteur de sortie de  la couche pr√©c√©dente est connect√© √† un neurone formel de l'entr√©e du  FFN, dans le m√™me ordre qu'ils le sont dans le vecteur). 
* Chaque  sous-couche poss√®de **en sortie une couche qui ajoute, additionne, les  sorties de la couche et du raccord √† une connexion dite r√©siduelle** (qui  connecte directement les valeurs d'entr√©e de la couche √† la sortie de la couche) et qui **normalise** l'ensemble.



**3 m√©canismes  d'attention type "cl√©-valeur"** 

1. **Auto-attention** (dans l'encodeur)
2. **Auto-attention avec les tous  √©l√©ments pr√©c√©demment g√©n√©r√©e** (en entr√©e du d√©codeur)
3. **Attention "masqu√©e"** (dans le d√©codeur) (masked attention, parce-qu‚Äôon applique un masque) nous verrons les d√©tails plus loin dans l'article) entre  **l'√©l√©ment √† g√©n√©rer dans le d√©codeur et tous les √©l√©ments de l'encodeur**. 

Les couches d'attention ont plusieurs t√™tes

Pour la g√©n√©ration de texte ou la traduction simultan√©e le **m√©canisme est auto-r√©gressif**, c'est √† dire qu'on fait entrer une s√©quence dans le premier encodeur,  la sortie pr√©dit un √©l√©ment, puis on repasse toute la s√©quence dans  l'encodeur et l'√©l√©ment pr√©dit dans le d√©codeur en parall√®le afin de  g√©n√©r√©e un deuxi√®me √©l√©ment, puis √† nouveau la s√©quence dans l'encodeur  et tous les √©l√©ments d√©j√† pr√©dits dans le d√©codeur en parall√®le etc...  jusqu'√† pr√©dire en sortie un <fin de s√©quence>.

En sortie une distribution de probabilit√© qui permet de pr√©dire l'√©l√©ment de sortie le plus probable.

Et le tout **se passe compl√®tement de LSTMs !** 



##### Comment BERT apprend

de fa√ßon non supervis√©e, l'entr√©e se suffit √† elle m√™me, pas besoin de labelliser

**Masked language model**

[CLS] indique un d√©but de s√©quence

[SEP] une s√©paration, en g√©n√©ral entre deux phrases dans notre cas.

[MASK] un mot masqu√©

<u>Les mots "masqu√©s"</u>


Ici la s√©quence d'entr√©e a √©t√© volontairement oblit√©r√©e d'un mot, le mot masqu√©, et **le mod√®le va apprendre √† pr√©dire ce mot masqu√©.**

<u>La phrase suivante</u>


Ici le mod√®le doit **d√©terminer si la s√©quence suivante (suivant la s√©paration[SEP]) est bien la s√©quence suivante**. Si oui, IsNext sera vrai, le label sera IsNext, si non, le label sera NotNext.



##### 	Les customisations possibles

une fois son  apprentissage non supervis√© termin√©, il est :

* capable de se sp√©cialiser sur beaucoup de t√¢ches diff√©rentes  (traduction, r√©ponse √† des questions etc.)

*  surclasse dans la plus part de ses sp√©cialisations les mod√®les sp√©cialis√©s existants



##### 	Les sous-parties du Transformer en d√©tails

exemple pour la g√©n√©ration de texte

<u>en entr√©e</u>

* embeddings : chaque **mot** est repr√©sent√© par un vecteur (colonne ou ligne de r√©els)
* On ajoute √©ventuellement √† ces embeddings, pour chaque mot, les embeddings d'un "**segment**" quand cela a du sens (par exemple chaque phrase est un  segment et on veut passer plusieurs phrases √† la fois, on va alors dire  dans quel segment se trouve chaque mot).
* On ajoute  ensuite le "**positional encoding**", qui est une fa√ßon d'encoder la place  de chaque √©l√©ment **dans la s√©quence**. 
  * Comme la longueur des phrases n'est  pas pr√©d√©termin√©e, on va utiliser des fonctions sinuso√Ødales donnant de  petites valeurs entre 0 et 1, pour modifier l√©g√®rement les embeddings de chaque mot. 
  * La dimension de l'embedding de position (√† sommer avec  l'embedding s√©mantique du mot) est la m√™me que celle de l'embedding  s√©mantique, soit 512, pour pouvoir sommer terme √† terme.
  * il existe beaucoup de fa√ßons d'encoder la position d'un √©l√©ment dans une s√©quence.



Ce qui donne en entr√©e une matrice de taille [longueur de la s√©quence] x [dimension des embeddings - 512] 



##### 		L'attention dans le cas sp√©cifique du Transformer



m√©canisme d'attention (auto-attention ou pas) de **type cl√©-valeur**.



Chaque mot, d√©crit comme la **somme de ses embeddings s√©mantiques et positionnels** va √™tre d√©compos√© en **trois abstractions** :

1. Q = Une **requ√™te** (query)
2. K = Une **cl√©** (key)
3. V = Une **valeur** (value)

Dans l'exemple, chacune de ces abstractions est ici un vecteur de dimension 64. Comme on veut des sorties de dimension [longueur de la s√©quence] x [dimension des embeddings - 512] tout au long du  parcours, et que dans l'attention on fera du multi-t√™tes (voir plus  loin) avec 8 t√™tes, qu'on concat√©nera la sortie de chaque t√™te, on aura  alors 8 x 64 = 512 en sortie de l'attention et c'est bien ce qu'on veut.

**Chacune de ces abstractions est calcul√©e et apprise** (mise √† jour des matrices  de poids) lors du processus d'apprentissage, gr√¢ce √† une matrice de  poids. Chaque matrice est distincte.

Les dimensions de ces matrices sont [64 (dimension requ√™te ou cl√© ou valeur)] x [longueur de la s√©quence].



**Multi-t√™tes** :

*  **Chaque t√™te de l'attention a ses propres matrices de poids** 
* on **concat√®ne la sortie de chaque t√™te** pour retrouver une matrice de  dimension [longueur de la s√©quence] x [dimension des embeddings, i.e.  512].

explication de la formule de l'attention

* [Q x Transpos√©e de K] est un produit scalaire entre les vecteurs  requ√™te et les vecteurs cl√©.
  * plus la cl√©  "ressemblera" √† la requ√™te, plus le score produit par [Q x Transpos√©e de K] sera grand pour cette cl√©.
* La partie *dk (=64)* pour normaliser, pas toujours utilis√© dans les m√©canismes d'attention.
* softmax donne une distribution de probabilit√©s qui va encore  augmenter la valeur pour les cl√©s similaires aux requ√™tes, et diminuer  celles des cl√©s dissemblables aux requ√™tes.
* les cl√©s  correspondent √† des valeurs, quand on multiplie le r√©sultat pr√©c√©dent  par V, **les valeurs correspondant aux cl√©s qui ont √©t√© "√©lues" √† l'√©tape  pr√©c√©dente sont sur-pond√©r√©es par rapport aux autres valeurs.**

Enfin on concat√®ne la sortie de chaque t√™te, et on multiplie par une matrice W0, de dimensions 512 x 512 ([(nombre de t√™tes) x (dimension requ√™te ou cl√© ou valeur, i.e. 64)]x[dimension des embeddings]), qui apprend √† projeter le r√©sultat sur un espace de sortie aux dimensions attendues.

##### 		La connexion r√©siduelle

**ajouter la repr√©sentation initiale** √† celle calcul√©e dans les couches d'attention ou dans le FFN.

-> *Cela revient √† dire* : Apprend les relations entre les √©l√©ments de la s√©quence, mais n'oublie pas ce que tu sais d√©j√† √† propos de toi.

appliquer un dropout de 10%, donc en sortie de chaque couche i



##### Le FFN

Couches de neurones formels avec une ReLU comme fonction d'activation

Ici W1 a pour dimensions [dimension des embeddings]x[dimmension d'entr√©e du FFN - au choix] et W2 [dimmension d'entr√©e du FFN - au choix] x  [dimension des embeddings

C'est le r√©seau de neurones "standard" 



##### *Rappelons le principe d'auto-r√©gression du Transformer*.

Par exemple pour traduire "Avoir la p√®che", nous avons vu que :

* 1er passage, on envoie 
  * dans l'encodeur une version  "embedd√©e" de la s√©quence [<CLS> Avoir la p√®che <fin de  phrase>], 
  * en m√™me temps l'amorce de s√©quence [<CLS>] dans le d√©codeur.
  * Le Transformer doit alors pr√©dire le 1er  mot de la traduction : "feel" (i.e. nous sortir la s√©quence [<CLS> feel])
* 2√®me passage, on envoie 
  * dans  l'encodeur de nouveau toute la version "embedd√©e" de la s√©quence [<CLS> Avoir  la p√®che <fin de phrase>],
  * une version "embedd√©e" de ce que le  Transformer a pr√©dit dans le d√©codeur.

*  etc... jusqu'√† ce que la s√©quence pr√©dite en sortie du Transformer finisse par <fin de phrase>.



##### L'auto-attention "masqu√©e" :

‚Äã	*- Pourquoi :*

Dans notre cas, la phrase a pr√©dire en sortie est d√©j√† connue lorsque nous entra√Ænons le mod√®le. Le jeu  d'entra√Ænement poss√®de d√©j√† la correspondance entre "avoir la p√®che" et  "feel great"

Or il faut faire apprendre au mod√®le que :

*  Au premier passage quand l'encodeur re√ßoit  [<CLS> Avoir la p√®che  <fin de phrase>] et le d√©codeur re√ßoit  [<CLS>], le mod√®le  doit pr√©dire  [<CLS> feel].
* Puis "feel" dans ce contexte doit pr√©dire "great" au passage suivant.
* Etc...

Il faut donc lors de l'entra√Ænement **"masquer" √† l'encodeur le reste des  mots √† traduire**. Quand "feel" sort du mod√®le, le d√©codeur ne doit pas  voir "great", **il doit apprendre seul que "feel" dans ce contexte doit  ensuite donner "great"**.

‚Äã	*- Comment :*

Eh bien on va simplement faire en sorte que dans la matrice softmax([Q x Transpos√©e de K]) de notre formule  d'attention, la ligne correspondant a chaque mot soit a **0 pour les  colonnes repr√©sentant les mots suivants "chronologiquement" le dernier  mot g√©n√©r√© par le mod√®le**.

on veut obtenir de quoi masquer √† chaque mot pr√©vu les mots qu'il devrait pr√©voir

donc on veut que les valeurs correspondantes aux mots √† venir n'aient aucune attention

il faut donc que ce soit multipli√©e avec V une matrice dont les valeurs dans le triangle sup√©rieur = 0; pour ce faire on force QK^T √† avoir les valeurs au-dessus de la diagonale de la matrice √† moins l'infini



##### L'attention encodeur-d√©codeur 

Le d√©codeur poss√®de une couche d'attention qui 

* prend en entr√©e le s√©quence de sortie du FFN de l'encodeur, 
* qu'il multiplie √† ses matrices cl√©s et  valeurs (Wki et Wvi pour la t√™te "i"), 
* tandis que la s√©quence sortant de la couche d'auto-attention "masqu√©e" de l'encodeur va se multiplier √†  la matrice des requ√™tes (Wqi pour la t√™te "i").

L'encodeur  d√©couvre 

* des choses int√©ressantes (features) √† propos de la s√©quence  d'entr√©e => les **valeurs**. 
* √Ä ces valeurs attribue un label, un  index, une fa√ßon d'adresser ces "choses" => la **cl√©**. 

Puis le d√©codeur avec sa **requ√™te** va d√©cider du type de valeur √† aller chercher. **La  requ√™te demande la valeur pour une cl√© en particulier.**





##### La sortie



*La couche lin√©aire*

Enfin nous y voila. Appelons S la matrice en sortie du d√©codeur. On la  multiplie par une matrice de poids (qui peuvent apprendre) W1. C'est une **couche totalement connect√©e** qui projette simplement la sortie pr√©c√©dente dans un espace de la taille de notre vocable.

W1 est la matrice qui va permettre d'extraire un mot dans notre  dictionnaire de vocabulaire. Elle aura donc pour dimensions [dimension  des embeddings, i.e. *dmodel*] x [nombre de mots dans notre vocable].



*La softmax*

c'est s√ªrement la derni√®re ligne de S.W1  (correspondant au dernier mot g√©n√©r√©, de dimension [1] x [taille du  vocable]) qui est pass√©e par la softmax. 

La softmax  nous donne alors l'√©l√©ment le plus probable √† pr√©dire (on prend le mot  de la colonne qui donne la probabilit√© la plus haute).



### [Dissecting BERT Part 1: The Encoder](https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3)

The **Encoder** used in BERT is an **attention-based architecture** for Natural Language Processing (NLP) 

the Transformer is composed of two parts, the Encoder and the Decoder. 

**BERT only uses the Encoder** 

##### information flow

1. each token represented as a vector of *emb_dim* size. 1 **embedding vector** for each of the input tokens -> *(input_length) x (emb_dim)* matrix for a specific input sequence.
2. then adds positional information (**positional encoding**). This step returns a matrix of dimensions *(input_length) x (emb_dim)*, just like in the previous step.
3. The data goes through N **encoder blocks**. After this, we obtain a matrix of dimensions *(input_length) x (emb_dim)*.

dimensions of the input and output of the encoder block are the same -> makes sense to use the output of one encoder block as the  input of the next encoder block

the number of blocks N was chosen to be 12 and 24.

the blocks do not share weights with each other

##### Tokenization, numericalization and word embeddings

The first step is to **tokenize** it:

followed by **numericalization**, mapping each token to a unique integer in the corpus‚Äô vocabulary.

 get the **embedding** for each word in the sequence. Each word of the sequence is mapped to a ***emb_dim* dimensional vector that the model will learn during training**. You can think about  it as a vector look-up for each token. The elements of those vectors are **treated as model parameters** and are optimized with back-propagation  just like any other weights.

padding was used to make the input sequences in a batch have the same length. That is, we increase the length of some of the sequences by adding ‚Äò<pad>‚Äô tokens

##### Positional Encoding

At this point, we have a matrix representation of our sequence. However, these representations are not encoding the fact that words appear in  different positions

 we aim to be able to **modify the represented meaning of a specific word  depending on its position**. We don't want to change the full  representation of the word but **we want to modify it a little to encode  its position**.

The approach chosen in the paper is to **add numbers between *[-1,1]* using predetermined (non-learned) sinusoidal functions to the token embeddings**. 

 the word will be represented slightly differently depending on the position the word is in (even if it is the same word).

Moreover, we would like the Encoder to be able to use the fact that some words are in a given position while, in the same sequence, other words are in other specific positions -> the network to be able to **understand relative positions and not only absolute ones**. 

**sinuosidal functions allow positions to be represented as linear combinations of each other and thus allow the network to learn relative relationships between the token positions.**

The approach chosen in the paper to add this information is adding to Z a matrix P with positional encodings.

The authors chose to use a combination of sinusoidal functions. Mathematically, using i for the position of the token in the sequence and j for the position of the embedding feature

advantages over learned positional representations:

* The input_length can be increased indefinitely since the functions can be calculated for any arbitrary position.
* Fewer parameters needed to be learned and the model trained quicker.

The resulting matrix is the input of the first encoder block and has dimensions (input_length) x (emb_dim).

##### Encoder block

N encoder blocks are chained together to generate the Encoder‚Äôs output

A specific block is in charge of finding relationships between the input representations and encode them in its output.

Intuitively, this **iterative process** through the blocks will help the neural network  **capture more complex relationships between words** in the input sequence.  You can think about it as **iteratively building the meaning of the input  sequence as a whole**.



##### Multi-Head Attention

= it computes attention h different times with different weight matrices and then concatenates the results together.

The result of each of those parallel computations of attention is called a **head**. 

once all the heads have been computed they will be concatenated

This will result in a matrix of dimensions *(input_length) x (h*d_v). Afterwards, a linear layer with weight matrix W‚Å∞ of dimensions (h*d_v) x (emb_dim) will be applied leading to a final result of dimensions (input_length) x (emb_dim). 

##### Scaled Dot-Product Attention

Each head is going to be characterized by 3 different projections (matrix multiplications) 

To compute a head we will take the input matrix X and separately project it with the above weight matrices

Once we have K_i, Q_i and V_i we use them to compute the Scaled Dot-Product Attention



**In the encoder block the computation of attention does not use a mask.**

This is the key of the architecture (the name of the paper is no  coincidence) so we need to understand it carefully. Let‚Äôs start by  looking at the matrix product between *Q_i* and *K_i* transposed:

![img](https://miro.medium.com/max/60/1*szTtSJSZBfej5q-KpLmf3Q.png?q=20)



*Q_i* and *K_i* = different projections of the tokens into a *d_k* dimensional space. 

we can think about the **dot product** of those projections as a **measure of similarity between tokens projections**

For every vector projected through *Q_i* the dot product with the projections through *K_i* measures the similarity between those vectors. 

 this is a measure of how similar are the directions of u_i and v_j and how large are their lengths (the closest the direction and the larger the length, the greater the dot product).

Another way of thinking about this matrix product is as the **encoding of a specific relationship between each of the tokens in the input sequence** (the relationship is defined by the matrices K_i, Q_i).

After this multiplication, the matrix is divided element-wise by the square root of d_k for **scaling** purposes.

The next step is a **Softmax** applied row-wise (one softmax computation for each row)

Thus, at this point, the representation of the token is the concatenation of *h* weighted combinations of token representations (centroids) through the *h* different learned projections.

##### Position-wise Feed-Forward Network

3 layers: FC linear layer -> ReLU -> FC linear layer

during this step, **vector representations of tokens don‚Äôt ‚Äúinteract‚Äù with each other.** It is equivalent to run the calculations row-wise and stack the resulting rows in a matrix

The output of this step has dimension *(input_length) x (emb_dim)*.

##### Dropout, Add & Norm

Before this layer, there is always a layer for which inputs and outputs have the same dimensions (Multi-Head Attention or Feed-Forward). We will call that layer Sublayer and its input x.

After each Sublayer, **dropout** is applied with 10% probability. Call this result Dropout(Sublayer(x)). This result is added to the Sublayer‚Äôs input x, and we get x + Dropout(Sublayer(x)).

Observe that in the context of a Multi-Head Attention layer, this means **adding the original representation** of a token x to the representation based on the relationship with other tokens. It is like telling the token:

‚ÄúLearn the relationship with the rest of the tokens, but don‚Äôt forget what we already learned about yourself!‚Äù

Finally, a **token-wise/row-wise normalization** is computed with the mean and standard deviation of each row. This **improves the stability** of the network.

### [Dissecting BERT Appendix: The Decoder](https://medium.com/dissecting-bert/dissecting-bert-appendix-the-decoder-3b86f66b0e5f)

the Decoder in depth; the part of the Transformer architecture that are not used in BERT. 

The problem that the Transformer addresses is translation. To translate a sentence into another language, we want our model to:

* Be able to capture the relationships between the words in the input sentence.
* Combine the information contained in the input sentence and what has already been translated at each step.

Imagine that the goal is to translate a sentence from English to Spanish 

* First, we want to process the information in the input sequence X by **combining the information in each of the words of the sequence** -> done inside the **Encoder**.
* Once we have this information in the output of the Encoder we want to **combine it with the target sequence** -> done in the **Decoder**.

Encoder and Decoder are specific parts of the **Transformer** architecture

**Information Flow**

The data flow through the architecture is as follows:

* (1) represents **each token as a vector** of dimension *emb_dim* -> matrix of dimensions *(input_length) x (emb_dimb)* for a specific input sequence
* (2) adds positional information (**positional encoding**) -> matrix of dimensions *(input_length) x (emb_dim)*
* (3) data goes through N **encoder blocks** -> matrix of dimensions *(input_length) x (emb_dim)*
* (4) target sequence **masked** and sent **through the decoder‚Äôs** **equivalent of 1) and 2)** -> *(target_length) x (emb_dim)* output
* (5) result of 4) goes through N **decoder blocks**. In each of the iterations, the decoder is using the encoder‚Äôs output 3) ->  (target_length) x (emb_dim) output
* (6) applies a **fully connected layer** and a **row-wise softmax** -> *(target_length) x (vocab_size)* output

the described algorithm is processing both the input sentence and the target sentence to train the network

input sentence encoded in The Encoder‚Äôs architecture

in the decoder: how **given a target sentence we obtain a matrix representing the target sentence** for the decoder blocks

same process, composed of two general steps:

* Token embeddings
* Encoding of the positions.

main difference: **the target sentence is shifted** -> before padding, the target sequence will be as follows:

The rest of the process to vectorize the target sequence = as the one described for input sentences in The Encoder‚Äôs architecture.

##### Decoder block ‚Äî Training vs Testing

During **test time** we don‚Äôt have the ground truth. The steps, in this case, will be as follows:

1. Compute the **embedding representation of the input** sequence.
2. Use a **starting sequence token**, for example ‚Äò<SS>‚Äô **as the first target sequence**: [<SS>] -> output = the next token.
3. **Add the last predicted token to the target sequence** and **use it to generate a new prediction** [‚Äò<SS>‚Äô, Prediction_1,‚Ä¶,Prediction_n]
4. Do step 3 **until the predicted token is the one representing the End of the Sequence**, for example <EOS>.

During **training** we have the ground truth, i.e. the tokens we would like the model to output for every iteration of the above process. Since **we have the target in advance**, we will **give the model the whole shifted target sequence at once and ask it to predict the non-shifted target**.

However, there is a problem here. What **if the model sees the expected token and uses it to predict itself**? For example, it might see ‚Äòestas‚Äô at the right of ‚Äòcomo‚Äô and use it to predict ‚Äòestas‚Äô. That‚Äôs **not what we want because the model will not be able to do that a testing time**.

 modify some of the attention layers to **prevent the model of seeing information on the right** (or down in the matrix of vector representation) **but allow it to use the already predicted words**.

transform the matrix of representation and add positional encoding

as in the encoder the output of the decoder block will be also a matrix of sizes *(target_length) x (emb_dim).* 

After a **row-wise linear layer** (a linear layer in the form of matrix product on the right) and a **Softmax** per row this will result in a matrix for which **the maximum element per row indicates the next word**.

we don‚Äôt have problems in the linear layers because they are defined to be token-wise/row-wise in the form of a matrix multiplication through the right

The problem will be in Multi-Head Attention and the input will need to be masked

At training time, the prediction of all rows matter. Given that at prediction time we are doing an iterative process we are just going to care about the prediction of the next word of the last token in the target/output sequence.

##### Masked Multi-Head Attention

This will work exactly as the Multi-Head Attention mechanism but **adding masking to our input**.

T**he only Multi-Head Attention block where masking is required is the first one of each decoder block.** 

the one in the middle is used to combine information between the encoded inputs and the outputs inherited from the previous layers. There is no problem in combining every target token‚Äôs representation with any of the input token‚Äôs representations (since we will have all of them at test time).

The modification will take place after computing the QK/sqrt(d) ratio matrix

the masking step is just going to set to minus infinity all the entries in the strictly upper triangular part of the matrix

* if those entries are relative attention measures per each row, the larger they are, the more attention we need to pay to that token.
* softmax output: the relative attention of those tokens that we were trying to ignore has indeed gone to zero.
* When multiplying this matrix with V_i the only elements that will be accounted for to predict the next word are the ones into its right, i.e. the ones that the model will have access to during test time.

the output of the modified Multi-Head Attention layer will be a matrix *(target_length) x (emb_dim)* because the sequence from which it has been calculated has a sequence length of target_length.

The rest of the process is identical as described in the Multi-Head Attention for the encoder.

##### Multi-Head Attention ‚Äî Encoder output and target

Observe that in this case we are using different inputs for that layer. More specifically, instead of deriving Q_i, K_i and V_i from X as we have been doing in previous Multi-Head Attention layers, this layer will use both the Encoder‚Äôs final output E (final result of all encoder blocks) and the Decoder‚Äôs previous layer output D (the masked Multi-Head Attention after going through the Dropout, Add & Norm layer).

[...]

**every token in the target sequence is represented in every head as a combination of encoded input tokens**. Moreover, this will happen for multiple heads and just as before, that is going to **allow each token of the target sequence to be represented by multiple relationships with the tokens in the input sequence**.



##### Linear and Softmax

This is the final step before being able to get the predicted token for every position in the target sequence. 

 The output from the last Add & Norm layer of the last Decoder block is a matrix X *(target_length)x(emb_dim)*.

linear layer: for every row in x of X compute xW_1

where W_1 is a matrix of learned weights of dimensions *(emb_dim) x (vocab_size)* -> for a specific row the result will be a vector of length *vocab_size*.

a **softmax** is applied to this vector -> **vector describing the probability of the next token**. Therefore, **taking the position corresponding to the maximum probability returns the most likely next word according to the model**.

### [French NLP: entamez le CamemBERT avec les librairies fast-bert et transformers](https://medium.com/@vitalshchutski/french-nlp-entamez-le-camembert-avec-les-librairies-fast-bert-et-transformers-14e65f84c148)

Afin de r√©v√©ler le potentiel de CamemBERT il faudra adapter son mod√®le de langage √† nos donn√©es. En anglais on parle de fine-tuning. Si vous √™tes novice dans le NLP, on peut comparer cette op√©ration √† l‚Äôajustement d‚Äôun costume trop large √† votre taille. Attention, **on ne pr√©-entra√Æne pas CamemBERT** √† nouveau. Ce mod√®le ‚Äúma√Ætrise d√©j√† la grammaire de la langue fran√ßaise‚Äù. On l‚Äôaide simplement √† mieux comprendre la langue et la structure des commentaires. Lors du fine-tuning il faudra utiliser un learning rate tr√®s bas, car il s‚Äôagit des petits ajustements et non pas de r√©-apprentissage.

Les deux principales m√©thodes d'apprentissage automatique de Word2Vec sont Skip-gram et Continuous Bag of Words.

* Le mod√®le **Skip-gram** pr√©dit les mots (contexte) autour du mot cible (cible) (cible -> contexte)
* le mod√®le **Continuous Bag of Words** pr√©dit le mot cible √† partir des mots autour de la cible (contexte) (contexte -> cible)

Le mot cible ne doit pas n√©cessairement se trouver au centre de la ¬´**fen√™tre contextuelle**¬ª qui est compos√©e d'un nombre donn√© de mots environnants, mais peut se trouver √† gauche ou √† droite de la fen√™tre contextuelle.

Un point important √† noter est que **les fen√™tres contextuelles mobiles sont unidirectionnelles**. C'est-√†-dire que la fen√™tre se d√©place sur les mots dans une seule direction, de gauche √† droite ou de droite √† gauche.

en plus de ceux de son nom, BERT apporte d'autres d√©veloppements passionnants dans le domaine de la compr√©hension du langage naturel.

* Pr√©-formation √† partir d'un texte non √©tiquet√©
* Mod√®les contextuels bidirectionnels
* L'utilisation d'une architecture de transformateur
* Mod√©lisation du langage masqu√©
* Attention focalis√©e
* Implication textuelle (pr√©diction de la phrase suivante)
* D√©sambigu√Øsation gr√¢ce au contexte open source

La  magie  du BERT est sa mise en ≈ìuvre d'une **formation bidirectionnelle sur un corpus de texte non √©tiquet√©**, 



BERT a √©t√© **le premier framework / architecture de langage naturel √† √™tre pr√©-form√© en utilisant un apprentissage non supervis√© sur du texte brut** pur (2,5 milliards de mots + de Wikipedia anglais) plut√¥t que sur des corpus √©tiquet√©s.

Les anciens mod√®les de formation en langage naturel ont √©t√© form√©s de mani√®re **unidirectionnelle**. La signification du mot dans une fen√™tre contextuelle s'est d√©plac√©e de gauche √† droite ou de droite √† gauche avec un nombre donn√© de mots autour du mot cible (le contexte du mot ou ¬´c'est la soci√©t√©¬ª). Cela signifie que **les mots qui ne sont pas encore vus dans leur contexte ne peuvent pas √™tre pris en consid√©ration** dans une phrase et qu'ils pourraient en fait changer le sens d'autres mots en langage naturel. Les fen√™tres contextuelles mobiles unidirectionnelles peuvent donc manquer certains contextes changeants importants.

attention - Essentiellement, le BERT est capable de regarder tout le contexte dans la coh√©sion du texte en concentrant l'attention sur un mot donn√© dans une phrase tout en identifiant √©galement tout le contexte des autres mots par rapport au mot. Ceci est r√©alis√© simultan√©ment en utilisant des transformateurs combin√©s avec une pr√©-formation bidirectionnelle.

Cela contribue √† un certain nombre de d√©fis linguistiques de longue date pour la compr√©hension du langage naturel, y compris la r√©solution de cor√©f√©rence. Cela est d√ª au fait que les entit√©s peuvent √™tre cibl√©es dans une phrase en tant que mot cible et que leurs pronoms ou les phrases nominales les r√©f√©ren√ßant sont r√©solus de nouveau vers l'entit√© ou les entit√©s dans la phrase ou l'expression.

De plus, l'attention focalis√©e aide √©galement √† la d√©sambigu√Øsation des mots polys√©miques et des homonymes en utilisant **une pr√©diction / pond√©ration de probabilit√© bas√©e sur le contexte entier du mot en contexte avec tous les autres mots de la phrase**.  Les autres mots re√ßoivent un score d'attention pond√©r√© pour indiquer  combien chacun ajoute au contexte du mot cible en tant que  repr√©sentation du ¬´sens¬ª.

 L'encodeur est l'entr√©e de phrase traduite en repr√©sentations de sens  des mots et le d√©codeur est la sortie de texte trait√©e sous une forme  contextualis√©e.



**Mod√©lisation du langage masqu√©** (formation MLM)

√âgalement connue sous le nom de ¬´**proc√©dure Cloze**¬ª qui existe depuis tr√®s longtemps. L'architecture BERT analyse les phrases avec certains mots masqu√©s de mani√®re al√©atoire et tente de pr√©dire correctement ce qu'est le mot ¬´cach√©¬ª.

Le but de ceci est **d'emp√™cher les mots cibles dans le processus d'apprentissage passant par l'architecture du transformateur BERT de se voir par inadvertance pendant l'entra√Ænement bidirectionnel** lorsque tous les mots sont examin√©s ensemble pour un contexte combin√©. C'est √† dire. cela √©vite un type de boucle infinie erron√©e dans l'apprentissage automatique du langage naturel, qui fausserait le sens du mot.



Implication textuelle (**pr√©diction de la phrase suivante**)

L'une des principales innovations du BERT est qu'il est cens√© √™tre capable de pr√©dire ce que vous allez dire ensuite

form√© pour pr√©dire √† partir de paires de phrases si la deuxi√®me phrase fournie correspond bien √† un corpus de texte.

L'implication textuelle est un type de "qu'est-ce qui vient ensuite?" dans un corps de texte. En plus de l'implication textuelle, le concept est √©galement connu sous le nom de ¬´pr√©diction de la phrase suivante¬ª.

 L'implication textuelle est une t√¢che de traitement du langage naturel impliquant des paires de phrases. 

La premi√®re phrase est analys√©e, puis un niveau de confiance d√©termin√© pour pr√©dire si une deuxi√®me phrase hypoth√©tique donn√©e dans la paire ¬´correspond¬ª logiquement √† la phrase suivante appropri√©e, ou non, avec une pr√©diction positive, n√©gative ou neutre, √† partir d'un texte collection sous examen.



### [Apprentissage de Repr√©sentation dans les R√©seaux de Documents : Application √† la Litt√©rature Scientifique](https://tel.archives-ouvertes.fr/tel-02899422/document)



Le succ√®s des algorithmes d‚Äôapprentissage artificiel d√©pend principalement des repr√©-
sentations des donn√©es sur lesquelles ils sont appliqu√©s.

**L‚Äôapprentissage de repr√©sentation** (AR) [Bengio et al., 2013] s‚Äôoppose √† la construction manuelle de caract√©ristiques des
donn√©es en apprenant automatiquement des descriptions qui rendent leur analyse plus
efficace. En d‚Äôautres termes, plut√¥t que de construire √† la main des repr√©sentations des
donn√©es en utilisant des connaissances expertes, l‚ÄôAR d√©signe une approche diff√©rente o√π
l‚Äôon va construire ces repr√©sentations par un algorithme d‚Äôapprentissage optimisant un ou
plusieurs crit√®res sur les donn√©es.

Un crit√®re couramment utilis√© en apprentissage de repr√©sentation du texte s‚Äôappuie sur
**l‚Äôhypoth√®se distributionnelle**. Celle-ci stipule que **des mots apparaissant dans les m√™mes**
**contextes linguistiques partagent des significations similaire**



 partant d‚Äôune re-
pr√©sentation symbolique du texte (s√©quences de jetons) et transcrivant cette hypoth√®se
dans un espace euclidien muni d‚Äôune mesure de similarit√©, nous sommes en mesure de
construire des vecteurs denses associ√©s aux mots du vocabulaire en rapprochant les mots
apparaissant dans les m√™mes contextes linguistiques.

Cette m√©thodologie est √† l‚Äôorigine
des m√©thodes de plongement de mot (**word embedding**) [Mikolov et al., 2013a] qui a en-
suite √©t√© √©tendue au plongement de r√©seau

Les techniques d‚Äôapprentissage de repr√©sentation pour le texte et pour les r√©seaux sont
intimement li√©s. 

En effet, ces deux types de donn√©es sont naturellement repr√©sent√©es par
des ensembles fini d‚Äô√©l√©ments (ex : mots et sommets) dont on peut mesurer des simila-
rit√©s deux √† deux (ex : nombre de co-occurrences entre mots et nombre de marches o√π
apparaissent des sommets).

es m√™mes m√©thodes de repr√©-
sentations, tels que les plongements ou les m√©canismes d‚Äôattention, soient appliqu√©es √†
ces deux types de donn√©es. 

Un cas simple et largement utilis√© d‚Äôalgorithme de **plongement de r√©seau** est l**‚Äôalgo-**
**rithme de Fruchterman-Reingold** [Fruchterman and Reingold, 1991]. Celui-ci permet de
repr√©senter les sommets dans un espace vectoriel √† deux dimensions, rendant ainsi la vi-
sualisation d‚Äôun r√©seau plus digeste; C‚Äôest une m√©thode it√©rative qui simule, comme un
mod√®le physique, une attraction des sommets connect√©s et une r√©pulsion latente qui vise
√† s√©parer toute paire de sommets.

Originellement, le plongement de graphe est utilis√© comme une m√©thode g√©n√©rale de
r√©duction de dimension

L‚Äô√©mergence des algorithmes de plongement de r√©seau a suivi celle des algorithmes
de plongement de mot (word embedding).

Les m√©thodes d‚Äôapprentissage de repr√©sentation pour le texte et pour les r√©seaux sont
fortement li√©es. En effet, ces deux types de donn√©es peuvent √™tre d√©crits par des √©l√©ments
symboliques (par exemple les mots et les sommets) dont les relations sont mesurables
(par exemple en terme de co-occurrences des mots dans un corpus et de sommets dans
des marches al√©atoires).

L‚Äô**hypoth√®se distributionnelle** est une hypoth√®se fondamentale des algorithmes de plon-
gement de mots. Celle-ci stipule que **la similarit√© distributionnelle des mots est fortement**
**corr√©l√©e avec la similarit√© de leurs sens**.

*  pour construire des repr√©-
  sentations vectorielles captant le sens des mots, il suffit d‚Äô√©tudier le voisinage de ceux-ci,
  c‚Äôest-√†-dire le contexte dans lequel ils apparaissent.
*  si un mod√®le est
  capable de reconstruire les mots contextes d‚Äôun certain mot cible, il est capable d‚Äôen
  repr√©senter le sens

**Skip-Gram** [Mikolov et al., 2013a], l‚Äôune des variantes de la suite logicielle Word2vec est un mod√®le qui **construit deux repr√©sentations pour chaque mot** œâi : **un vecteur cible**
ui et **un vecteur contexte** hi. Ces deux vecteurs sont utilis√©s pour **calculer la probabilit√©**
**conditionnelle d‚Äôobserver un mot selon son contexte**, exprim√©e comme la fonction **softmax
du produit scalaire de leurs repr√©sentations** 

 Cet ensemble est construit en faisant glisser une fen√™tre de taille œÑ sur un corpus de
texte

 Skip-Gram mod√©lise les probabilit√©s d‚Äôoccurrence d‚Äôun mot cible conditionnellement
√† chaque mot contexte, de mani√®re ind√©pendante

a popularit√© de Skip-Gram vient aussi des propri√©t√©s g√©om√©triques des repr√©-
sentations qu‚Äôil construit. En effet, celles-ci semblent √™tre en lien direct avec les sens
s√©mantiques et syntaxiques des mots.

DeepWalk [Perozzi et al., 2014] est une m√©thode de plongement de r√©seau qui s‚Äôinspire
de Skip-Gram. 

‚Äôintuition centrale de cette approche est que les chemins g√©n√©r√©s par de
courtes marches al√©atoires dans un graphe sont similaires √† des phrases en langage naturel.

La fr√©quence d‚Äôapparition des sommets dans ces marches suit une loi puissance, similai-
rement aux fr√©quences des mots dans un corpus

l‚Äôexemple de la traduction
automatique, particuli√®rement telle qu‚Äôelle est abord√©e dans un mod√®le de type encodeur-
d√©codeur neuronal [Bahdanau et al., 2014]. La t√¢che consiste √† produire en sortie une
s√©quence de plongements de mots (y1,...,y_y) √©tant donn√©e une s√©quence de plongements
de mots d‚Äôentr√©e (x1,...,x_x).

Les mots en sortie correspondent par exemple √† de l‚Äôanglais
alors que les mots en entr√©e correspondent √† du fran√ßais.

Le processus est **auto-r√©gressif**, ce
qui signifie que l**a s√©quence de sortie est g√©n√©r√©e mot par mot**.

√Ä chaque √©tape i, l‚Äôencodeur
utilise la s√©quence d‚Äôentr√©e (x1,...,x`x) et le morceau de s√©quence de sortie pr√©c√©demment
g√©n√©r√© (y1,...,yi‚àí1) afin de pr√©dire le mot suivant yi.

Notons que l‚Äôon d√©finit le premier
vecteur de sortie comme une constante y1 = ystart ce qui permet de d√©finir la r√©currence
pour la premi√®re it√©ration. 

le probl√®me auquel cette approche fait face est la gestion de
la diversit√© d‚Äôinformation pr√©sente en entr√©e pour pr√©dire chaque sortie

 En traduction
automatique, pr√©dire un mot ne d√©pend souvent que d‚Äôune infime proportion des vecteurs
pr√©sent√©s √† l‚Äôencodeur-d√©codeur (x1,...,x`x,y1,...,yi‚àí1). 

. Lorsque cette s√©quence est longue,
il devient difficile pour le mod√®le d‚Äôen faire le tri. C‚Äôest pr√©cis√©ment pour faciliter ce tri que
les **m√©canismes d‚Äôattention** sont utilis√©s. Ils vont **permettre au mod√®le de ne s√©lectionner qu‚Äôun sous-ensemble pr√©cis de la s√©quence d‚Äôentr√©e afin de pr√©dire le prochain mot**.

Le premier m√©canisme d‚Äôattention pour les r√©seaux profonds est pr√©sent√© dans [Xu
et al., 2015] pour la g√©n√©ration automatique de l√©gendes pour images. Le mod√®le est d√©-
crit comme un **encodeur-d√©codeur**. L‚Äôencodeur extrait un ensemble de N repr√©sentations
vectorielles (a1,...,aN) d‚Äôune image via un **CNN**, appel√©es vecteurs d‚Äôannotation. Ces vec-
teurs sont g√©n√©r√©s de sorte √† capturer les diff√©rentes composantes de l‚Äôimage. Le d√©codeur,
un **LSTM**, produit de mani√®re auto-r√©gressive une s√©quence de mots en sortie. 

Pour ce
faire, il prend en entr√©e les mots pr√©c√©demment g√©n√©r√©s (y1,...,yi‚àí1). **L‚Äôattention est in-**
**t√©gr√©e pour conditionner cette g√©n√©ration au vecteurs d‚Äôannotation**. 

Le **vecteur contexte**
ci du LSTM est calcul√© non plus comme une fonction de ci‚àí1 et de hi‚àí1 mais comme une
**moyenne pond√©r√©e des vecteurs d‚Äôannotation issus de l‚Äôimage** 

Les poids
Œ±k sont calcul√©s par produit scalaire entre le vecteur cach√© hi‚àí1 de la pr√©c√©dente √©tape
(capturant l‚Äôhistorique des mots g√©n√©r√©s (y1,...,yi‚àí1)) avec chacun des vecteurs d‚Äôannota-
tion ak.

chaque mot est g√©n√©r√© en confrontant les caract√©ristiques de l‚Äôimage
et l‚Äôhistorique des mots pr√©c√©demment g√©n√©r√©s en l√©gende

 Ceci permet au mod√®le de se
concentrer alternativement sur les diff√©rents √©l√©ments de l‚Äôimage afin de construire une
description en langage naturelle de celle-ci

en √©tudiant les poids Œ±k, on peut
facilement identifier la r√©gion de l‚Äôimage ayant motiv√© le choix du mot g√©n√©r√© par le
mod√®le

Le **Transformer** [Vaswani et al., 2017] est **le premier mod√®le de traduction automa-**
**tique reposant uniquement sur un m√©canisme d‚Äôattention, sans RNN ni CNN**

**L‚Äôaspect**
**s√©quentiel de l‚Äôentra√Ænement d‚Äôun RNN (pr√©dire chaque mot l‚Äôun apr√®s l‚Äôautre) consti-**
**tue le v√©ritable point faible** de ces architectures car il rend difficile leur parall√©lisation.

Le Transformer reprend l‚Äôarchitecture **encodeur-d√©codeur** couramment utilis√©e en traduc-
tion automatique et introduit un **m√©canisme d‚Äôattention**, le **scaled dot-product attention**
(SDPA), **parall√©lisable** et reposant principalement sur des op√©rations matricielles.

Dans
sa version la plus simple, SDPA transforme une s√©quence de vecteurs d‚Äôentr√©e (x1,...,x_l)
en une s√©quence de vecteurs de sortie (y1,...,y_l) dont **chaque repr√©sentation yi repr√©-*
sente xi conditionnellement √† l‚Äôensemble des vecteurs (x1,...,x_l).** 

SDPA construit 3 repr√©sentations distinctes des xi par projections lin√©aires : 

* les **requ√™tes** Q = XWq,
* les **clefs** K = XWk et 
* les **valeurs** V = XWv. 

En notant œÅw la dimension de toutes les repr√©sentations X,Y,Q,K et V , le m√©canisme d‚Äôattention s‚Äô√©crit :
$ Y = softmax( \frac{QK^T}{‚àöœÅ_w})V$ 

le **nominateur** = une matrice dont chaque valeur contient le produit scalaire entre une requ√™te qi et une clef kj. 

le **d√©nominateur** permet de r√©duire le probl√®me de fuite du gradient qui
peut se produire lorsque le softmax prend des valeurs extr√™mes, en limitant la magnitude
du produit scalaire dont l‚Äô√©tendue des valeurs augmente naturellement avec la dimension.

Le **softmax**, op√©r√© sur chaque ligne de la matrice, permet d‚Äôobtenir une pond√©ration com-
pos√©e de valeurs positives sommant √† un, similaire √† des **probabilit√©s associ√©es aux n mots**
d‚Äôentr√©e, pour chaque requ√™te q

Les poids d‚Äôattention s‚Äô√©crivent $ Œ± = softmax( \frac{QK^T}{‚àöœÅ_w})$  et se somment √† 1

La multiplication matricielle entre ces poids d‚Äôattention et $V$ consiste √† r√©aliser des moyennes pond√©r√©es des v_j

 **Le produit scalaire entre requ√™tes et clefs g√©n√®re**
**des poids d‚Äôattention qui permettent de pond√©rer les valeurs, de sorte √† repr√©senter ce mot**
**contextuellement aux autres mots de la phrase.**

L'intuition derri√®re cette formule est que si Œ±ij est proche de 1, le vecteur yi sera
fortement influenc√© par la valeur vj (et donc √† une projection lin√©aire pr√®s par xj).

si Œ±ij est proche de 0, c‚Äôest que yi n‚Äôest pas li√© √† xj. 

les seuls param√®tres
de ce m√©canisme sont Wq,Wk et Wv de dimensions œÅw √óœÅw et leur r√¥le est de projeter
la s√©quence d‚Äôentr√©e dans trois espaces Q,K et V de sorte √† capturer les d√©pendances
entre les mots. 

 Les concepts de ¬´ clef ¬ª, ¬´ valeur ¬ª et ¬´ requ√™te ¬ª proviennent de syst√®mes
de stockage des donn√©es

. Par exemple, lorsque l‚Äôon saisit une **requ√™te** pour rechercher un
document dans une base de donn√©es, un moteur de recherche associe cette requ√™te √† un
ensemble de **clefs** enregistr√©s dans la base (titre du document, contenu, date, etc.), puis il
pr√©sente les meilleures correspondances de documents (**valeurs**)

Dans le Transformer, **le m√©canisme d‚Äôattention est utilis√© de deux fa√ßons diff√©rentes** :

1. dans des m√©canismes d‚Äô**auto-attention** (self-attention
   * de sorte √† **construire des repr√©-**
     **sentations contextuelles Xc des mots de la s√©quence d‚Äôentr√©e et Y c des mots de la s√©quence**
     **de sortie** 
2. dans le d√©codeur
   *  les **requ√™tes sont construites √† partir des sorties**
     **contextuelles** Y c
   * **les clefs et valeurs sont construites √† partir des entr√©es contextuelles**
     **Xc**
   * **Le d√©codeur fabrique ainsi des repr√©sentations de la s√©quence de sortie capturant ses**
     **d√©pendances avec la s√©quence d‚Äôentr√©e**, facilitant la pr√©diction du prochain mot

le
mod√®le dans son int√©gralit√© est en r√©alit√© compos√© de multiples m√©canismes d‚Äôattention
op√©r√©s en parall√®les, concat√©n√©s puis r√©utilis√©s sur de multiples couches. L‚Äôarchitecture
enti√®re fait intervenir un grand nombre de perceptrons √† plusieurs couches augmentant
significativement le nombres de param√®tres et rendant non trivial l‚Äôentra√Ænement du mo-
d√®le

Le Transformer, plus pr√©cis√©ment son **encodeur avec ses m√©canismes d‚Äôauto-attention**,
constitue la brique de base de **BERT** [

L‚Äôid√©e principale est de pr√©-entra√Æner ce m√©canisme d‚Äôattention sur
des t√¢ches tr√®s g√©n√©rales ne requ√©rant pas d‚Äôannotation particuli√®re avant d‚Äôaffiner les
param√®tres du mod√®le sur des t√¢ches sp√©cifiques, telles que la d√©tection d‚Äôentit√© nomm√©e et
l‚Äôanalyse de sentiment. 

Pour le **pr√©-entra√Ænement**, **deux t√¢ches** sont propos√©es : 

1. **la pr√©diction de mots masqu√©s** 
   * on
     pr√©sente au mod√®le des phrases dont 15% des mots ont √©t√© remplac√©s par un vecteur
     sp√©cial de masque et on optimise les param√®tres du mod√®le de sorte √† ce que les vecteurs
     de sortie associ√©s aux masques soient les plus proches possible des vecteurs des mots
     cach√©s. 
2. la **pr√©diction de phrases successives**.
   * on tire al√©atoirement 50% de paires de phrases qui se
     suivent dans un corpus et 50% de paires de phrases qui ne se suivent pas et on entra√Æne
     un classifieur √† pr√©dire si ces phrases se suivent √† partir d‚Äôune repr√©sentation de sortie
     produite √† partir d‚Äôun vecteur sp√©cial de classification ajout√© aux phrases d‚Äôentr√©e.



### [Comprendre le langage √† l'aide de XLNet avec pr√©-formation autor√©gressive](https://ichi.pro/fr/comprendre-le-langage-a-l-aide-de-xlnet-avec-pre-formation-autoregressive-172103194027075)                    

**XLNet** surpasse le BERT sur 20 t√¢ches de r√©f√©rence en PNL

XLNet exploite le meilleur de la mod√©lisation de **langage autor√©gressif** (AR) et de **l'autoencodage** (AE), les deux objectifs de pr√©-formation les plus connus, tout en √©vitant leurs limites

Consid√©r√© comme l'un des d√©veloppements les plus importants de 2019 en PNL, XLNet **combine le mod√®le de langage autor√©gressif**, Transformer-XL , **et la capacit√© bidirectionnelle** de BERT pour lib√©rer la puissance de cet important outil de mod√©lisation de langage

Pour la phase de pr√©-formation, les deux architectures les plus r√©ussies sont la mod√©lisation du langage autor√©gressif (AR) et l'autoencodage  (AE). 

1. ##### Mod√©lisation du langage autor√©gressif (AR)

Dans les mod√®les AR conventionnels, le contexte unidirectionnel dans le sens avant ou arri√®re dans une s√©quence de texte est cod√©

 Il est utile pour les t√¢ches NLP g√©n√©ratives qui g√©n√®rent un contexte dans le sens direct.

Cependant, AR √©choue dans le cas o√π le contexte bidirectionnel doit √™tre utilis√© simultan√©ment . Cela pourrait devenir probl√©matique, en particulier avec la t√¢che de compr√©hension du langage en aval o√π des informations de contexte bidirectionnelles sont requises.

2. ##### Mod√®le de langage de codage automatique (AE)

Un mod√®le bas√© sur AE a la capacit√© de mod√©liser des contextes bidirectionnels en reconstruisant le texte original √† partir d'une entr√©e corrompue ([MASK]). Le mod√®le AE est donc meilleur que le mod√®le AR lorsqu'il s'agit de mieux capturer le contexte bidirectionnel.

Un exemple notable d'AE est **[BERT](https://arxiv.org/pdf/1810.04805.pdf) qui est bas√© sur l'auto-encodage de d√©bruitage**. 

il souffre d'un **√©cart de pr√©-entra√Ænement-finetune** r√©sultant de la d√©pendance entre les jetons masqu√©s et ceux non masqu√©s.

[MASK] utilis√© dans l'√©tape de pr√©-formation est absent des donn√©es r√©elles utilis√©es aux t√¢ches en aval, y compris l'√©tape de r√©glage fin. 

Pour les caract√©ristiques de d√©pendance d'ordre √©lev√© et √† longue port√©e en langage naturel, BERT simplifie √† l'extr√™me le probl√®me en supposant que les jetons pr√©dits (masqu√©s dans l'entr√©e) sont ind√©pendants les uns des autres tant que les jetons non masqu√©s sont donn√©s.

Alors que AR peut estimer la probabilit√© d'un produit avant ou arri√®re sous la forme d'une distribution de probabilit√© conditionnelle, **BERT ne peut pas mod√©liser la probabilit√© conjointe en utilisant la r√®gle du produit en raison de son hypoth√®se d'ind√©pendance pour les jetons masqu√©s.**

3. ##### En quoi XLNet diff√®re-t-il des AR et AE conventionnels (BERT)?

Les auteurs de XLNet proposent de **conserver les avantages du mod√®le de langage AR tout en lui faisant apprendre du contexte bidirectionnel en tant que mod√®les AE (par exemple, BERT) pendant la phase de pr√©-formation**. 

**L'interd√©pendance entre les jetons sera pr√©serv√©e**, contrairement √† BERT. 

Le nouvel objectif propos√© est appel√© "**Mod√©lisation du langage de permutation**. "

Diff√©rent de BERT et d'autres transformateurs qui combinent l'incorporation de position et l'incorporation de contenu pour la pr√©diction, XLNet pr√©dit la distribution du prochain jeton en prenant en compte la position cible z_t comme entr√©e. 

L'architecture **d'auto-attention √† deux flux** est utilis√©e pour  r√©soudre les probl√®mes que pose le transformateur traditionnel; se compose de **deux types  d'attention personnelle**. 

1. la **repr√©sentation du flux de  contenu**
   * identique √† l'auto-attention standard de Transformer qui prend  en compte √† la fois le contenu (x_ {z_t}) et les informations de  position (z_t). 
2. la **repr√©sentation de requ√™te**, 
   * **remplace  essentiellement le [MASK]** de BERT, appris par l'attention du flux de  requ√™te pour pr√©dire x_ {z_t} **uniquement avec des informations de  position mais pas son contenu**.
   * seules les informations de position du  jeton cible et les informations de contexte avant le jeton sont  disponibles.

Le r√©sultat final de l'attention √† deux flux est une **distribution de  pr√©diction sensible √† la cible**. 

La principale diff√©rence entre XLNet et  BERT est que **XLNet n'est pas bas√© sur la corruption de donn√©es** comme le  fait BERT, il peut donc √©viter les limitations de BERT r√©sultant du  masquage

XLNet int√®gre un **sch√©ma de codage relatif** et un **m√©canisme de r√©currence de segment** de [Transformer-XL](https://arxiv.org/pdf/1901.02860.pdf) pour capturer les d√©pendances qui sont plus √©loign√©es que les RNN et Transformer. 

Le **codage  positionnel relatif** est appliqu√© en fonction de la s√©quence d'origine. 

Le **m√©canisme de r√©currence au niveau du segment** √©vite la fragmentation  de contexte repr√©sent√©e par le traitement de segment de longueur fixe.  Il permet de r√©utiliser des segments de phrase du pass√© avec le nouveau  segment. Le Transformer-XL r√©alise cela en incluant la r√©currence au  niveau du segment dans les √©tats masqu√©s.

le Transformer standard contient des **informations de position dans les codages de position**, matrice U , avec int√©gration de position absolue. 

Transformateur-XL **code pour la distance relative dynamiquement dans le score de l' attention** en introduisant la matrice R . 

Dans le score d'attention de Transformer-XL, les 4 termes repr√©sentent respectivement l'adressage bas√© sur le contenu, le biais de position d√©pendant du contenu, le biais de contenu global et le biais de position global. 

Avec Transformer-XL, des articles de texte coh√©rents peuvent √™tre g√©n√©r√©s et il y a √©galement une acc√©l√©ration substantielle lors de l'√©valuation par rapport aux RNN et au Transformer standard.

**Dans XLNet, Transformer-XL est inclus dans le cadre de pr√©-formation**. Le m√©canisme de r√©currence de Transformer-XL est ainsi incorpor√© dans le param√®tre de permutation propos√© dans XLNet pour r√©utiliser les √©tats cach√©s des segments pr√©c√©dents.

 L'ordre de factorisation dans la permutation des segments pr√©c√©dents ne sera pas mis en cache et r√©utilis√© √† l'avenir. 

Seule la repr√©sentation du contenu du segment est conserv√©e dans les √©tats masqu√©s.

**XLNet combine la capacit√© bidirectionnelle de BERT et la technologie autor√©gressive de Transformer-XL** pour r√©aliser une am√©lioration substantielle; il bat BERT dans plus d'une douzaine de t√¢ches. 



### [Review: BioBERT paper](https://medium.com/@raghudeep/biobert-insights-b4c66fde8fa7)

The major contribution is a pre-trained bio-medical language representation model for various bio-medical text mining tasks. Tasks such as NER from Bio-medical data, relation extraction, question & answer in the  biomedical field.

BioBERT is a **contextualized language representation model, based on BERT**, a pre-trained model which is trained on different combinations of general & biomedical domain corpora.

Approach

* Initialize BioBERT with BERT pre-trained model trained on Wikipedia 2.5 billion words & Books Corpus 0.8 billion words. 
  * rather than taking a random initialization of weights, pre-trained weights from BERT model are taken. 
  * Transferring representations learned from previous corpora, it's similar to the transfer learning used for image data problems.
* pre-training on the domain data, here BioBERT is pre-trained on PubMed Abstracts 4.5 billion words & PMC Full-text articles 13.5 billion words.
* the pre-trained model is used to fine-tune on various biomedical text mining tasks like NER, question & answer, relation extraction.

The interesting part was that the **pre-training was not just on biomedical corpora but rather took different combinations of general & biomedical corpora** since their research objective was to figure out the performance of BERT with different combinations of corpora & the amount of data needed to pre-train the model.

Authors have used Word piece tokenizer (read sec 4.1 of the paper) as used by the BERT paper to mitigate OOV (out of the vocabulary) problem.

### [Approaches to Biomedical Text Mining with BERT](https://medium.com/geekculture/approaches-to-biomedical-text-mining-dca3408397b0)

BERT includes a pre-training step and a fine-tuning step.

##### (1) A pre-training step builds a generic language model.

BERT builds a language model by learning the context of words in a  left-to-right and right-to-left manner (**bidirectional**). 

during training, 15% of all words are masked (blanked out)  in sequences at random. 

The training process predicts the masked-out  word using a standard back propagation of errors method

BERT **also  learns the relationships between two sentences** by predicting whether the next sentence following the previous sentence is the actual *next* sentence or a random sentence. 

when choosing two  sentences A and B for each training sample, 50% of the time, B is the  actual next sentence that follows A, and 50% of the time it‚Äôs a random  sentence from the text. 

expensive labeling is not required in  either case

##### (2) A fine-tuning step with labeled data after pre-training with unlabeled data.

example of one of several fine-tuning, answering questions. 

**Fine-tuning** a supervised downstream task has the advantage of having to **learn only a few additional parameters with a relatively small, labeled dataset**.

BERT (fine-tuned on SQuAD) **learns two extra vectors that mark the beginning and the end of the answer span**. They are the start-token classifier and the end-token classifier that demarcate the answer (from the paragraph).

##### Applying BERT to the Biomedical Domain ‚Äî BioBERT

The word distribution and context in the general text domain (WikiPedia and a large collection of books) is quite different from the biomedical domain (PubMed) and thus fine-tuning or adaptation is required (the architecture remains the same, however). Medical literature has a preponderance of medical terms; proper nouns (e.g., BRCA1, c.248T>C) and terms (e.g., transcriptional, antimicrobial), which are readily understood by biomedical researchers.

##### Pre-training BioBERT

Step 1, **initialize BioBERT with weights from BERT** (**transfer learning**).

Step 2, **BioBERT is pre-trained on biomedical domain text** (PubMed abstracts and PebMed Central full-text articles).



###  [ILLUSTRATION DE BERT](https://lbourdois.github.io/blog/nlp/BERT/) 

#####  1.1 R√©capitulatif sur le word embeddings 

La pratique a fait √©merg√©e que c‚Äô√©tait une excellente id√©e d‚Äôutiliser des **embeddings pr√©-entrain√©s sur de grandes quantit√©s de donn√©es textuelles** au lieu de les former avec le mod√®le sur ce qui √©tait souvent un petit jeu de donn√©es. Il est donc devenu possible de t√©l√©charger une liste de mots et leurs embeddings g√©n√©r√©es par le pr√©-entra√Ænement avec Word2Vec ou GloVe. V

La structure encodeur-d√©codeur du Transformer le rend tr√®s efficace pour la traduction automatique. 

#####  **1.5 L‚ÄôOpen AI Transformer (Pr√©-entra√Ænement d‚Äôun d√©codeur de Transformer pour la mod√©lisation du langage)** 

nous n‚Äôavons pas besoin d‚Äôun Transformer complet pour adopter l‚Äôapprentissage par transfert dans le cadre de taches de NLP. Nous pouvons nous contenter du decodeur du Transformer. 

Le d√©codeur est un bon choix parce que c‚Äôest un choix naturel pour la mod√©lisation du langage (pr√©dire le mot suivant). En effet il est construit pour masquer les futurs tokens ‚Äì une fonction pr√©cieuse lorsqu‚Äôil g√©n√®re une traduction mot √† mot.

Le mod√®le empile douze couches de d√©codeurs. Puisqu‚Äôil n‚Äôy a pas d‚Äôencodeur, les couches de d√©codeurs n‚Äôont pas la sous-couche d‚Äôattention encodeur-d√©codeur comme dans le Transformer classique. Ils ont cependant toujours la couche d‚Äôauto-attention.

nous pouvons proc√©der √† l‚Äôentra√Ænement du mod√®le sur la m√™me t√¢che de mod√©lisation du langage : pr√©dire le mot suivant en utilisant des ensembles de donn√©es massifs (sans label). 

L‚Äôentra√Ænement est r√©alis√© sur 7.000 livres car ils permettent au mod√®le d‚Äôapprendre √† associer des informations connexes

 l‚ÄôOpenAI Transformer est pr√©-entrain√© et que ses couches ont √©t√© ajust√©es pour g√©rer raisonnablement le langage, nous pouvons commencer √† l‚Äôutiliser pour des t√¢ches plus sp√©cialis√©es. 

GPT-2

2. ##### BERT : du d√©codeur √† l‚Äôencodeur 

#####  **2.1 Architecture du mod√®le** 

L‚Äôarticle original pr√©sente deux tailles de mod√®les pour BERT :

- BERT BASE de taille comparable √† celle de l‚ÄôOpenAI Transformer afin de comparer les performances.
- BERT LARGE, un mod√®le beaucoup plus grand qui a atteint l‚Äô√©tat de l‚Äôart des r√©sultats rapport√©s dans l‚Äôarticle.

Les deux mod√®les BERT ont un grand nombre de **couches d‚Äôencodeurs** (appell√©es **Transformer Block** dans l‚Äôarticle d‚Äôorigine) :

* 12 pour la version de base
* 24 pour la version large. 

Ils ont √©galement **des r√©seaux feedforward** plus grands (768 et 1024 unit√©s cach√©es respectivement) et plus de **t√™tes d‚Äôattention** (12 et 16 respectivement) que la configuration par d√©faut dans l‚Äôimpl√©mentation initial du Transformer 

#####  2.2 Entr√©es du mod√®le 

Le premier token d‚Äôentr√©e est un jeton sp√©cial [CLS]

Tout comme l‚Äôencodeur du Transformer, BERT prend une s√©quence de mots en entr√©e qui remonte dans la pile**. Chaque couche applique l‚Äôauto-attention et transmet ses r√©sultats √† un r√©seau feed-forward, puis les transmet √† l‚Äôencodeur suivant**

Trouver la bonne mani√®re d‚Äôentra√Æner une pile d‚Äôencodeurs est un  obstacle complexe que BERT r√©sout en adoptant un concept de ¬´ **mod√®le de  langage masqu√©** ¬ª (Masked LM en anglais) tir√© de la litt√©rature  ant√©rieure (il s‚Äôagit d‚Äôune **Cloze task**).

* prendre al√©atoirement 15% des tokens en entr√©e puis √† masquer 80% d‚Äôentre eux, en remplacer 10% par un autre token compl√®tement al√©atoire (un autre mot) et de ne rien faire dans le cas des 10% restant
* objectif: que le mod√®le pr√©dise correctement le token original modifi√© (via la perte d‚Äôentropie crois√©e)
* Le mod√®le est donc **oblig√© de conserver une repr√©sentation contextuelle** distributionnelle de chaque jeton d‚Äôentr√©e.

Afin d‚Äôam√©liorer BERT dans **la gestion des relations** existant **entre plusieurs phrases**, le processus de pr√©-entra√Ænement comprend une t√¢che suppl√©mentaire : √©tant donn√© deux phrases (A et B), B est-il susceptible d‚Äô√™tre la phrase qui suit A, ou non ?

#####  2.3 Sorties du mod√®le 

nous nous concentrons uniquement sur la sortie de la premi√®re position (√† laquelle nous avons pass√© le jeton sp√©cial [CLS]).

Ce vecteur peut maintenant √™tre utilis√© comme entr√©e pour un classifieur de notre choix. L‚Äôarticle obtient d‚Äôexcellents r√©sultats en utilisant simplement **un r√©seau neuronal √† une seule couche comme classifieur**.

en cas de plusieurs labels: modifier le r√©seau du classifieur pour avoir plus de neurones de sortie qui passent ensuite par la couche softmax.

#####  2.4 Mod√®les sp√©cifiques √† une t√¢che 

 les auteurs de BERT pr√©cisent les approches de fine-tuning appliqu√©es pour quatre t√¢ches de NLP diff√©rentes

1. classification
2. tests de logique
3. QA
4. NER

#####  2.5 BERT pour l‚Äôextraction de features 

L‚Äôapproche fine-tuning n‚Äôest pas l‚Äôunique mani√®re d‚Äôutiliser BERT. Tout comme ELMo, vous pouvez utiliser BERT pr√©-entrain√© pour cr√©er des word embeddings contextualis√©s. Vous pouvez ensuite int√©grer ces embeddings √† votre mod√®le existant.

##### remarques

BERT ne consid√®re pas les mots comme des tokens. Il regarde plut√¥t les WordPieces (par exemple : playing donne play + ##ing).



### [ILLUSTRATION DU WORD EMBEDDING ET DU WORD2VEC](https://lbourdois.github.io/blog/nlp/word_embedding/)               



Au lieu de regarder seulement deux mots avant le mot cible, nous pouvons aussi regarder deux mots apr√®s lui. C‚Äôest ce qu‚Äôon appelle une architecture de **continuous bag of words**.

 Au lieu de deviner un mot en fonction de son contexte (les mots avant et apr√®s), cette autre architecture essaie de deviner les mots voisins en utilisant le mot courant. Cette m√©thode s‚Äôappelle l‚Äôarchitecture **skipgram**.

Pour g√©n√©rer des embeddings de haute qualit√©, nous pouvons passer d‚Äôun mod√®le de la pr√©diction d‚Äôun mot voisin √† un mod√®le qui prend le mot d‚Äôentr√©e et le mot de sortie, et sort un score indiquant s‚Äôils sont voisins ou non (0 pour ¬´ non voisin ¬ª, 1 pour ¬´ voisin ¬ª).

Nous passons d‚Äôun r√©seau neuronal √† un mod√®le de r√©gression logistique qui est ainsi beaucoup plus simple et beaucoup plus rapide √† calculer.

 Mais il y a une faille √† combler. Si tous nos exemples sont positifs (cible : 1), nous nous ouvrons √† la possibilit√© d‚Äôun mod√®le qui renvoie toujours 1 ‚Äì atteignant 100% de pr√©cision, mais n‚Äôapprenant rien et g√©n√©rant des embeddings de d√©chets.

 nous devons introduire des √©chantillons n√©gatifs dans notre ensemble de donn√©es, c‚Äôest √† dire des √©chantillons de mots qui ne sont pas voisins. Notre mod√®le doit retourner 0 pour ces √©chantillons. (**negative sampling**) -> **Skipgram with Negative Sampling**

11. ##### Processus d‚Äôentra√Ænement de Word2vec 

Au d√©but de la phase d‚Äôentra√Ænement, nous cr√©ons deux matrices ‚Äì une **Embedding matrix** et une **Context matrix**. 

Ces deux matrices ont un embedding pour chaque mot de notre vocabulaire (*vocab_size* est donc une de leurs dimensions). 

La seconde dimension est la longueur que nous voulons que chaque vecteur d‚Äôembedding soit (une valeur g√©n√©ralement utilis√©e de *embedding_size* est 300, mais nous avons regard√© un exemple de 50 plus t√¥t dans ce post).

au d√©but de l‚Äôentra√Ænement, nous initialisons ces matrices avec des valeurs al√©atoires.

A chaque √©tape de l‚Äôentra√Ænement, nous prenons un exemple positif et les exemples n√©gatifs qui y sont associ√©s

Nous proc√©dons √† la recherche de leurs embeddings. Pour le mot d‚Äôentr√©e, nous regardons dans l‚ÄôEmbedding matrix. Pour les mots de contexte, nous regardons dans la Context matrix.

 nous effectuons le produit scalaire de l‚Äôembeddings d‚Äôentr√©e avec chacun des embeddings de contexte.

transformer ces scores en quelque chose qui ressemble √† des probabilit√©s. Nous avons besoin qu‚Äôils soient tous positifs et qu‚Äôils aient des valeurs entre z√©ro et un. Pour cela, nous utilisons la fonction sigmo√Øde.

La taille de la fen√™tre et le nombre d‚Äô√©chantillons n√©gatifs sont deux hyperparam√®tres cl√©s dans le processus d‚Äôentra√Ænement de word2vec.

Une heuristique est que des fen√™tres de petite taille (2-15) conduisent √† des embeddings avec des scores de similarit√© √©lev√©s entre deux embeddings. Cela signifie que les mots sont interchangeables 

Des fen√™tres de plus grande taille (15-50, ou m√™me plus) m√®nent √† des embeddings o√π la similarit√© donne une indication sur la parent√© des mots

###  [ILLUSTRATION DU TRANSFORMER](https://lbourdois.github.io/blog/nlp/Transformer/) 

Transformer, un mod√®le qui utilise l‚Äôattention pour augmenter la vitesse √† laquelle ces mod√®les peuvent √™tre entra√Æn√©s

 Le Transformer surpasse le mod√®le de traduction automatique de Google dans des t√¢ches sp√©cifiques

 **Le plus grand avantage, vient de la fa√ßon dont le Transformer se pr√™te √† la parall√©lisation.**

1. ##### apper√ßu haut niveau

un composant d‚Äôencodage, un composant de d√©codage et des connexions entre eux.

Le composant d‚Äôencodage est une pile **d‚Äôencodeurs** (l‚Äôarticle empile six encodeurs les uns sur les autres)

Le composant de d√©codage est une pile de **d√©codeurs** du m√™me nombre.

**Les encodeurs**

* tous **identiques mais ne partagent pas leurs poids**
* chacun divis√© en 2 sous-couches :
  1. Les entr√©es de l‚Äôencodeur passent d‚Äôabord par **une couche  d‚Äôauto-attention** 
     * **aide l‚Äôencodeur √† regarder les autres  mots dans la phrase d‚Äôentr√©e** lorsqu‚Äôil code un mot sp√©cifique
  2. Les sorties de la couche d‚Äôauto-attention sont transmises √† un **r√©seau feed-forward**. 
     * Le m√™me r√©seau feed-forward **appliqu√© ind√©pendamment √† chaque encodeur**.

**Le d√©codeur**

* poss√®de ces 2 couches, mais entre elles se trouve une **couche d‚Äôattention qui aide le d√©codeur √† se concentrer sur les parties pertinentes de la phrase d‚Äôentr√©e** **(encoder-decoder attention**; comme dans les mod√®les seq2seq).

#####  **2. Les tenseurs** 

 nous commen√ßons par transformer chaque mot d‚Äôentr√©e en vecteur √† l‚Äôaide d‚Äôun algorithme d‚Äôembedding.

**L‚Äôembedding n‚Äôa lieu que dans l‚Äôencoder inf√©rieur.** Le point commun √† **tous les encodeurs est qu‚Äôils re√ßoivent une liste de vecteurs de la taille 512.** Dans l‚Äôencoder du bas cela serait le word embeddings, mais dans les autres encodeurs, ce serait la sortie de l‚Äôencodeur qui serait juste en dessous.

La **taille de la liste** est un hyperparam√®tre que nous pouvons d√©finir. Il s‚Äôagirait essentiellement de **la longueur de la phrase la plus longue** dans notre ensemble de donn√©es d‚Äôentra√Ænement.

Apr√®s avoir enchass√© les mots dans notre s√©quence d‚Äôentr√©e, chacun d‚Äôentre eux traverse chacune des deux couches de l‚Äôencodeur.

 dans chacune des positions, le mot circule √† travers son propre chemin dans l‚Äôencodeur

Il y a des **d√©pendances entre ces chemins dans la couche d‚Äôauto-attention.**

**La couche feed-forward n‚Äôa pas ces d√©pendances** et donc les diff√©rents chemins peuvent √™tre ex√©cut√©s en parall√®le lors de cette couche.

#####  **3. L‚Äôencodage** 

un encodeur re√ßoit une liste de vecteurs en entr√©e. Il traite cette liste en passant ces vecteurs dans une couche d‚Äôauto-attention, puis dans un r√©seau feed-forward, et enfin envoie la sortie vers le haut au codeur suivant.

 Le mot √† chaque position passe par un processus d‚Äôauto-attention. Ensuite, chacun d‚Äôeux passe par un r√©seau feed-forward (le m√™me r√©seau feed-forward pour chaque vecteur mais chacun le traverse s√©par√©ment). 

#####  **4. Introduction √† l‚Äôauto-attention** 

Au fur et √† mesure que le mod√®le traite chaque mot (chaque position dans la s√©quence d‚Äôentr√©e, **l‚Äôauto-attention lui permet d‚Äôexaminer d‚Äôautres positions dans la s√©quence d‚Äôentr√©e √† la recherche d‚Äôindices qui peuvent aider √† un meilleur codage pour ce mot.**

L‚Äôauto-attention est la m√©thode que le Transformer utilise pour  **am√©liorer la compr√©hension du mot** qu‚Äôil est en train de traiter en  fonction des autres mots pertinents.

5. ##### L‚Äôauto-attention en d√©tail 

*1√®re √©tape*: **cr√©er 3 vecteurs √† partir de chacun des vecteurs d‚Äôentr√©e** xi de l‚Äôencodeur (dans ce cas, l‚Äôembedding de chaque mot).

Chaque vecteur d‚Äôentr√©e xi est utilis√© de 3 mani√®res diff√©rentes dans l‚Äôop√©ration d‚Äôauto-attention :

1. Il est **compar√© √† tous les autres vecteurs** pour √©tablir les **pond√©rations pour sa propre production** yi ->  forme le **vecteur de requ√™te** (**Query**)
2. Il est **compar√© √† tous les autres vecteurs** pour √©tablir les **pond√©rations pour la sortie** du j-√®me vecteur yj -> forme le **vecteur de cl√©** (**Key**).
3. Il est **utilis√© comme partie de la somme pond√©r√©e** pour **calculer chaque vecteur de sortie** une fois que les pond√©rations ont √©t√© √©tablies -> forme le **vecteur de valeur** (**Value**).

Ces vecteurs sont cr√©√©s en multipliant l‚Äôembedding par trois matrices que nous avons form√©es pendant le processus d‚Äôentra√Ænement

ces nouveaux vecteurs sont de plus petite dimension que le vecteur d‚Äôembedding; Ils n‚Äôont pas besoin d‚Äô√™tre plus petits. C‚Äôest un choix d‚Äôarchitecture pour rendre la computation des t√™tes d‚Äôattentions constante.

*2√®me √©tape*:  calculer  un score. Example: calcul de l‚Äôauto-attention pour le 1er mot; il faut noter chaque mot de la phrase d‚Äôentr√©e par rapport √† ce mot.  **Le score d√©termine le degr√© de concentration √† placer sur les autres  parties de la phrase d‚Äôentr√©e** au fur et √† mesure que nous codons un mot √† une certaine position.

score est calcul√© en prenant le **produit scalaire du vecteur de requ√™te avec le vecteur cl√©** du mot que nous √©valuons. Donc, si nous traitons l‚Äôauto-attention pour le mot en position #1, le premier score serait le produit scalaire de q1  et k1. Le deuxi√®me score serait le produit scalaire de q1 et k2.

*3√®me et 4√®me √©tape*: diviser les scores  par la racine carr√©e de la dimension des vecteurs cl√©s utilis√©s -> permet d‚Äôobtenir des gradients plus stables.

softmax peut √™tre sensible √† de tr√®s grandes valeurs d‚Äôentr√©e -> tue le gradient et ralentit l‚Äôapprentissage, ou l‚Äôarr√™te compl√®tement. 

la valeur moyenne du produit scalaire augmente avec la dimension de l‚Äôembedding, il est utile de redimensionner un peu le produit scalaire pour emp√™cher les entr√©es de la fonction softmax de devenir trop grandes.

Il pourrait y avoir d‚Äôautres valeurs possibles que la racine carr√©e de la dimension, mais c‚Äôest la valeur par d√©faut.

**Softmax** permet de normaliser les scores pour qu‚Äôils soient tous positifs et somment √† 1.

Ce score softmax d√©termine **√† quel point chaque mot sera exprim√© √† sa  position**. Il est donc logique que le mot √† sa position aura le score  softmax le plus √©lev√©, mais **le score des autres mots permet de  d√©terminer leur pertinence par rapport au mot trait√©**.

*5√®me √©tape*. **multiplier chaque vecteur de valeur par le score softmax** (en vue de les additionner) -> garder intactes les valeurs du ou des mots sur lesquels nous voulons nous concentrer, et de **noyer les mots non pertinents** (en les multipliant par de petits nombres comme 0,001, par exemple).

*6√®me √©tape*. **r√©sumer les vecteurs de valeurs pond√©r√©es**. Ceci **produit la sortie de la couche d‚Äôauto-attention √† cette position** 

Les vecteurs zi r√©sultants peuvent √™tre envoy√©s au r√©seau feed-forward. En pratique cependant, ce calcul est effectu√© sous forme de matrice pour un traitement plus rapide

6. ##### Les matrices de calcul de l‚Äôauto-attention 

*1√®re √©tape*. **calculer les matrices Requ√™te, Cl√© et Valeur**. concat√©ner les embeddings dans une matrice X et la multiplier par les matrices de poids que nous avons entra√Æn√©s 

*√©tapes 2 √† 6*. peuvent √™tre concat√©n√©es en 1 formule pour calculer les sorties de la couche  d‚Äôauto-attention.

 7. ##### La b√™te √† plusieurs t√™tes

**Au lieu d‚Äôex√©cuter une seule fonction d‚Äôattention** les auteurs de l‚Äôarticle ont trouv√© avantageux de **projeter lin√©airement les requ√™tes, les cl√©s et les valeurs h fois avec diff√©rentes projections lin√©aires** apprises sur les dimensions dk, dk et dv, respectivement.

Ce m√©canisme est appel√© ¬´ **attention multi-t√™tes** ¬ª. Cela **am√©liore les performances de la couche d‚Äôattention** de deux fa√ßons :

1. √©largit la capacit√© du mod√®le √† se concentrer sur diff√©rentes positions
   * ¬´ Marie a donn√© des roses √† Susane ¬ª: ¬´ donn√© ¬ª a des relations diff√©rentes aux diff√©rentes parties de la phrase. ¬´ Marie ¬ª exprime qui fait le don, ¬´ roses ¬ª exprime ce qui est donn√©, et ¬´ Susane ¬ª exprime qui est le destinataire. 
   * **En une seule op√©ration d‚Äôauto-attention, toutes ces informations ne font que s‚Äôadditionner** -> Si c‚Äô√©tait Suzanne qui avait donn√© les roses plut√¥t que Marie, le vecteur de sortie zdonn√© serait le m√™me, m√™me si le sens a chang√©.
2. donne √† la couche d‚Äôattention de **multiples ¬´ sous-espaces de repr√©sentation ¬ª**. 
   * avec l‚Äôattention √† plusieurs t√™tes, nous n‚Äôavons pas seulement un, mais **plusieurs ensembles de matrices de poids** Query/Key/Value (le Transformer utilise huit t√™tes d‚Äôattention, donc nous obtenons huit ensembles pour chaque encodeur/d√©codeur)
   * chacun de ces ensembles est **initialis√© au hasard**. 
   * apr√®s l‚Äôentra√Ænement, chaque ensemble est utilis√© pour projeter les embedding d‚Äôentr√©e (ou les vecteurs des encodeurs/d√©codeurs inf√©rieurs) dans un **sous-espace de repr√©sentation diff√©rent.**

Si nous faisons le m√™me calcul d‚Äôauto-attention que nous avons d√©crit ci-dessus, huit fois avec des matrices de poids diff√©rentes, nous obtenons huit matrices Z diff√©rentes.

mais la couche de feed-forward attend une matrice unique (un vecteur pour chaque mot). 

pour condenser ces huit √©l√©ments en une seule matrice: **concat√©ner** les matrices puis les multiplier par **une matrice de poids suppl√©mentaire** WO.

#####  **8. Le codage positionnel** 

 fa√ßon de **rendre compte de l‚Äôordre des mots dans la s√©quence d‚Äôentr√©e.**

 le Transformer **ajoute un vecteur √† chaque embedding** d‚Äôentr√©e. Ces vecteurs suivent un mod√®le sp√©cifique que le mod√®le apprend ce qui l‚Äôaide √† d√©terminer la position de chaque mot (ou la distance entre les diff√©rents mots dans la s√©quence). L‚Äôintuition ici est que l‚Äôajout de ces valeurs √† l‚Äôembedding **fournit des distances significatives entre les vecteurs d‚Äôembedding** une fois qu‚Äôils sont projet√©s dans les vecteurs Q/K/V (puis pendant l‚Äôapplication du produit scalaire).

diff√©rentes m√©thodes possibles pour le codage positionnel; e.g. les valeurs de la moiti√© gauche sont g√©n√©r√©es par une fonction (qui utilise le sinus), et la moiti√© droite est g√©n√©r√©e par une autre fonction (qui utilise le cosinus). Ils sont ensuite concat√©n√©s pour former chacun des vecteurs d‚Äôencodage positionnel. 

#####  **9. Les connexions r√©siduelles** 

 **chaque sous-couche (auto-attention, feed-forward) dans chaque codeur a une *connexion r√©siduelle* autour de lui et est suivie d‚Äôune √©tape de *normalisation*.**

Cela vaut √©galement pour les sous-couches du d√©codeur.

10. ##### Le decodeur 

L‚Äôencoder commence par traiter la s√©quence d‚Äôentr√©e. 

La **sortie de l‚Äôencoder sup√©rieur** est ensuite transform√©e en un ensemble de **vecteurs d‚Äôattention K et V**. 

Ceux-ci doivent √™tre **utilis√©s par chaque d√©codeur dans sa couche ¬´ attention encodeur-d√©codeur ¬ª** qui permet au decodeur de se concentrer sur les endroits appropri√©s dans la s√©quence d‚Äôentr√©e 

Chaque √©tape de la phase de d√©codage produit un √©l√©ment de la s√©quence de sortie

Les √©tapes suivantes r√©p√®tent le processus jusqu‚Äô√† ce qu‚Äôun symbole sp√©cial indique au d√©codeur que le Transformer a compl√©t√© enti√®rement la sortie. 

**La sortie de chaque √©tape (mot ici) est envoy√©e au d√©codeur le plus bas pour le traitement du mot suivant**. 

 tout comme pour les entr√©es encodeur, nous ¬´ embeddons ¬ª et **ajoutons un codage positionnel √† ces entr√©es d√©codeur** pour indiquer la position de chaque mot.

Les couches **d‚Äôauto-attention du d√©codeur** fonctionnent d‚Äôune mani√®re l√©g√®rement diff√©rente de celle de l‚Äôencodeur.

* la couche d‚Äô**auto-attention** ne peut s‚Äôoccuper **que des positions ant√©rieures** dans la s√©quence de sortie. 
  * fait en masquant les positions futures (en les r√©glant sur -inf) avant l‚Äô√©tape softmax du calcul de l‚Äôauto-attention.

* la couche ¬´ **Attention encodeur-d√©codeur** ¬ª fonctionne comme une auto-attention √† plusieurs t√™tes, sauf qu‚Äôelle cr√©e sa **matrice de requ√™tes √† partir de la couche inf√©rieure**, et prend la **matrice des cl√©s et des valeurs √† la sortie de la pile encodeur**. 

 11. ##### Les couches finales : lin√©aire et sofmax

La pile de decodeurs d√©livre un vecteur de float. 

Comment le transformer en mots ? C‚Äôest le travail de la couche Lin√©aire qui est suivie d‚Äôune couche Softmax.

La **couche lin√©aire** est un simple r√©seau neuronal enti√®rement connect√© qui **projette le vecteur produit par la pile de decodeurs dans un vecteur beaucoup (beaucoup) plus grand appel√© vecteur logits**.

Supposons que notre mod√®le connaisse 10 000 mots anglais uniques (le ¬´ vocabulaire de sortie ¬ª de notre mod√®le) qu‚Äôil a appris de son ensemble de donn√©es d‚Äôentra√Ænement. Cela rendrait le vecteur logit large de 10 000 cellules, **chaque cellule correspondant au score d‚Äôun mot unique**. C‚Äôest ainsi que nous interpr√©tons la sortie du mod√®le suivie de la couche lin√©aire.

La **couche softmax** **transforme ensuite ces scores en probabilit√©s** (tous positifs dont la somme vaut 1). La cellule ayant la probabilit√© la plus √©lev√©e est choisie et le mot qui lui est associ√© est produit comme sortie pour ce pas de temps.

#####  **12. L‚Äôentra√Ænement** 

Pendant l‚Äôentra√Ænement, un mod√®le non entra√Æn√© passerait exactement par le m√™me processus. Mais puisque nous l‚Äôentra√Ænons sur un ensemble de donn√©es d‚Äôentra√Ænement labellis√©, nous pouvons comparer sa sortie avec la sortie correcte r√©elle.

Une fois que nous avons d√©fini notre vocabulaire de sortie, nous pouvons utiliser un vecteur de la m√™me largeur pour indiquer chaque mot de notre vocabulaire. C‚Äôest ce qu‚Äôon appelle aussi le **one-hot encoding**

#####  **13. La fonction de perte** 

Comment comparer deux distributions de probabilit√©s ? Nous  soustrayons simplement l‚Äôune √† l‚Äôautre. Pour plus de d√©tails, voir  l‚Äôentropie crois√©e et la divergence de Kullback-Leibler.

 Par exemple en entr√©e : ¬´ Je suis √©tudiant ¬ª et comme r√©sultat attendu : ¬´ I am a student ¬ª. Ce que cela signifie vraiment, c‚Äôest que nous voulons que notre mod√®le produise successivement des distributions de probabilit√©s o√π :

* Chaque distribution de probabilit√© est repr√©sent√©e par un vecteur de largeur vocab_size (6 dans notre exemple, mais de fa√ßon plus r√©aliste un nombre comme 3 000 ou 10 000)
* La premi√®re distribution de probabilit√©s a la probabilit√© la plus √©lev√©e √† la cellule associ√©e au mot ¬´ I ¬ª
* La deuxi√®me distribution de probabilit√© a la probabilit√© la plus √©lev√©e √† la cellule associ√©e au mot ¬´ am ¬ª
* Et ainsi de suite jusqu‚Äô√† ce que la cinqui√®me distribution de sortie indique ‚Äò‚Äô, auquel est √©galement associ√©e une cellule du vocabulaire √† 10 000 √©l√©ments



### [LE SEQ2SEQ ET LE PROCESSUS D‚ÄôATTENTION](https://lbourdois.github.io/blog/nlp/Seq2seq-et-attention/)               

Un sequence-to-sequence model est un mod√®le qui prend une s√©quence d‚Äô√©l√©ments (mots, lettres, caract√©ristiques d‚Äôune image‚Ä¶etc) et en sort une autre s√©quence. 

Sous le capot, le mod√®le est compos√© d‚Äôun encodeur et d‚Äôun d√©codeur.

L‚Äô **encodeur** traite chaque √©l√©ment de la s√©quence d‚Äôentr√©e. Il compile les informations qu‚Äôil capture dans un vecteur (appel√© **context**). Apr√®s avoir trait√© toute la s√©quence d‚Äôentr√©e, l‚Äôencodeur envoie le context au **d√©codeur**, qui commence √† produire la s√©quence de sortie item par item.

L‚Äôencodeur et le d√©codeur ont tendance √† √™tre tous deux des r√©seaux neuronaux r√©currents.

Le **vecteur de contexte s‚Äôest av√©r√© √™tre un goulot d‚Äô√©tranglement** pour ces types de mod√®les. Il √©tait donc difficile pour les mod√®les de composer avec de longues phrases. Une solution a √©t√© propos√©e dans Bahdanau et al., 2014 et Luong et al., 2015. Ces articles introduisirent et affin√®rent une technique appel√©e ¬´ **Attention** ¬ª, qui am√©liora consid√©rablement la qualit√© des syst√®mes de traduction automatique. L‚Äôattention **permet au mod√®le de se concentrer sur les parties pertinentes de la s√©quence d‚Äôentr√©e si n√©cessaire.**

Cette **capacit√© d‚Äôamplifier le signal de la partie pertinente** de la s√©quence d‚Äôentr√©e permet aux mod√®les d‚Äôattention de produire de meilleurs r√©sultats que les mod√®les sans attention.

Un mod√®le d‚Äôattention diff√®re d‚Äôun sequence-to-sequence model classique de deux fa√ßons principales :

1. **l‚Äôencodeur transmet beaucoup plus de donn√©es au decodeur**. 
   * Au lieu de passer le dernier √©tat cach√© de l‚Äô√©tape d‚Äôencodage, **l‚Äôencodeur passe tous les √©tats cach√©s au decodeur **
2.  un d√©codeur d‚Äôattention fait une √©tape suppl√©mentaire avant de produire sa sortie. Pour se concentrer sur les parties de l‚Äôentr√©e qui sont pertinentes, le d√©codeur
   * **regarde l‚Äôensemble des √©tats cach√©s de l‚Äôencodeur** qu‚Äôil a re√ßu (chaque √©tat cach√© de l‚Äôencoder est le plus souvent associ√© √† un certain mot dans la phrase d‚Äôentr√©e).
   * donne un **score √† chaque √©tat cach√©** 
   * **multiplie chaque √©tat cach√© par son score** attribu√© via softmax (amplifiant ainsi les √©tats cach√©s avec des scores √©lev√©s, et noyant les √©tats cach√©s avec des scores faibles)

 Le ¬´ scorage ¬ª se fait √† chaque pas de temps (nouveau mot) du c√¥t√© du d√©codeur.

 le mod√®le n‚Äôassocie pas seulement le premier mot de la sortie avec le premier mot de l‚Äôentr√©e. En fait, il a appris pendant la phase d‚Äôentrainement la fa√ßon dont sont li√©s les mots dans cette paire de langues (le fran√ßais et l‚Äôanglais dans notre exemple). 

###  [LES RNN, LES LSTM, LES GRU ET ELMO](https://lbourdois.github.io/blog/nlp/RNN-LSTM-GRU-ELMO/) 

Les RNN (recurrent neural network ou r√©seaux de neurones r√©currents en fran√ßais) sont des r√©seaux de neurones qui ont jusqu‚Äô√† encore 2017/2018, √©t√© majoritairement utilis√© dans le cadre de probl√®me de NLP.

Cette architecture poss√®de un probl√®me. **Lorsque la s√©quence √† traiter est trop longue, la r√©tropropagation du gradient de l‚Äôerreur peut soit devenir beaucoup trop grande et exploser, soit au contraire devenir beaucoup trop petite**. **Le r√©seau ne fait alors plus la diff√©rence entre une information qu‚Äôil doit prendre en compte ou non**. Il se trouve ainsi dans l‚Äôincapacit√© d‚Äôapprendre √† long terme. 

solution propos√©e par Les LSTM: prendre en compte un **vecteur m√©moire** **via un syst√®me de 3 portes (gates) et 2 √©tats**

- **Forget** **gate** (capacit√© √† oublier de l‚Äôinformation, quand celle-ci est inutile)
- **Input** **gate** (capacit√© √† prendre en compte de nouvelles informations utiles)
- **Output** **gate** (quel est l‚Äô√©tat de la cellule √† l‚Äôinstant t sachant la forget et la input gate)
- **Hidden** **state** (√©tat cach√©)
- **Cell** **state** (√©tat de la cellule)

**GRU** = une variante des LSTM; structure plus simple que les LSTM car moins de param√®tres entrent en jeu. **2 portes et 1 √©tat**:

* **Reset** **gate** (porte de reset)
* **Update** **gate** (porte de mise √† jour)
* **Cell** **state** (√©tat de la cellule)

En pratique, les GRU et les LSTM permettent d‚Äôobtenir des r√©sultats comparables. **L‚Äôint√©r√™t des GRU par rapport aux LSTM √©tant le temps d‚Äôex√©cution qui est plus rapide puisque moins de param√®tres doivent √™tre calcul√©s.**



#####  ELMo (l‚Äôimportance du contexte)

Embeddings from Language Models = bas√© sur un LSTM bidirectionnel

un mot peut avoir plusieurs sens selon la mani√®re dont o√π il est utilis√©; Pourquoi ne pas lui donner un embedding bas√© sur le contexte dans lequel il est utilis√© ? A la fois pour capturer le sens du mot dans ce contexte ainsi que d‚Äôautres informations contextuelles ->**contextualized word-embeddings**.

Au lieu d‚Äôutiliser un embedding fixe pour chaque mot, **ELMo examine l‚Äôensemble de la phrase avant d‚Äôassigner une embedding** √† chaque mot qu‚Äôelle contient. Il utilise un **LSTM bidirectionnel form√© sur une t√¢che sp√©cifique pour pouvoir cr√©er ces embedding**.

ELMo a constitu√© **un pas important vers le pr√©-entra√Ænement** dans le contexte du NLP. En effet, nous pouvons l‚Äôentra√Æner sur un ensemble massif de donn√©es dans la langue de notre ensemble de donn√©es, et ensuite nous pouvons **l‚Äôutiliser comme un composant dans d‚Äôautres mod√®les** qui ont besoin de traiter le langage.

Plus pr√©cis√©ment, **ELMo est entra√Æn√© √† pr√©dire le mot suivant dans une s√©quence de mots** ‚Äì une t√¢che appel√©e mod√©lisation du langage (Language Modeling). C‚Äôest pratique car nous disposons d‚Äôune grande quantit√© de donn√©es textuelles dont un tel mod√®le peut s‚Äôinspirer sans avoir besoin de labellisation.

‚Äã                                                                                                                              

ELMo va m√™me plus loin et forme un **LSTM bidirectionnel**

ELMo propose **l‚Äôembedding contextualis√© en regroupant les √©tats cach√©s (et l‚Äôembedding initial)** d‚Äôune certaine mani√®re (concat√©nation suivie d‚Äôune sommation pond√©r√©e).



### [ILLUSTRATION DU GPT2](https://lbourdois.github.io/blog/nlp/GPT2/)               

bas√© sur un Transformer entra√Æn√© sur un ensemble de donn√©es massif. 

le mod√®le de Transformer original est compos√© d‚Äôun encodeur et d‚Äôun  d√©codeur (chacun est une pile de ce que nous pouvons appeler des  transformer blocks). Cette architecture est appropri√©e parce que le  mod√®le s‚Äôattaque √† la traduction automatique

##### **Une diff√©rence par rapport √† BERT** 

**Le GPT-2 est construit √† l‚Äôaide de blocs d√©codeurs**. 

**BERT, pour sa  part, utilise des blocs d‚Äôencodeurs**. 

l‚Äôune des principales diff√©rences entre les  deux est que **le GPT-2, comme les mod√®les de langage traditionnels,  produit un seul token √† la fois**.

 Invitons par exemple un GPT-2 bien  entra√Æn√© √† r√©citer la premi√®re loi de la robotique :  ¬´ A robot may not  injure a human being or, through inaction, allow a human being to come  to harm ¬ª.

La fa√ßon dont fonctionnent r√©ellement ces mod√®les est qu‚Äô**apr√®s chaque token produit, le token est ajout√© √† la s√©quence des entr√©e**s. Cette nouvelle s√©quence devient l‚Äôentr√©e du mod√®le pour la prochaine √©tape. Cette id√©e est appel√©e ¬´ **autor√©gression** ¬ª et a permis aux RNN d‚Äô√™tre efficaces.

**Le GPT2 et certains mod√®les plus r√©cents comme TransformerXL et XLNet sont de nature autor√©gressive**; **BERT ne l‚Äôest pas.**

C‚Äôest un compromis. **En perdant l‚Äôautor√©gression, BERT a acquis la  capacit√© d‚Äôincorporer le contexte des deux c√¥t√©s d‚Äôun mot** pour obtenir  de meilleurs r√©sultats. 

**XLNet ram√®ne l‚Äôautor√©gression tout en trouvant  une autre fa√ßon d‚Äôint√©grer le contexte des deux c√¥t√©s.**

 Un bloc encodeur de l‚Äôarticle d‚Äôorigine peut recevoir des entr√©es jusqu‚Äô√† une certaine longueur maximale (512 tokens). Si une s√©quence d‚Äôentr√©e est plus courte que cette limite, nous avons simplement √† rembourrer le reste de la s√©quence. (ajouter <pad>)

**Une diff√©rence cl√© dans la couche d‚Äôauto-attention est qu‚Äôelle masque les futurs tokens ‚Äì non pas en changeant le mot en [mask] comme BERT,  mais en interf√©rant dans le calcul de l‚Äôauto-attention en bloquant les  informations des tokens qui sont √† la droite de la position √† calculer.**

Un bloc d‚Äôauto-attention normal permet √† une position d‚Äôatteindre le  sommet des tokens √† sa droite. L‚Äôauto-attention masqu√©e emp√™che que cela se produise 

Le mod√®le OpenAI GPT-2 utilise uniquement ces blocs d√©codeurs.  tr√®s similaires aux blocs d√©codeurs d‚Äôorigine, sauf qu‚Äôils suppriment la deuxi√®me couche d‚Äôauto-attention.

 Le GPT-2 peut traiter 1024 jetons. Chaque jeton parcourt tous les blocs d√©codeurs le long de son propre chemin. 

La fa√ßon la plus simple d‚Äôex√©cuter un GPT-2 entra√Æn√© est de lui  permettre de se promener de lui-m√™me (ce qui est techniquement appel√©  **generating unconditional samples**).   

Nous pouvons aussi le pousser √† ce  qu‚Äôil parle d‚Äôun certain sujet (**generating interactive conditional  samples**). Dans le premier cas, nous pouvons simplement lui donner le  token de d√©marrage et lui faire commencer √† g√©n√©rer des mots (le mod√®le  utilise *<|endoftext|>* comme token de d√©marrage. Appelons-le < s > √† la place pour simplifier les graphiques).

Le token est trait√© successivement √† travers toutes les couches, puis un vecteur est produit le long de ce chemin. 

GPT-2 a un param√®tre appel√© top-k que nous pouvons utiliser pour que le mod√®le consid√®re des mots d‚Äô√©chantillonnage autres que le top mot (ce qui est le cas lorsque top-k = 1).



##### encodage de l'entr√©e

Comme dans d‚Äôautres mod√®les de NLP, le GPT-2 recherche l‚Äô**embedding** du mot d‚Äôentr√©e dans son embedding matrix (obtenue apr√®s entra√Ænement).

Ainsi au d√©but nous recherchons l‚Äôembedding du token de d√©part < s > dans la matrice. Avant de transmettre cela au premier bloc du  mod√®le, nous devons incorporer le **codage positionnel** (un signal qui  indique aux blocs l‚Äôordre des mots dans la s√©quence). Une partie du  mod√®le entra√Æn√© contient une matrice ayant un vecteur de codage  positionnel pour chacune des 1024 positions de l‚Äôentr√©e.

 Envoyer un mot au premier bloc du Transformer, c‚Äôest rechercher son embedding et additionner le vecteur de codage positionnel pour la position #1.



##### voyage dans le bloc

Le premier bloc peut maintenant traiter le premier token en le  faisant passer d‚Äôabord par le processus d‚Äôauto-attention, puis par sa  couche feed forward. 

Une fois le traitement effectu√©, le bloc envoie le  vecteur r√©sultant pour qu‚Äôil soit trait√© par le bloc suivant. 

Le  processus est identique dans chaque bloc mais **chaque bloc a des poids  qui lui sont propres dans l‚Äôauto-attention et dans les sous-couches du  r√©seau neuronal**.



##### l'auto-attention

 on attribue des notes √† la pertinence de chaque mot du segment et additionne leur repr√©sentation vectorielle.

##### Le processus de l‚Äôauto-attention 

L‚Äôauto-attention est trait√©e le long du parcours de chaque token. Les composantes significatives sont **trois vecteurs** :

- **Query** : la requ√™te est une **repr√©sentation du mot  courant**. Elle est **utilis√©e pour scorer le mot vis-√†-vis des autres mots**  (en utilisant leurs cl√©s).
- **Key** : les vecteurs cl√©s sont comme des **labels**  pour tous les mots de la s√©quence. C‚Äôest **contre eux que nous nous  mesurons dans notre recherche de mots pertinents**.
- **Value** : les vecteurs de valeurs sont des  **repr√©sentations de mots r√©els**. Une fois que nous avons √©valu√© la  pertinence de chaque mot, ce sont les valeurs que nous **additionnons pour repr√©senter le mot courant**.

Une analogie grossi√®re est de penser √† la recherche dans un classeur. 

* La **requ√™te (query) est le sujet que vous recherchez.** 
* Les **cl√©s (key)  sont comme les √©tiquettes** des chemises √† l‚Äôint√©rieur de l‚Äôarmoire. 
* Lorsque vous faites correspondre la requ√™te et la cl√©, nous enlevons le  contenu du dossier. **Le contenu correspond au vecteur de valeur (value)**.  

Sauf que vous ne recherchez pas seulement une valeur, mais un m√©lange de valeurs √† partir d‚Äôun m√©lange de dossiers.

**Multiplier le vecteur de requ√™te par chaque vecteur cl√© produit un score** pour chaque dossier (techniquement : le produit scalaire suivi de softmax).

**Nous multiplions chaque valeur par son score et sommons. Cela donne le r√©sultat de notre auto-attention.**

Cette op√©ration permet d‚Äôobtenir un vecteur pond√©r√©



##### **Sortie du mod√®le**

Lorsque le bloc le plus haut du mod√®le produit son vecteur de sortie (le r√©sultat de sa propre auto-attention suivie de son propre r√©seau feed-forward), le mod√®le **multiplie ce vecteur par la matrice d‚Äôembedding**.

chaque ligne de la matrice d‚Äôembedding correspond √† un word embedding dans le vocabulaire du mod√®le. Le r√©sultat de cette multiplication est interpr√©t√© comme **un score pour chaque mot du vocabulaire** du mod√®le.

Nous pouvons simplement s√©lectionner le token avec le score le plus √©lev√© (top_k = 1). Mais de meilleurs r√©sultats sont obtenus si le mod√®le tient √©galement compte d‚Äôautres termes. Ainsi, une bonne strat√©gie consiste √† tirer au hasard un mot provenant du vocabulaire. **Chaque mot ayant comme probabilit√© d‚Äô√™tre s√©lectionner, le score qui lui a √©t√© attribu√©** (de sorte que les mots avec un score plus √©lev√© ont une plus grande chance d‚Äô√™tre s√©lectionn√©s). Un terrain d‚Äôentente consiste √† **fixer top_k √† 40,** et √† demander au mod√®le de prendre en compte les 40 mots ayant obtenu les scores les plus √©lev√©s.

Le mod√®le a alors termin√© une it√©ration aboutissant √† l‚Äô√©dition d‚Äôun  seul mot. **Le mod√®le it√®re alors jusqu‚Äô√† ce que le contexte entier soit  g√©n√©r√© (1024 tokens) ou qu‚Äôun token de fin de s√©quence soit produit.**

en r√©alit√©, le GPT2 utilise le **Byte Pair Encoding** pour cr√©er les tokens dans son vocabulaire = les tokens sont g√©n√©ralement des parties des mots



##### **Auto-attention (sans masking)**

 dans un bloc encoder. L‚Äôauto-attention s‚Äôapplique en **trois** **√©tapes** principales :

1. **Cr√©ation des vecteurs Query, Key et Value** pour chaque chemin.
2. Pour chaque token d‚Äôentr√©e, on utilise son **vecteur de requ√™te pour lui attribuer un score par rapport √† tous les autres vecteurs cl√©s.**
3. **Sommation des vecteurs de valeurs apr√®s les avoir multipli√©s par leurs scores associ√©s**. 



##### **Cr√©ation des vecteurs Query, Key et Value** 

1. Pour le premier token, nous prenons sa requ√™te et la comparons √†  toutes les cl√©s. Cela produit un score pour chaque cl√©. La **premi√®re  √©tape de l‚Äôauto-attention consiste √† calculer les trois vecteurs pour  chaque token**  (en multipliant par la matrice de poids) (ignorons les t√™tes d‚Äôattention pour le moment) 
2. **multiplions sa requ√™te par tous les autres vecteurs cl√©s** pour obtenir un score pour chacun des quatre tokens.
3. **multiplier les scores par les vecteurs de valeurs**. Une valeur avec un score √©lev√© constituera une grande partie du vecteur r√©sultant une fois que nous les aurons additionn√©s.

Si nous faisons **la m√™me op√©ration pour chaque token, nous obtenons** **un vecteur repr√©sentant et tenant compte du contexte pour chacun d‚Äôeux**. Ces vecteurs sont ensuite pr√©sent√©s √† la sous-couche suivante du bloc (le r√©seau de neurones feed-forward).



##### Auto-attention (avec masking)

identique √† l‚Äôauto-attention, sauf √† l‚Äô√©tape 2.

Supposons que le mod√®le n‚Äôa que deux tokens en entr√©e et que nous observons le deuxi√®me token. Dans ce cas, les deux derniers tokens sont masqu√©s. Le mod√®le attribue alors toujours aux futurs tokens un score de 0.

Ce ¬´ masquage ¬ª est souvent mis en ≈ìuvre sous la forme d‚Äôune matrice appel√©e **masque d‚Äôattention**.

A titre d‚Äôexemple, supposons avoir une s√©quence de quatre mots : ¬´ robot must obey orders ¬ª.

Sous forme matricielle, nous calculons les scores en multipliant une matrice de requ√™tes par une matrice cl√©s 

le masque d‚Äôattention **r√®gle les cellules que l‚Äôon veut **masquer sur **-inf ou un nombre n√©gatif tr√®s important** (par exemple -1 milliard pour le GPT2) 

l‚Äôapplication de softmax produit les scores r√©els que nous utilisons pour l‚Äôauto-attention

#####  L‚Äôauto-attention masqu√©e du GPT-2

**Le GPT-2 conserve les vecteurs cl√© et valeur des tokens qu‚Äôil a d√©j√† trait√© afin de ne peut √† avoir √† les recalculer √† chaque fois** qu‚Äôun nouveau token est trait√©. 

*Etape 1 : Cr√©ation des vecteurs Query, Key et Value*

Chaque bloc d‚Äôun Transformer a ses propres poids. Nous nous servons de **la matrice de poids pour cr√©er les vecteurs des requ√™tes, des cl√©s et des valeurs**. Cela consiste en pratique √† une **simple multiplication**.

 En multipliant le vecteur d‚Äôentr√©e par le vecteur de poids d‚Äôattention (et en ajoutant un vecteur de biais non repr√©sent√© ici), on obtient les vecteurs cl√©, valeur et requ√™te pour ce token. 

*Les t√™tes d'attention*

**L‚Äôauto-attention est men√©e plusieurs fois sur diff√©rentes parties des vecteurs Q,K,V.**

S√©parer les t√™tes d‚Äôattention, c‚Äôest simplement **reconstruire le vecteur long sous forme de matrice.**

Le plus petit GPT2 poss√®de 12 t√™tes d‚Äôattention. Il s‚Äôagit donc de la premi√®re dimension de la matrice remodel√©e 

 *Etape 2 : Scoring*

 *Etape 3 : Somme*

nous multiplions maintenant chaque valeur par son score, puis nous les additionnons pour obtenir le r√©sultat de l‚Äôattention port√©e √† la t√™te d‚Äôattention n¬∞1 

 *Fusion des t√™tes d‚Äôattention*

Nous concat√©nons les t√™tes d‚Äôattention.

 *Etape 4 : Projection*

la 2√®me grande matrice de poids qui projette les r√©sultats des t√™tes d‚Äôattention dans le vecteur de sortie de la sous-couche d‚Äôauto-attention 

 *Etape 5 : Fully Connected Neural Network*

Le r√©seau neuronal enti√®rement connect√© est l‚Äôendroit o√π le bloc traite son token d‚Äôentr√©e apr√®s que l‚Äôauto-attention a inclus le contexte appropri√© dans sa repr√©sentation. Il est compos√© de **deux couches**.

La premi√®re couche est quatre fois plus grande que le mod√®le (768x4 =  3072). Cela semble donner aux mod√®les  de Transformer une capacit√© de repr√©sentation suffisante pour faire face aux t√¢ches qui leur ont √©t√© confi√©es jusqu‚Äô√† pr√©sent.

La deuxi√®me couche projette le r√©sultat de la premi√®re couche dans la dimension du mod√®le (768 pour le petit GPT2). **Le r√©sultat de cette multiplication est le r√©sultat du bloc Transformer pour ce token.**

##### r√©sum√©

Chaque bloc a **son propre jeu de ces poids**. D‚Äôautre part, le mod√®le n‚Äôa qu‚Äô**une seule matrice d‚Äôembedding** de token et **une seule matrice de codage positionnel** 



### [bert-base-uncased](https://huggingface.co/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France)



BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion.

it was pretrained with two objectives:

- **Masked language modeling** (MLM)
- **Next sentence prediction** (NSP)

This way, the model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard classifier using the features produced by the BERT model as inputs.

 **this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation you should look at model like GPT2.**

The inputs of the model are then of the form:

[CLS] Sentence A [SEP] Sentence B [SEP]

With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that **what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two "sentences" has a combined length of less than 512 tokens.**

The details of the masking procedure for each sentence are the following:

15% of the tokens are masked.
In 80% of the cases, the masked tokens are replaced by [MASK].
In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.
In the 10% remaining cases, the masked tokens are left as is.
