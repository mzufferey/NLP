### Some examples of applying BERT in specific domain](https://towardsdatascience.com/how-to-apply-bert-in-scientific-domain-2d9db0480bd9)

 SciBERT which is based on BERT to address the performance on scientific data. It uses a pre-trained model from BERT and fine-tune  contextualized embeddings by using scientific publications which  including 18% papers from computer science domain and 82% from the broad biomedical domain.

generic pretrained NLP model may not work very well in specific domain  data. Therefore, they fine-tuned BERT to be BioBERT and 0.51% ~ 9.61%  absolute improvement in biomedical‚Äôs NER, relation extraction and  question answering NLP tasks

Both SciBERT and BioBERT also introduce domain specific data for pre-training.

![img](https://miro.medium.com/max/560/1*MhNie2aPLIid3hq_uh0qqg.png)

Both SciBERT and BioBERT follow BERT model architecture which is **multi  bidirectional transformer** and learning text representation by predicting masked token and next sentence.

 A sequence of tokens will be transform to token embeddings, segment  embeddings and position embeddings. 

**Token embeddings** refers to  contextualized word embeddings;

**segment embeddings** only include 2  embeddings which are either 0 or 1 to represent first sentence and  second sentence

**position embeddings** stores the token position relative  to the sequence. 

### [How BERT leverage attention mechanism and transformer to learn word contextual relations](https://towardsdatascience.com/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb)

a new state-of-the-art NLP paper is released by Google. They call this  approach as BERT (Bidirectional Encoder Representations from  Transformers).

 **transformer** architecture to learn the text representations. 

BERT uses **bidirectional transformer** (both  left-to-right and right-to-left direction) rather than dictional  transformer (left-to-right direction). 

BERT uses **bidirectional language model** to learn the text representations 

BERT uses deep neural network

##### Input Representation

 3 embeddings to compute the input representations

1. token embeddings;  
   * general word embeddings
   * uses vector to represent token (or word) (see [here](https://towardsdatascience.com/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a) for detail)

2. segment embeddings 
   * sentence embeddings 
   * If input includes 2 sentence,  corresponding sentence embeddings will be assigned to particular words.  
   * If input only include 1 sentence, one and only one sentence embeddings will be used. 
   * learnt before computing BERT (see [here](https://towardsdatascience.com/transforming-text-to-sentence-embeddings-layer-via-some-thoughts-b77bed60822c) for detail)
3. position embeddings
   *  token sequence of input
   * even if there are 2 sentences, position will be accumulated.

‚ÄúCLS‚Äù is the reserved token to represent the start of sequence while ‚ÄúSEP‚Äù separate segment (or sentence)

##### Pre-Training Tasks

First pre-training task is **masked language model** while the second task is **predicting next sentence**.

*Masked language model* (masked LM)

bidirectional rather than traditional directional as a  pre-training objective

If using traditional approach to train a  bidirectional model, each word will able to see ‚Äúitself‚Äù indirectly

Masked Language Model (MLM) approach instead: by masking  some tokens randomly, using other token to predicted those masked token  to learn the representations

Unlike other approaches, predicts masked token rather than entire input.

the experiment picks 15% of token randomly to be replaced.

downsides:

1.  MASK token (actual  token will be replaced by this token) will never be seen in fine-tuning  stage and actual prediction. Therefore, Devlin et al, the selected token for masking will not alway be masked but
   * A: 80% of time, it will be replaced by [MASK] token
   * B: 10% of time, it will be replaced by other actual token
   * C: 10% of time, it will be keep as original.

2. only 15% token is masked (predicted) per batch, a longer time will take for training.

*Next Sentence Prediction*

 predict next sentence

overcome the issue of first task as it cannot learn the relationship between sentences

simple objective: only classifying whether second sentence is next sentence or not

##### Training the model

2 phases training: 1) using generic data set to perform first training; 2) fine tuning by providing domain specific data set.

*Pre-training phase*

sentences retrieved from BooksCorpus (800M words) and English Wikipedia (2500M words)

step done by Google research team and we can leverage this pre-trained model to further fine tuning model based on own data.

*Fine-tuning phase*

Only some model hyperparameters are changed such as batch size, learning rate and number of training epochs, most mode hyperparameters are kept as same in pre-training phase.

Fine-tuning procedure is different and it depends on downstream tasks.

* classification: 
  * [CLS] token feed as the final hidden state
  * Label  (C) probabilities computed with a softmax
  * fine-tuned to maximize the log-probability of the correct label.
* Named Entity Recognition: 
  * Final hidden representation of token feed into the  classification layer. 
  * Surrounding words considered on the  prediction. 
  * the classification only focus on the token  itself and no Conditional Random Field (CRF).

### [Transformers from scratch](http://peterbloem.nl/blog/transformers)

##### Self-attention

The fundamental operation of any transformer architecture is the *self-attention operation*.

Self-attention is a **sequence-to-sequence operation**: a sequence of vectors goes in, and a sequence of vectors comes out. 

To produce output vector ùê≤i, the self attention operation simply takes *a **weighted average over all the input vectors***

ùê≤i=‚àëjwijùê±j.

Where j indexes over the whole sequence and the weights sum to one over all j. The weight wij is not a parameter, as in a normal neural net, but it is *derived* from a function over ùê±i and ùê±j. The simplest option for this function is the dot product:

The dot product gives us a value anywhere between negative and positive infinity, so we apply a softmax to map the values to [0,1] and to ensure that they sum to 1 over the whole sequence:

wij=exp w‚Ä≤ij‚àëjexp w‚Ä≤ij

And that‚Äôs the basic operation of self attention.

this is **the only operation in the whole architecture that propagates information *between* vectors**.

Every other operation in the transformer is applied to each vector in the input sequence without interactions between vectors.

##### Understanding why self-attention works

example: customer movie features matching movie features, dot product to get the match between them; Annotating a database of millions of movies is very costly, and  annotating users with their likes and dislikes is pretty much impossible;  instead is that we make the movie features and user features ***parameters*** of the model. We then ask users for a small number of movies that they  like and **we optimize the user features and movie features so that their  dot product matches the known likes**

Even though we don‚Äôt tell the model what any of the features should mean, in practice, it turns out that after training the features do actually reflect meaningful semantics about the movie content

This is the basic principle at work in the self-attention

 Let‚Äôs say we are faced with a sequence of words. To apply self-attention, 

* **embedding layer**: 
  * assign each word t in our vocabulary an *embedding vector* ùêØt (the values of which we‚Äôll learn)
  * turns the word sequence into the vector sequence
* **self-attention layer**
  * input is the embedding vector, the output is another sequence of vectors 
  * which element of the output vector is a weighted sum over all the embedding vectors in the first sequence, weighted by their (normalized) dot-product with the element of the embedding vector

we are *learning* what the values in the embedding vector should be, how "related" two words are is entirely determined by the task.

The **dot product** expresses how related two vectors in the input sequence  are, with ‚Äú**related**‚Äù defined by the learning task, and the output vectors are **weighted sums over the whole input sequence**, with the **weights  determined by these dot products**.

following properties, which are unusual for a sequence-to-sequence operation

* There are **no parameters** 
* Self attention sees its input as a *set*, not a sequence. If we  permute the input sequence, the output sequence will be exactly the  same, except permuted also (i.e. **self-attention is *permutation  equivariant***); self-attention by itself actually ignores the sequential nature of the input

##### basic self-attention implementation

‚Äã			

```
import torch
import torch.nn.functional as F

# assume we have some tensor x with size (b, t, k)
x = ...

# The set of all raw dot products w‚Ä≤ij forms a matrix, which we can compute simply by multiplying ùêó by its transpose: 

raw_weights = torch.bmm(x, x.transpose(1, 2))
# - torch.bmm is a batched matrix multiplication. It 
#   applies matrix multiplication over batches of 
#   matrices

#  turn the raw weights w‚Ä≤ij into positive values that sum to one, we apply a *row-wise* softmax:
weights = F.softmax(raw_weights, dim=2)

# Finally, to compute the output sequence, we just multiply the weight matrix by ùêó. This results in a batch of output matrices ùêò of size `(b, t, k)` whose rows are weighted sums over the rows of ùêó.
y = torch.bmm(weights, x)
```

##### Additional tricks

The actual self-attention used in modern transformers relies on three additional tricks.

*1) Queries, keys and values*

Every input vector ùê±i is used in 3 different ways in the self attention operation:

1. compared to every other vector to establish the weights for its own output ùê≤i (**query**)
2. compared to every other vector to establish the weights for the output  of the j-th vector ùê≤j (**key**)
3. used as part of the weighted sum to compute each output vector once the weights have been established (**value**)

basic self-attention: each input vector must play all three roles

make easier: by deriving new  vectors for each role, by applying a linear transformation to the  original input vector. In other words, we add three k√ók weight matrices ùêñq, ùêñk,ùêñv and compute three linear transformations of each xi, for the three different parts of the self attention:

This gives the self-attention layer some controllable parameters, and allows it to modify the incoming vectors to suit the three roles they  must play.

*2) Scaling the dot product*

The softmax function can be sensitive to very large input values.  These kill the gradient, and slow down learning, or cause it to stop  altogether. Since the average value of the  dot product grows with the  embedding dimension k, it helps to scale the dot product back a little to stop the inputs to the softmax function from growing too large

3) Multi-head attention

account for the fact that a word can mean different things to different neighbours

ex: mary,gave,roses,to,susan -> different relations for gave. mary expresses who‚Äôs doing the giving, roses expresses what‚Äôs being given, and susan expresses who the recipient is.

In a single self-attention operation, all this information just gets summed together. If Susan gave Mary the roses instead, the output vector ùê≤gave would be the same, even though the meaning has changed.

give the self attention greater power of discrimination, by combining several self attention mechanisms (which we'll index with r), each with different matrices ùêñrq, ùêñrk,ùêñrv. These are called **attention heads**.

For input ùê±i each attention head produces a different output vector ùê≤ri. We **concatenate these, and pass them through a linear transformation** to reduce the dimension back to k.

<u>Efficient multi-head self-attention</u>. The simplest way to understand **multi-head self-attention** is to see it as a small number of copies of the self-attention mechanism applied in parallel, each with their own key, value and query transformation. This works well, but for R heads, the self-attention operation is R times as slow.

there is a way to implement multi-head self-attention so that it is roughly as fast as the single-head version, but we still get the benefit of having different attention matrices in parallel. To accomplish this, we **cut each incoming vector into chunks**: if the input vector has 256 dimensions, and we have 8 attention heads, we cut it into 8 chunks of 32 dimensions. For each chunk, we **generate keys, values and queries of 32 dimensions each**. This means that the matrices ùêñrq, ùêñrk,ùêñrv are all 32√ó32.

##### Building transformers

A transformer is not just a self-attention layer, it is an **architecture**. It‚Äôs not quite clear what does and doesn‚Äôt qualify as a transformer, but here we‚Äôll use the following definition:

a **transformer** = any architecture designed to **process a connected set of units**‚Äîsuch as the tokens in a sequence or the pixels in an image‚Äîwhere the only **interaction between units is through self-attention**. 

standard approach for how to build self-attention layers up into a larger network. 1st step: wrap the self-attention into a block that we can repeat.

*The transformer block*

some variations on how to build a basic transformer block, but most of them are structured roughly like this:

![img](http://peterbloem.nl/files/transformers/transformer-block.svg)

the block applies, in sequence: 

* a self attention layer
* layer normalization
* a feed forward layer (a single MLP applied independently to each vector)
* another layer normalization.
* residual connections are added around both, before the normalization. 

The order of the  various components is not set in stone; the important thing is to **combine self-attention with a local feedforward, and to add  normalization and residual connections**.

Normalization and residual connections are standard tricks used to help  deep neural networks train faster and more accurately. The layer  normalization is applied over the embedding dimension only.

*Classification transformer*

The simplest transformer = a sequence classifier. 

The heart of the architecture will simply be a large chain of transformer blocks. 

All we need to do is work out how to feed it the input sequences, and how to transform the final output sequence into a a single classification.



The most common way to build a **sequence classifier** out of  sequence-to-sequence layers, is to apply **global average pooling** to the  final output sequence, and to **map the result to a softmaxed class  vector**.

![img](http://peterbloem.nl/files/transformers/classifier.svg) Overview of a simple sequence classification transformer. **The output sequence is averaged to produce a single vector representing the whole sequence. This vector is projected down to a vector with one element per class and softmaxed  to produce probabilities**.



##### Input: using the positions

embedding layer to represent the words.

stacking permutation equivariant layers, and the final global average pooling is permutation *in*variant, so the network as a whole is also permutation invariant.

if we shuffle up the words in the sentence, we get the exact same classification, whatever weights we learn

but we want our state-of-the-art language model to have at least some sensitivity to word order,

solution: create a second vector of equal length, that represents the position of  the word in the current sentence, and add this to the word embedding; 2 options

1. **position embeddings**:  embed the positions like we did the words. 
   * drawback: we have to see sequences of every length during training, otherwise the relevant position embeddings don't get trained
   * benefit: works pretty well, and easy to implement
2. **position encodings**: work in the same way as embeddings, except that we don't *learn* the position vectors, we just choose some function to map the positions to real valued vectors, and let the network figure out how to interpret these encodings
   * benefit: for a well  chosen function, the network should be able to deal with sequences that  are longer than those it's seen during training (it's unlikely to  perform well on them, but at least we can check)
   * drawbacks: the choice of encoding function is a complicated hyperparameter, and it complicates the implementation a little.

##### Text generation transformer

try an **autoregressive model**; train a ***character* level transformer to predict the next character** in a sequence.

give the sequence-to-sequence model a sequence, and ask it to  predict the next character at each point in the sequence; the target output is the same sequence shifted one character to  the left:

![img](http://peterbloem.nl/files/transformers/generator.svg)

With RNNs this is all we need to do, since they cannot look forward into the input sequence: output i depends only on inputs 0 to i. 

**With a transformer, the output depends on the entire input sequence**, so prediction of the next character becomes vacuously easy, just retrieve  it from the input.

To use **self-attention as an autoregressive model**, need to  **ensure that it cannot look forward into the sequence**. We do this by  **applying a mask to the matrix of dot products**, before the softmax is  applied. This mask **disables all elements above the diagonal of the  matrix**.



##### Design considerations

The main point of the transformer was to overcome the problems of the previous state-of-the-art architecture, the RNN 

 big weakness here is the **recurrent connection**

*  this allows information to propagate along the sequence, 
* but means that we cannot compute the cell at time step i until we‚Äôve computed the cell at timestep i‚àí1.

contrasts with 1D convolution: 

* each vector can be computed in parallel with  every other output vector; much faster
* drawback: limited in modeling *long range dependencies*. In one convolution layer,  only words that are closer together than the kernel size can interact  with each other. For longer dependence we need to stack many  convolutions.

The transformer is an attempt to capture the best of both worlds.

* can model dependencies over the whole range of the input sequence  just as easily as they can for words that are next to each other (in  fact, without the position vectors, they can‚Äôt even tell the  difference)
* no recurrent connections, so the whole  model can be computed in a very efficient feedforward fashion.

The rest of the design of the transformer is based primarily on one  consideration: depth. Most choices follow from the desire to train big  stacks of transformer blocks. 

only  2  places in the transformer where non-linearities occur: 

1. the softmax in  the self-attention 
2. the ReLU in the feedforward layer. 

The rest of the model is entirely composed of linear transformations, which perfectly preserve the gradient.

layer normalization likely also nonlinear, but this nonlinearity helps to keep the gradient stable as it  propagates back down the network.

##### Why is it called self-*attention*?

Before self-attention, sequence models consisted  mostly of recurrent networks or convolutions stacked together. 

it was discovered that these models could be helped by adding ***attention mechanisms***: instead of feeding the output sequence of the previous layer directly  to the input of the next, **an intermediate mechanism was introduced, that decided which elements of the input were relevant for a particular word of the output**.

The general mechanism was as follows.

*  call the input the **values**
* some (trainable) mechanism assigns a **key** to each value
* then to each output, some other mechanism assigns a **query**.

These names derive from the datastructure of a **key-value store**. In  that case we expect only one item in our store to have a key that  matches the query, which is returned when the query is executed.  

Attention is a softened version of this: ***every* key in the store matches the query to some extent. All are returned, and we take a sum,  weighted by the extent to which each key matches the query**.

The great breakthrough of **self-attention** was that **attention by itself is a strong enough mechanism to do all the learning**; **The key, query and value are all the same vectors (with minor linear transformations).** They ***attend to themselves*** and **stacking such self-attention provides sufficient nonlinearity and  representational power** to learn very complicated functions.

##### BERT 

BERT was one of the first models to show that transformers could reach human-level performance on a variety of language based tasks: question answering, sentiment classification or classifying whether two sentences naturally follow one another.

BERT consists of a simple **stack of transformer blocks**

pre-trained on a large general-domain corpus consisting of 800M words from English books (modern work, from unpublished authors), and 2.5B words of text from English Wikipedia articles (without markup).

**Pretraining** is done through two tasks:

1. **Masking**
   * A certain number of  words in the input sequence are: masked out, replaced with a random word or kept as is. 
   * The model is then asked to predict, for these words, what the original words were
   * **the model doesn't need to predict the entire denoised sentence, just the modified words**
   * Since the model doesn't know which words it will be asked about, it learns a representation for every word in the sequence.
2. Next sequence classification
   * 2 sequences of about 256 words are sampled that either (a) follow each other directly in the corpus, or (b) are both taken from random places. 
   * The model must then predict whether a or b is the case.

BERT uses **WordPiece tokenization**

* somewhere in **between word-level and character level sequences.** 
* breaks words like walking up into the tokens walk and ##ing -> allows the model to make some inferences based on word structure: two verbs ending in -ing have similar grammatical functions, and two verbs starting with walk- have similar semantic function.

The input is **prepended with a special <cls> token**. **The output vector corresponding to this token is used as a sentence representation in sequence classification tasks** like the next sentence classification (as opposed to the global average pooling over all vectors that we used in our classification model above).

After pretraining, a **single task-specific layer** is placed after the body of transformer blocks, which maps the general purpose representation to a task specific output. 

For classification tasks, this simply maps the first output token to softmax probabilities over the classes. 

For more complex tasks, a final sequence-to-sequence layer is designed specifically for the task.

The whole model is then re-trained to finetune the model for the specific task at hand.

In an ablation experiment, the authors show that **the largest improvement as compared to previous models comes from the bidirectional nature of BERT**. 

* previous models like GPT used an **autoregressive mask**, which allowed attention only over previous tokens. 
* in BERT **all attention is over the whole sequence** is the main cause of the improved performance. This is why the B in BERT stands for "bidirectional".

The largest BERT model uses 24 transformer blocks, an embedding dimension of 1024 and 16 attention heads, resulting in 340M parameters.

##### Sparse transformers

 tackle the problem of quadratic memory use  head-on. I**nstead of computing a dense matrix of attention weights (which grows quadratically), they compute the self-attention only for  particular pairs of input tokens**, resulting in a ***sparse* attention matrix**, with only n*‚àön explicit elements.

* benefit: 
  * allows models with very large context sizes, for instance for  generative modeling over images, with large dependencies between pixels; allow to train transformers with very large  sequence lengths
  *  also allows a very elegant way  of designing an inductive bias. We take our input as a collection of  units (words, characters, pixels in an image, nodes in a graph) and we  specify, through the sparsity of the attention matrix, which units we  believe to be related. The rest is just a matter of building the  transformer up as deep as it will go and seeing if it trains.
* tradeoff: sparsity structure is not learned, so by the  choice of sparse matrix, we are disabling some interactions between  input tokens that might otherwise have been useful. However, two units  that are not directly related may still interact in higher layers of the transformer (similar to the way a convolutional net builds up a larger  receptive field with more



### [Fleuret NLP lectures](https://fleuret.org/dlc/)

A common word embedding is the Continuous Bag of Words (CBOW) version
of **word2vec** (Mikolov et al., 2013a).
In this model, the embedding vectors are chosen so that a word can be
[linearly] predicted from the sum of the embeddings of words around it.

An alternative algorithm is the **skip-gram model**, which optimizes the
embedding so that a word can be predicted by any individual word in its context
(Mikolov et al., 2013a).

The main benefit of word embeddings is that they are trained with unsupervised
corpora, hence possibly extremely large.

in all the operations we have seen such as fully connected layers, convolutions,
or poolings, **the contribution of a value in the input tensor to a value in the**
**output tensor is entirely driven by their [relative] locations [in the tensor].**

**Attention mechanisms** aggregate features with an importance score that

* depends on the feature themselves, not on their positions in the tensor,
* relax locality constraints

Attention mechanisms **modulate dynamically the weighting of different parts of**
**a signal and allow the representation and allocation of information channels to**
**be dependent on the activations themselves**.

While they were developed to equip deep-learning models with memory-like
modules (Graves et al., 2014), their main use now is to **provide long-term**
**dependency for sequence-to-sequence translation** (Vaswani et al., 2017).



##### attention mechanisms

the simplest form of attention is **content-based attention**.

which differs from **context attention**, 

the most classical version of attention is a **context-attention with a dot-product**
**for attention function**

 using the terminology of Graves et al. (2014), attention is an **averaging of**
**values associated to keys matching a query**. Hence the keys used for
computing attention and the values to average are different quantities

attention layer to equip the model with the **ability to combine**
**information from parts of the signal that it actively identifies as relevant**.



##### Transformer networks

Vaswani et al. (2017) proposed to go one step further: instead of using
attention mechanisms as a supplement to standard convolutional and recurrent
operations, they designed **a model combining only attention layers**.

They designed this ‚Äú**transformer**‚Äù for a sequence-to-sequence translation task,
but it is currently key to state-of-the-art approaches across NLP tasks

they first introduce a multi-head attention module

Their complete model is composed of:

* An **encoder** that combines N = 6 modules each composed of a **multi-head**
  **attention sub-module**, and a **[per-component] one hidden-layer MLP**, with
  residual pass-through and layer normalization.
* A **decoder** with a similar structure, but with **causal attention layers** to allow
  for regression training, and **additional attention layers** that attend to the
  layers of the encoder.

Positional information is provided through an **additive positional encoding**

The **Universal Transformer** (Dehghani et al., 2018) is a similar model where **all**
**the blocks are identical**, resulting in a recurrent model that iterates over
consecutive revisions of the representation instead of positions.
Additionally the **number of steps is modulated per position dynamically**.

transformer networks were introduced for translation, and trained with a
supervised procedure, from pairs of sentences

 they can be trained in a unsupervised
manner, for auto-regression or as denoising auto-encoders, from very large
data-sets, and fine-tuned on supervised tasks with small data-sets

BERT (Bidirectional Encoder Representation from Transformers) is a transformer pre-trained with:

* **Masked Language Model** (MLM), that consists in predicting [15% of]
  words which have been replaced with a ‚ÄúMASK‚Äù token.
* **Next Sentence Prediction** (NSP), which consists in predicting if a certain
  sentence follows the current one.

It is then fine-tuned on multiple NLP tasks.

##### Attention in computer vision

They insert ‚Äúnon-local blocks‚Äù in residual architectures and get improvements
on both video and images classification.

### [BERT Explained: A Complete Guide with Theory and Tutorial](https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial/)

**pre-training** = raining general purpose language representation models using the  enormous piles of unannotated text on the web 

These general purpose pre-trained models can then be ***fine-tuned\*** on smaller task-specific datasets, e.g., when working with problems like question answering and sentiment analysis.

BERT is a recent addition to these techniques for NLP pre-training

we can either use the BERT models to extract high quality language  features from our text data, or we can fine-tune these models on a  specific task

In the pre-BERT world, **one-directional approach** works well for generating sentences ‚Äî we can  predict the next word, append that to the sequence, then predict the  next to next word until we have a complete sentence.

 BERT, a language model which is **bidirectionally trained** (this is also its key technical innovation). This means we can now have a  deeper sense of language context and flow compared to the  single-direction language models.

Instead of predicting the next word in a sequence, BERT makes use of a novel technique called **Masked LM** (MLM): it randomly masks words in the sentence and then it tries to predict them. 

**Masking**  means that the model looks in both directions and it uses the full  context of the sentence, both left and right surroundings, in order to  predict the masked word. 

Unlike the previous language models, it takes  **both the previous and next tokens into account** at the **same time.** 

The existing combined left-to-right and right-to-left LSTM based models  were missing this ‚Äúsame-time part‚Äù. (It might be more accurate to say  that BERT is non-directional though.)

Pre-trained language representations can either be

* **context-free** (e.g. word2vec)
  * generate a single word embedding representation (a vector of numbers) for each word in the vocabulary.
  * e.g.  ‚Äúbank‚Äù would have the same context-free representation in ‚Äúbank account‚Äù and ‚Äúbank of the river"
* **context-based**
  * generate a representation of each word that is based on the other words in the sentence
  * **unidirectional** 
    * e.g.  in  ‚ÄúI accessed the bank account,‚Äù represent ‚Äúbank‚Äù based on ‚ÄúI accessed the‚Äù 
  * *bidirectional ** (e.g. BERT)
    * e.g. represents ‚Äúbank‚Äù using both its previous and next context ‚Äî ‚ÄúI accessed the ‚Ä¶ account‚Äù

Moreover, BERT is based on the **[Transformer model architecture](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html),** instead of LSTMs. 

A **Transformer** works by performing a small, constant number of steps. 

* In  each step, it applies an **attention mechanism** to understand relationships between all words in a sentence, regardless of their respective  position. 
* For example, given the sentence, ‚ÄúI arrived at the bank after crossing the river‚Äù, to determine that the word ‚Äúbank‚Äù refers to the  shore of a river and not a financial institution, the Transformer can  **learn to immediately pay attention to the word ‚Äúriver‚Äù and make this  decision in just one step.**

BERT relies on a **Transformer** (the **attention mechanism that learns contextual relationships between words** in a text). A basic Transformer consists of an **encoder** to read the text input and a **decoder** to produce a prediction for the task. 

Since **BERT‚Äôs goal is to generate a language  representation model**, it only needs the **encoder** part. 

The **input** to the  encoder for BERT is a **sequence of tokens**, which are first converted into **vectors** and then processed in the neural network. But before processing can start, BERT needs the input to be massaged and decorated with some  extra metadata:

1.  **Token embeddings**:
   * A [CLS] token is added to the input word tokens **at the beginning of the first sentence** 
   * a [SEP] token is inserted at the end of each  sentence.
2. **Segment embeddings**: 
   * A marker **indicating Sentence A or Sentence B** is added to each token. This allows the encoder to distinguish between sentences.
3. **Positional embeddings**: 
   * A positional embedding is added to each token to **indicate its position in the sentence**.

Essentially, t**he Transformer stacks a layer that maps sequences to sequences**, so the output is also a sequence of vectors with a 1:1  correspondence between input and output tokens at the same index. 

BERT does not try to predict the next word in the  sentence. 

Training makes use of the following two strategies:

**1. Masked LM (MLM)**

Procedure:

* Randomly mask out 15% of the words in the  input ‚Äî replacing them with a [MASK] token 
* run the entire sequence  through the BERT attention based encoder and then predict only the  masked words, based on the context provided by the other non-masked  words in the sequence. 

problem with this naive  masking approach: the model only tries to predict when the [MASK]  token is present in the input, while we want the model to try to predict the correct tokens regardless of what token is present in the input. To deal with this issue, out of the 15% of the tokens selected for  masking:

- 80% of the tokens actually replaced with the token [MASK].
- 10% replaced with a random token.
- 10% left unchanged.

While training the BERT loss function **considers only the prediction of the  masked tokens** and ignores the prediction of the non-masked ones. This  results in a model that **converges much more slowly** than left-to-right or right-to-left models.

2. **Next Sentence Prediction (NSP)**

In order to **understand relationship between two sentences**, BERT training process also uses next sentence prediction. 

A pre-trained model with this kind of understanding is **relevant for tasks like question answering**. 

During training the model gets as i**nput pairs of sentences and it learns to predict if the second sentence is the next sentence in the original text** as well.

As we have seen earlier, BERT separates sentences with a special [SEP] token. During training the model is fed with two input sentences at a time such that:

* 50% of the time the second sentence comes after the first one.
* 50% of the time it is a a random sentence from the full corpus.

BERT is then required to **predict whether the second sentence is random or not**, with the assumption that the random sentence will be disconnected from the first sentence (predict label "IsNext" or "NotNext")

To predict if the second sentence is connected to the first one or not,  basically the complete input sequence goes through the Transformer based model, the output of the [CLS] token is transformed into a 2√ó1 shaped  vector using a simple classification layer, and the IsNext-Label is  assigned using softmax.

The model is **trained with both Masked LM and Next Sentence Prediction  together**. This is to **minimize the combined loss function** of the two  strategies ‚Äî *‚Äútogether is better‚Äù*.

##### Architecture

There are **four types of pre-trained versions of BERT** depending on the scale of the model architecture:

**`BERT-Base`**: 12-layer, 768-hidden-nodes, 12-attention-heads, 110M parameters
 **`BERT-Large`**: 24-layer, 1024-hidden-nodes, 16-attention-heads, 340M parameters


if we want to fine-tune the original model based on our own dataset, we  can do so by just adding a single layer on top of the core model.

For example, say we are creating **a question answering application**. In essence question answering is just a **prediction task** ‚Äî on receiving a question as input, the goal of the application is to identify the  right answer from some corpus. 

So, given a question and a context  paragraph, **the model predicts a start and an end token from the  paragraph that most likely answers the question** -> BERT can be trained by learning **two extra  vectors that mark the beginning and the end of the answer**.

Just like sentence pair tasks, **the question becomes the first  sentence and paragraph the second sentence in the input sequence**.  However, this time there are two new parameters learned during  fine-tuning: a **start vector** and an **end vector.**

in case we want to do fine-tuning, we need to **transform our input into  the specific format** that was used for pre-training the core BERT models, e.g., we would need to add special tokens to mark the beginning ([CLS]) and separation/end of sentences ([SEP]) and segment IDs used to  distinguish different sentences ‚Äî convert the data into features that  BERT uses.

, we can also do custom **fine tuning** by **creating a single new layer** **trained to adapt BERT** to our sentiment task (or any other task). 

we need to preprocess our data so that it matches the data  BERT was trained on. For this, we'll need to do a couple of things (but  don't worry--this is also included in the Python library):

* Lowercase our text (if we're using a BERT lowercase model)

* Tokenize it (i.e. "sally says hi" -> ["sally", "says", "hi"]) 
* Break words into WordPieces (i.e. "calling" -> ["call", "##ing"]) 
* Map our words to indexes using a vocab file that BERT provides 
* Add special "CLS" and "SEP" tokens (see the [readme](https://github.com/google-research/bert)) 
* Append "index" and "segment" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf)) 

First, it loads the BERT tf hub module again (this time to extract the computation graph). Next, it creates a single new layer that will be trained to adapt BERT to our sentiment task (i.e. classifying whether a movie review is positive or negative). This strategy of using a mostly trained model is called fine-tuning.

### [BERT : Le "Transformer model" qui s‚Äôentra√Æne et qui repr√©sente](https://lesdieuxducode.com/blog/2019/4/bert--le-transformer-model-qui-sentraine-et-qui-represente)

**BERT** c'est pour **B**idirectional **E**ncoder **R**epresentations from **T**ransformers. Il est sorti des labos [Google AI](https://ai.google/) fin 2018, et s'il est ce jour l'objet de notre attention c'est que son mod√®le est √† la fois :

- Plus performant que ses pr√©d√©cesseurs en terme de [r√©sultats](https://rajpurkar.github.io/SQuAD-explorer/).
- Plus performant que ses pr√©d√©cesseurs en terme de rapidit√© d'apprentissage.
- Une fois **pr√©-entra√Æn√©, de fa√ßon non supervis√©e** (initialement avec avec tout - absolument tout - le corpus anglophone de Wikipedia),  il poss√®de une "repr√©sentation" linguistique qui lui est propre. Il est  ensuite possible, sur la base de cette repr√©sentation initiale, de le  customiser pour une t√¢che particuli√®re. Il peut √™tre **entra√Æn√© en mode incr√©mental (de fa√ßon supervis√©e cette fois)** pour sp√©cialiser le mod√®le rapidement et avec peu de donn√©es.
- Enfin il peut fonctionner de fa√ßon **multi-mod√®le**, en prenant en entr√©e des  donn√©es de diff√©rents types comme des images ou/et du texte, moyennant  quelques manipulations.

Il a l'avantage par rapport √† ses concurrents Open AI GTP (GPT-2 est ici pour ceux que √ßa int√©resse) et ELMo d'√™tre **bidirectionnel**, il n'est pas oblig√© de ne regarder qu'en arri√®re comme OpenAI GPT ou de concat√©ner la vue "arri√®re" et la vue "avant" entra√Æn√©es ind√©pendamment comme pour ELMo.

Pour faire du "sequence to sequence", i.e. de la traduction simultan√©e, ou du text-to-speech, ou encore du speech-to-text, l'√©tat de l'art jusque ces derni√®res ann√©es, c'√©tait les **RNNs**, nourris avec des s√©quences de word-embeddings , et parfois quelques couches de convolution sur les s√©quences d'entr√©e pour en extraire des caract√©ristiques (features) plus ou moins fines (fonction du nombre de couches), afin d'acc√©l√©rer les calculs, avant de passer les infos au RNN.

Notez que **BERT utilise des embeddings sur des morceaux de mots**. Donc ni des embeddings niveau caract√®re, ni des embeddings au niveau de chaque mot, mais un interm√©diaire.


Contrairement aux r√©seaux de neurones "classiques" (FFN pour feed-forward neural networks), qui connectent des couches de neurones formels les unes √† la suite des autres, avec pour chaque couche sa propre matrice de poids "entra√Ænable" - c'est √† dire dont on peut modifier les poids petit √† petit lors de l'apprentissage - et qui prennent en entr√©e des fourn√©es (batchs) de donn√©es, **les RNNs traitent des s√©quences, qu'ils parcourent pas √† pas, avec une m√™me matrice de poids**. Pour cette raison (le pas-√†-pas), dans le cadre des RNNs, **on ne peut pas parall√©liser les calculs.**

Les s√©quences sont des objets dont chaque √©l√©ment poss√®de un ordre, une position, une inscription dans le temps. Par exemple dans une phrase, chaque mot vient dans un certain ordre, est prononc√© sur un intervalle de temps distinct de celui des autres.


Nous allons les repr√©senter de fa√ßon d√©roul√©e pour des raisons de lisibilit√©, mais en r√©alit√© il s'agit d'**it√©rer dans l'ordre sur chaque √©l√©ment d'une s√©quence**. Il n'y a qu'**une seule matrice de poids, ou "les poids sont partag√©s dans le temps"** si vous pr√©f√©rez. Ci-dessous la repr√©sentation r√©elle et la repr√©sentation d√©roul√©e.

Une seule matrice de poids aide √† traiter de **s√©quences de longueurs  diff√©rentes** 

2 s√©quences diff√©rentes peuvent avoir un  sens tr√®s similaire; dans ce cas une seule  matrice de poids permet de partager les m√™me param√®tres √† chaque √©tape  de traitement, et par cons√©quence de donner un r√©sultat global assez  similaire en sortie pour deux phrase ayant le m√™me sens, bien qu'ayant  une composition diff√©rente.

Les FFNs sont soumis √†  l'explosion ou √† l'√©vanouissement du gradient (**exploding/vanishing  gradient descent**), et ce ph√©nom√®ne est d'autant plus prononc√© qu'on  traite de longue s√©quences ou qu'on les approfondis. 

les RNNs y sont soumis quand on augmente le nombre d'√©tapes de  traitement, mais de fa√ßon aggrav√©e par rapport aux FFNs. En effet il  peut y avoir des dizaines/centaines de mots dans une phrase, et **rarement autant de couches dans un FFN**. De plus, **dans un FFN, chaque couche  poss√®de sa propre matrice de poids et ses propres fonctions  d'activation. Les matrices peuvent parfois se contre-balancer les unes  les autres et "compenser"** ce ph√©nom√®ne. **Dans un RNN avec une seule  matrice les probl√®mes de gradient sont plus prononc√©s.**

*Note* : Il peut y avoir plusieurs couches dans un RNN, auquel cas chaque  couche aura sa propre matrice de poids. Entendez par l√† qu'en sus du  nombre d'√©tapes de traitement, un RNN peut aussi √™tre un r√©seau  "profond". On peut empiler plusieurs couches de RNN connect√©es entre  elles.

Afin de conserver la m√©moire du contexte, et  d'att√©nuer les probl√®mes de descente de gradient, les RNNs sont  g√©n√©ralement compos√©s d'unit√©s un peu particuli√®res, les **LSTMs** (long  short term memory), ou d'une de leur variantes les **GRUs** (gated recurrent units). Il existe en sus d'autres techniques pour les soucis de  gradient dans les RNNs - gradient clipping quand √ßa explose, sauts de  connexion, i.e. additive/concatenative skip/residual connexion quand √ßa  "vanishe" etc... 

**les LSTMs sont construites pour m√©moriser une partie des autres √©l√©ments d'une s√©quence**, et sont donc bien adapt√©es √† des t√¢ches traitant d'objets **dont les  √©l√©ments ont des d√©pendances entre eux** √† plus ou moins long terme, comme la traduction simultan√©e, qui ne peut pas se faire mot √† mot, sans le  contexte de ce qui a √©t√© prononc√© avant.

 pour faire du "sequence to sequence", la topologie utilis√©e  classiquement a un encodeur, suivi d'un d√©codeur.

* La sortie de l'encodeur est une suite de chiffres qui repr√©sente le  sens de la phrase d'entr√©e
* Pass√©e au d√©codeur, celui-ci va g√©n√©rer un  mot apr√®s l'autre, jusqu'√† tomber sur un √©l√©ment qui signifie "fin de la phrase".

vecteur qui contient tout le sens de la phrase en sortie de  l'encodeur. Il est souvent appel√© "**context vector**", vecteur contexte.



##### Les m√©canismes d'attention

Les m√©canismes  d'attention = les moyens de **faire passer au d√©codeur l'information de quelles √©tapes de l'encodeur (i.e. quels mots de la s√©quence d'entr√©e)  sont les plus importantes au moment de g√©n√©rer un mot de sortie**. Quels  sont les mots de la s√©quence d'entr√©e qui se rapportent le plus au mot  qu'il est en train de g√©n√©rer en sortie, soit qu'ils s'y rapportent  comme contexte pour lui donner un sens, soit qu'ils s'y rapportent comme mots "cousins" de signification proche.

**Les m√©canismes d'auto-attention** (self-attention) sont similaires, sauf qu'**au lieu de s'op√©rer entre les √©l√©ments de l'encodeur et du d√©codeur**, ils **s'op√®rent sur les √©l√©ments de l'input entre eux** (le pr√©sent regarde le pass√© et le futur) **et de l'output entre eux** aussi (le pr√©sent regarde le pass√©, vu que le futur est encore √† g√©n√©rer).

Par exemple au moment de g√©n√©rer "feel", c'est en fait l'ensemble "avoir la p√™che" qui a du sens, il doit faire attention √† tous les mots, et idem  quand il g√©n√®re "great", car il s'agit de la traduction d'une expression idiomatique.

**convolution**: souvent int√©gr√©e aux RNNs; r√©pond - en partie - √† un objectif similaire, fournit un "contexte" √† une suite de mots. On peut en effet passer la s√©quence d'entr√©e dans une ou plusieurs couches de convolution, avant de la passer dans l'encodeur. 

Les produits de convolution vont alors extraire des caract√©ristiques contextuelles entre mots se trouvant √† proximit√© les uns des autres, exacerber le poids de certains mots, et att√©nuer le poids de certains autres mots, et ceci de fa√ßon tr√®s positionnelle. 

**La convolution permet couche apr√®s couche d'extraire des caract√©ristiques (features) spatiales (dans diverses zones de la phrase), de plus en plus fines au fur et √† mesure que l'on plonge plus profond dans le r√©seau**.

Par analogie avec la convolution sur les images, pour appliquer la convolution √† une phrase on peut par exemple √† mettre un mot par ligne, chaque ligne √©tant le vecteur d'embeddings du mot correspondant. Chaque case du tableau de chiffres que cela produit √©tant alors l'√©quivalent de l'intensit√© lumineuse d'un pixel pour une image.

Les CNN (convolutional neural networks) ont aussi l'avantage que les calculs **peuvent se faire en parall√®le** (O(n/k) vs. O(n) pour un RNN), qu'on peut les utiliser pour concentrer l'information, et par cons√©quent **diminuer les probl√®mes de d'explosion/√©vanouissement du gradient**, utile s'il s'agit, par exemple, de g√©n√©rer une texte de plus de 1000 mots. J(e.g. bons r√©sultats avec WaveNet et ByteNet)

Ce qui sortira du CNN sera une info du type : "les mots 8, 11 et 23 sont tr√®s importants pour donner le sens exact de cette phrase, de plus il faudra combiner ou corr√©ler 8 et 11, retiens √ßa au moment de d√©coder la phrase". √Ä chaque √©tape on d√©cide de conserver tel ou tel mot (concentrant ainsi l'information) qui est cens√© avoir de l'importance au moment de g√©n√©rer le mot suivant. Ceci-dit, **c'est tr√®s positionnel**. √áa d√©pend beaucoup de la position des mots dans la phrase et de leur position les uns par rapport aux autres pour cr√©er un contexte, plus que de leur similarit√© de contexte s√©mantique.

! diff√©rence fondamentale avec **l'attention, qui ne va pas regarder dans quelle position se trouvent les mots, mais plut√¥t quels sont les mots les plus "similaires" entre eux**. Comme la notion de "position" d'un √©l√©ment dans une s√©quence reste tr√®s importante, les m√©canismes d'attention nous obligeront √† passer cette notion par d'autres moyens, mais ils ont **l'avantage de pouvoir faire des liens entre des √©l√©ments tr√®s distants d'une s√©quence, de fa√ßon plus l√©g√®re qu'avec un produit de convolution,** et d'une fa√ßon plus naturelle (sans se baser sur la position mais plut√¥t **en comparant les similarit√©s entre les √©l√©ments**).


Les RNNs avec LSTMs ont en r√©alit√© d√©j√† leurs m√©canismes d'attention, il existe plusieurs fa√ßons de faire; 3 grandes familles 

1. **attention stricte** (hard attention), qui **focalise sur un seul √©l√©ment** du contexte  et qui est **stochastique** (pas de notion de d√©riv√©e, la s√©lection de  l'√©l√©ment "√©lu" et la r√©tro-propagation se font via des m√©thodes  statistiques, √©chantillonnages, distributions)

- **attention  locale** qui ne s√©lectionne que **quelques √©l√©ments proches** sur lesquels  porter l'attention; hybride entre la soft et la hard attention.
- **attention "molle"** (soft attention), la plus classique



**R√©solution de co-r√©f√©rences - Sch√©mas de Winograd**

li√© aux m√©canismes d'attention; aucun autre mod√®le que BERT ne donne de bonnes performances sur ce point

La **co-r√©f√©rence** c'est lorsque √©l√©ment en r√©f√©rence un autre mais de  fa√ßon suffisamment ambigu√´ pour qu'il faille une compr√©hension fine de  la phrase pour comprendre ce qu'il r√©f√©rence.

*Exemple* : Je ne peux pas garer ma voiture sur cette place parce-qu‚Äôelle est  trop petite. <- Ici le pronom personnel "elle" renvoie √† la place de  parking, pas √† la voiture. Il faut une compr√©hension assez fine de la  phrase pour le comprendre. BERT y arrive tr√®s bien.

avec l'**attention Multi-t√™tes** chaque t√™te "voit" des choses que les autres ne voient pas et  ce "coll√®ge" se compl√®te bien.



##### Architecture

BERT n'utilise qu'une partie de l'architecture Transformer. Comme son nom l'indique (Bidirectional Encoder Representations from Transformers) **Bert n'est compos√© que d'un empilement de blocs type "Encodeur" sans "D√©codeur"**. Il y a aussi des mod√®les comme GPT-2 compos√©s de couches "D√©codeur"  seulement et plus sp√©cialis√©s pour de la g√©n√©ration de texte. 

architecture pr√©sent√©e dans l'article original

- 6 **encodeurs** empil√©s (le Nx du sch√©ma), 
  - chaque encodeur prenant en entr√©e la sortie de l'encodeur pr√©c√©dent (sauf le premier qui prend  en entr√©e les embeddings),
- suivi de 6 **d√©codeurs** empil√©s
  - prenant en  entr√©e la sortie du d√©codeur pr√©c√©dent et la sortie du dernier encodeur  (sauf pour le premier d√©codeur qui ne prend en entr√©e que la sortie du  dernier d√©codeur

Les 12 blocs (ou 24 selon les versions de BERT) **ne partagent pas les m√™mes matrices de poids**.

Chaque **encodeur** 

* se compose de 2 sous-couches : 

1. une couche  d'**auto-attention "multi-t√™tes"** 
2. suivie d'un **FFN compl√®tement connect√© et position-wise** (i.e. chaque √©l√©ment du vecteur de sortie de la couche  pr√©c√©dente est connect√© √† un neurone formel de l'entr√©e du FFN, dans le  m√™me ordre qu'ils le sont dans le vecteur). 

* Chaque sous-couche poss√®de  en sortie **une couche qui ajoute, additionne, les sorties de la couche et du raccord √† une connexion dite r√©siduelle** (qui connecte directement  les valeurs d'entr√©e de la couche √† la sortie de la couche) et qui  **normalise** l'ensemble.

Chaque **d√©codeur** 

* se compose de 3 couches : 
  1. une couche d'**auto-attention "multi-t√™tes",** 
  2. suivie d'une couche d'**attention avec le dernier encodeur**, 
  3. puis un **FFN compl√®tement  connect√© et position-wise** (i.e. chaque √©l√©ment du vecteur de sortie de  la couche pr√©c√©dente est connect√© √† un neurone formel de l'entr√©e du  FFN, dans le m√™me ordre qu'ils le sont dans le vecteur). 
* Chaque  sous-couche poss√®de **en sortie une couche qui ajoute, additionne, les  sorties de la couche et du raccord √† une connexion dite r√©siduelle** (qui  connecte directement les valeurs d'entr√©e de la couche √† la sortie de la couche) et qui **normalise** l'ensemble.



**3 m√©canismes  d'attention type "cl√©-valeur"** 

1. **Auto-attention** (dans l'encodeur)
2. **Auto-attention avec les tous  √©l√©ments pr√©c√©demment g√©n√©r√©e** (en entr√©e du d√©codeur)
3. **Attention "masqu√©e"** (dans le d√©codeur) (masked attention, parce-qu‚Äôon applique un masque) nous verrons les d√©tails plus loin dans l'article) entre  **l'√©l√©ment √† g√©n√©rer dans le d√©codeur et tous les √©l√©ments de l'encodeur**. 

Les couches d'attention ont plusieurs t√™tes

Pour la g√©n√©ration de texte ou la traduction simultan√©e le **m√©canisme est auto-r√©gressif**, c'est √† dire qu'on fait entrer une s√©quence dans le premier encodeur,  la sortie pr√©dit un √©l√©ment, puis on repasse toute la s√©quence dans  l'encodeur et l'√©l√©ment pr√©dit dans le d√©codeur en parall√®le afin de  g√©n√©r√©e un deuxi√®me √©l√©ment, puis √† nouveau la s√©quence dans l'encodeur  et tous les √©l√©ments d√©j√† pr√©dits dans le d√©codeur en parall√®le etc...  jusqu'√† pr√©dire en sortie un <fin de s√©quence>.

En sortie une distribution de probabilit√© qui permet de pr√©dire l'√©l√©ment de sortie le plus probable.

Et le tout **se passe compl√®tement de LSTMs !** 



##### Comment BERT apprend

de fa√ßon non supervis√©e, l'entr√©e se suffit √† elle m√™me, pas besoin de labelliser

**Masked language model**

[CLS] indique un d√©but de s√©quence

[SEP] une s√©paration, en g√©n√©ral entre deux phrases dans notre cas.

[MASK] un mot masqu√©

<u>Les mots "masqu√©s"</u>


Ici la s√©quence d'entr√©e a √©t√© volontairement oblit√©r√©e d'un mot, le mot masqu√©, et **le mod√®le va apprendre √† pr√©dire ce mot masqu√©.**

<u>La phrase suivante</u>


Ici le mod√®le doit **d√©terminer si la s√©quence suivante (suivant la s√©paration[SEP]) est bien la s√©quence suivante**. Si oui, IsNext sera vrai, le label sera IsNext, si non, le label sera NotNext.



##### 	Les customisations possibles

une fois son  apprentissage non supervis√© termin√©, il est :

* capable de se sp√©cialiser sur beaucoup de t√¢ches diff√©rentes  (traduction, r√©ponse √† des questions etc.)

*  surclasse dans la plus part de ses sp√©cialisations les mod√®les sp√©cialis√©s existants



##### 	Les sous-parties du Transformer en d√©tails

exemple pour la g√©n√©ration de texte

<u>en entr√©e</u>

* embeddings : chaque **mot** est repr√©sent√© par un vecteur (colonne ou ligne de r√©els)
* On ajoute √©ventuellement √† ces embeddings, pour chaque mot, les embeddings d'un "**segment**" quand cela a du sens (par exemple chaque phrase est un  segment et on veut passer plusieurs phrases √† la fois, on va alors dire  dans quel segment se trouve chaque mot).
* On ajoute  ensuite le "**positional encoding**", qui est une fa√ßon d'encoder la place  de chaque √©l√©ment **dans la s√©quence**. 
  * Comme la longueur des phrases n'est  pas pr√©d√©termin√©e, on va utiliser des fonctions sinuso√Ødales donnant de  petites valeurs entre 0 et 1, pour modifier l√©g√®rement les embeddings de chaque mot. 
  * La dimension de l'embedding de position (√† sommer avec  l'embedding s√©mantique du mot) est la m√™me que celle de l'embedding  s√©mantique, soit 512, pour pouvoir sommer terme √† terme.
  * il existe beaucoup de fa√ßons d'encoder la position d'un √©l√©ment dans une s√©quence.



Ce qui donne en entr√©e une matrice de taille [longueur de la s√©quence] x [dimension des embeddings - 512] 



##### 		L'attention dans le cas sp√©cifique du Transformer



m√©canisme d'attention (auto-attention ou pas) de **type cl√©-valeur**.



Chaque mot, d√©crit comme la **somme de ses embeddings s√©mantiques et positionnels** va √™tre d√©compos√© en **trois abstractions** :

1. Q = Une **requ√™te** (query)
2. K = Une **cl√©** (key)
3. V = Une **valeur** (value)

Dans l'exemple, chacune de ces abstractions est ici un vecteur de dimension 64. Comme on veut des sorties de dimension [longueur de la s√©quence] x [dimension des embeddings - 512] tout au long du  parcours, et que dans l'attention on fera du multi-t√™tes (voir plus  loin) avec 8 t√™tes, qu'on concat√©nera la sortie de chaque t√™te, on aura  alors 8 x 64 = 512 en sortie de l'attention et c'est bien ce qu'on veut.

**Chacune de ces abstractions est calcul√©e et apprise** (mise √† jour des matrices  de poids) lors du processus d'apprentissage, gr√¢ce √† une matrice de  poids. Chaque matrice est distincte.

Les dimensions de ces matrices sont [64 (dimension requ√™te ou cl√© ou valeur)] x [longueur de la s√©quence].



**Multi-t√™tes** :

*  **Chaque t√™te de l'attention a ses propres matrices de poids** 
* on **concat√®ne la sortie de chaque t√™te** pour retrouver une matrice de  dimension [longueur de la s√©quence] x [dimension des embeddings, i.e.  512].

explication de la formule de l'attention

* [Q x Transpos√©e de K] est un produit scalaire entre les vecteurs  requ√™te et les vecteurs cl√©.
  * plus la cl√©  "ressemblera" √† la requ√™te, plus le score produit par [Q x Transpos√©e de K] sera grand pour cette cl√©.
* La partie *dk (=64)* pour normaliser, pas toujours utilis√© dans les m√©canismes d'attention.
* softmax donne une distribution de probabilit√©s qui va encore  augmenter la valeur pour les cl√©s similaires aux requ√™tes, et diminuer  celles des cl√©s dissemblables aux requ√™tes.
* les cl√©s  correspondent √† des valeurs, quand on multiplie le r√©sultat pr√©c√©dent  par V, **les valeurs correspondant aux cl√©s qui ont √©t√© "√©lues" √† l'√©tape  pr√©c√©dente sont sur-pond√©r√©es par rapport aux autres valeurs.**

Enfin on concat√®ne la sortie de chaque t√™te, et on multiplie par une matrice W0, de dimensions 512 x 512 ([(nombre de t√™tes) x (dimension requ√™te ou cl√© ou valeur, i.e. 64)]x[dimension des embeddings]), qui apprend √† projeter le r√©sultat sur un espace de sortie aux dimensions attendues.

##### 		La connexion r√©siduelle

**ajouter la repr√©sentation initiale** √† celle calcul√©e dans les couches d'attention ou dans le FFN.

-> *Cela revient √† dire* : Apprend les relations entre les √©l√©ments de la s√©quence, mais n'oublie pas ce que tu sais d√©j√† √† propos de toi.

appliquer un dropout de 10%, donc en sortie de chaque couche i



##### Le FFN

Couches de neurones formels avec une ReLU comme fonction d'activation

Ici W1 a pour dimensions [dimension des embeddings]x[dimmension d'entr√©e du FFN - au choix] et W2 [dimmension d'entr√©e du FFN - au choix] x  [dimension des embeddings

C'est le r√©seau de neurones "standard" 



##### *Rappelons le principe d'auto-r√©gression du Transformer*.

Par exemple pour traduire "Avoir la p√®che", nous avons vu que :

* 1er passage, on envoie 
  * dans l'encodeur une version  "embedd√©e" de la s√©quence [<CLS> Avoir la p√®che <fin de  phrase>], 
  * en m√™me temps l'amorce de s√©quence [<CLS>] dans le d√©codeur.
  * Le Transformer doit alors pr√©dire le 1er  mot de la traduction : "feel" (i.e. nous sortir la s√©quence [<CLS> feel])
* 2√®me passage, on envoie 
  * dans  l'encodeur de nouveau toute la version "embedd√©e" de la s√©quence [<CLS> Avoir  la p√®che <fin de phrase>],
  * une version "embedd√©e" de ce que le  Transformer a pr√©dit dans le d√©codeur.

*  etc... jusqu'√† ce que la s√©quence pr√©dite en sortie du Transformer finisse par <fin de phrase>.



##### L'auto-attention "masqu√©e" :

‚Äã	*- Pourquoi :*

Dans notre cas, la phrase a pr√©dire en sortie est d√©j√† connue lorsque nous entra√Ænons le mod√®le. Le jeu  d'entra√Ænement poss√®de d√©j√† la correspondance entre "avoir la p√®che" et  "feel great"

Or il faut faire apprendre au mod√®le que :

*  Au premier passage quand l'encodeur re√ßoit  [<CLS> Avoir la p√®che  <fin de phrase>] et le d√©codeur re√ßoit  [<CLS>], le mod√®le  doit pr√©dire  [<CLS> feel].
* Puis "feel" dans ce contexte doit pr√©dire "great" au passage suivant.
* Etc...

Il faut donc lors de l'entra√Ænement **"masquer" √† l'encodeur le reste des  mots √† traduire**. Quand "feel" sort du mod√®le, le d√©codeur ne doit pas  voir "great", **il doit apprendre seul que "feel" dans ce contexte doit  ensuite donner "great"**.

‚Äã	*- Comment :*

Eh bien on va simplement faire en sorte que dans la matrice softmax([Q x Transpos√©e de K]) de notre formule  d'attention, la ligne correspondant a chaque mot soit a **0 pour les  colonnes repr√©sentant les mots suivants "chronologiquement" le dernier  mot g√©n√©r√© par le mod√®le**.

on veut obtenir de quoi masquer √† chaque mot pr√©vu les mots qu'il devrait pr√©voir

donc on veut que les valeurs correspondantes aux mots √† venir n'aient aucune attention

il faut donc que ce soit multipli√©e avec V une matrice dont les valeurs dans le triangle sup√©rieur = 0; pour ce faire on force QK^T √† avoir les valeurs au-dessus de la diagonale de la matrice √† moins l'infini



##### L'attention encodeur-d√©codeur 

Le d√©codeur poss√®de une couche d'attention qui 

* prend en entr√©e le s√©quence de sortie du FFN de l'encodeur, 
* qu'il multiplie √† ses matrices cl√©s et  valeurs (Wki et Wvi pour la t√™te "i"), 
* tandis que la s√©quence sortant de la couche d'auto-attention "masqu√©e" de l'encodeur va se multiplier √†  la matrice des requ√™tes (Wqi pour la t√™te "i").

L'encodeur  d√©couvre 

* des choses int√©ressantes (features) √† propos de la s√©quence  d'entr√©e => les **valeurs**. 
* √Ä ces valeurs attribue un label, un  index, une fa√ßon d'adresser ces "choses" => la **cl√©**. 

Puis le d√©codeur avec sa **requ√™te** va d√©cider du type de valeur √† aller chercher. **La  requ√™te demande la valeur pour une cl√© en particulier.**





##### La sortie



*La couche lin√©aire*

Enfin nous y voila. Appelons S la matrice en sortie du d√©codeur. On la  multiplie par une matrice de poids (qui peuvent apprendre) W1. C'est une **couche totalement connect√©e** qui projette simplement la sortie pr√©c√©dente dans un espace de la taille de notre vocable.

W1 est la matrice qui va permettre d'extraire un mot dans notre  dictionnaire de vocabulaire. Elle aura donc pour dimensions [dimension  des embeddings, i.e. *dmodel*] x [nombre de mots dans notre vocable].



*La softmax*

c'est s√ªrement la derni√®re ligne de S.W1  (correspondant au dernier mot g√©n√©r√©, de dimension [1] x [taille du  vocable]) qui est pass√©e par la softmax. 

La softmax  nous donne alors l'√©l√©ment le plus probable √† pr√©dire (on prend le mot  de la colonne qui donne la probabilit√© la plus haute).



### [Dissecting BERT Part 1: The Encoder](https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3)

The **Encoder** used in BERT is an **attention-based architecture** for Natural Language Processing (NLP) 

the Transformer is composed of two parts, the Encoder and the Decoder. 

**BERT only uses the Encoder** 

##### information flow

1. each token represented as a vector of *emb_dim* size. 1 **embedding vector** for each of the input tokens -> *(input_length) x (emb_dim)* matrix for a specific input sequence.
2. then adds positional information (**positional encoding**). This step returns a matrix of dimensions *(input_length) x (emb_dim)*, just like in the previous step.
3. The data goes through N **encoder blocks**. After this, we obtain a matrix of dimensions *(input_length) x (emb_dim)*.

dimensions of the input and output of the encoder block are the same -> makes sense to use the output of one encoder block as the  input of the next encoder block

the number of blocks N was chosen to be 12 and 24.

the blocks do not share weights with each other

##### Tokenization, numericalization and word embeddings

The first step is to **tokenize** it:

followed by **numericalization**, mapping each token to a unique integer in the corpus‚Äô vocabulary.

 get the **embedding** for each word in the sequence. Each word of the sequence is mapped to a ***emb_dim* dimensional vector that the model will learn during training**. You can think about  it as a vector look-up for each token. The elements of those vectors are **treated as model parameters** and are optimized with back-propagation  just like any other weights.

padding was used to make the input sequences in a batch have the same length. That is, we increase the length of some of the sequences by adding ‚Äò<pad>‚Äô tokens

##### Positional Encoding

At this point, we have a matrix representation of our sequence. However, these representations are not encoding the fact that words appear in  different positions

 we aim to be able to **modify the represented meaning of a specific word  depending on its position**. We don't want to change the full  representation of the word but **we want to modify it a little to encode  its position**.

The approach chosen in the paper is to **add numbers between *[-1,1]* using predetermined (non-learned) sinusoidal functions to the token embeddings**. 

 the word will be represented slightly differently depending on the position the word is in (even if it is the same word).

Moreover, we would like the Encoder to be able to use the fact that some words are in a given position while, in the same sequence, other words are in other specific positions -> the network to be able to **understand relative positions and not only absolute ones**. 

**sinuosidal functions allow positions to be represented as linear combinations of each other and thus allow the network to learn relative relationships between the token positions.**

The approach chosen in the paper to add this information is adding to Z a matrix P with positional encodings.

The authors chose to use a combination of sinusoidal functions. Mathematically, using i for the position of the token in the sequence and j for the position of the embedding feature

advantages over learned positional representations:

* The input_length can be increased indefinitely since the functions can be calculated for any arbitrary position.
* Fewer parameters needed to be learned and the model trained quicker.

The resulting matrix is the input of the first encoder block and has dimensions (input_length) x (emb_dim).

##### Encoder block

N encoder blocks are chained together to generate the Encoder‚Äôs output

A specific block is in charge of finding relationships between the input representations and encode them in its output.

Intuitively, this **iterative process** through the blocks will help the neural network  **capture more complex relationships between words** in the input sequence.  You can think about it as **iteratively building the meaning of the input  sequence as a whole**.



##### Multi-Head Attention

= it computes attention h different times with different weight matrices and then concatenates the results together.

The result of each of those parallel computations of attention is called a **head**. 

once all the heads have been computed they will be concatenated

This will result in a matrix of dimensions *(input_length) x (h*d_v). Afterwards, a linear layer with weight matrix W‚Å∞ of dimensions (h*d_v) x (emb_dim) will be applied leading to a final result of dimensions (input_length) x (emb_dim). 

##### Scaled Dot-Product Attention

Each head is going to be characterized by 3 different projections (matrix multiplications) 

To compute a head we will take the input matrix X and separately project it with the above weight matrices

Once we have K_i, Q_i and V_i we use them to compute the Scaled Dot-Product Attention



**In the encoder block the computation of attention does not use a mask.**

This is the key of the architecture (the name of the paper is no  coincidence) so we need to understand it carefully. Let‚Äôs start by  looking at the matrix product between *Q_i* and *K_i* transposed:

![img](https://miro.medium.com/max/60/1*szTtSJSZBfej5q-KpLmf3Q.png?q=20)



*Q_i* and *K_i* = different projections of the tokens into a *d_k* dimensional space. 

we can think about the **dot product** of those projections as a **measure of similarity between tokens projections**

For every vector projected through *Q_i* the dot product with the projections through *K_i* measures the similarity between those vectors. 

 this is a measure of how similar are the directions of u_i and v_j and how large are their lengths (the closest the direction and the larger the length, the greater the dot product).

Another way of thinking about this matrix product is as the **encoding of a specific relationship between each of the tokens in the input sequence** (the relationship is defined by the matrices K_i, Q_i).

After this multiplication, the matrix is divided element-wise by the square root of d_k for **scaling** purposes.

The next step is a **Softmax** applied row-wise (one softmax computation for each row)

Thus, at this point, the representation of the token is the concatenation of *h* weighted combinations of token representations (centroids) through the *h* different learned projections.

##### Position-wise Feed-Forward Network

3 layers: FC linear layer -> ReLU -> FC linear layer

during this step, **vector representations of tokens don‚Äôt ‚Äúinteract‚Äù with each other.** It is equivalent to run the calculations row-wise and stack the resulting rows in a matrix

The output of this step has dimension *(input_length) x (emb_dim)*.

##### Dropout, Add & Norm

Before this layer, there is always a layer for which inputs and outputs have the same dimensions (Multi-Head Attention or Feed-Forward). We will call that layer Sublayer and its input x.

After each Sublayer, **dropout** is applied with 10% probability. Call this result Dropout(Sublayer(x)). This result is added to the Sublayer‚Äôs input x, and we get x + Dropout(Sublayer(x)).

Observe that in the context of a Multi-Head Attention layer, this means **adding the original representation** of a token x to the representation based on the relationship with other tokens. It is like telling the token:

‚ÄúLearn the relationship with the rest of the tokens, but don‚Äôt forget what we already learned about yourself!‚Äù

Finally, a **token-wise/row-wise normalization** is computed with the mean and standard deviation of each row. This **improves the stability** of the network.

### [Dissecting BERT Appendix: The Decoder](https://medium.com/dissecting-bert/dissecting-bert-appendix-the-decoder-3b86f66b0e5f)

the Decoder in depth; the part of the Transformer architecture that are not used in BERT. 

The problem that the Transformer addresses is translation. To translate a sentence into another language, we want our model to:

* Be able to capture the relationships between the words in the input sentence.
* Combine the information contained in the input sentence and what has already been translated at each step.

Imagine that the goal is to translate a sentence from English to Spanish 

* First, we want to process the information in the input sequence X by **combining the information in each of the words of the sequence** -> done inside the **Encoder**.
* Once we have this information in the output of the Encoder we want to **combine it with the target sequence** -> done in the **Decoder**.

Encoder and Decoder are specific parts of the **Transformer** architecture

**Information Flow**

The data flow through the architecture is as follows:

* (1) represents **each token as a vector** of dimension *emb_dim* -> matrix of dimensions *(input_length) x (emb_dimb)* for a specific input sequence
* (2) adds positional information (**positional encoding**) -> matrix of dimensions *(input_length) x (emb_dim)*
* (3) data goes through N **encoder blocks** -> matrix of dimensions *(input_length) x (emb_dim)*
* (4) target sequence **masked** and sent **through the decoder‚Äôs** **equivalent of 1) and 2)** -> *(target_length) x (emb_dim)* output
* (5) result of 4) goes through N **decoder blocks**. In each of the iterations, the decoder is using the encoder‚Äôs output 3) ->  (target_length) x (emb_dim) output
* (6) applies a **fully connected layer** and a **row-wise softmax** -> *(target_length) x (vocab_size)* output

the described algorithm is processing both the input sentence and the target sentence to train the network

input sentence encoded in The Encoder‚Äôs architecture

in the decoder: how **given a target sentence we obtain a matrix representing the target sentence** for the decoder blocks

same process, composed of two general steps:

* Token embeddings
* Encoding of the positions.

main difference: **the target sentence is shifted** -> before padding, the target sequence will be as follows:

The rest of the process to vectorize the target sequence = as the one described for input sentences in The Encoder‚Äôs architecture.

##### Decoder block ‚Äî Training vs Testing

During **test time** we don‚Äôt have the ground truth. The steps, in this case, will be as follows:

1. Compute the **embedding representation of the input** sequence.
2. Use a **starting sequence token**, for example ‚Äò<SS>‚Äô **as the first target sequence**: [<SS>] -> output = the next token.
3. **Add the last predicted token to the target sequence** and **use it to generate a new prediction** [‚Äò<SS>‚Äô, Prediction_1,‚Ä¶,Prediction_n]
4. Do step 3 **until the predicted token is the one representing the End of the Sequence**, for example <EOS>.

During **training** we have the ground truth, i.e. the tokens we would like the model to output for every iteration of the above process. Since **we have the target in advance**, we will **give the model the whole shifted target sequence at once and ask it to predict the non-shifted target**.

However, there is a problem here. What **if the model sees the expected token and uses it to predict itself**? For example, it might see ‚Äòestas‚Äô at the right of ‚Äòcomo‚Äô and use it to predict ‚Äòestas‚Äô. That‚Äôs **not what we want because the model will not be able to do that a testing time**.

 modify some of the attention layers to **prevent the model of seeing information on the right** (or down in the matrix of vector representation) **but allow it to use the already predicted words**.

transform the matrix of representation and add positional encoding

as in the encoder the output of the decoder block will be also a matrix of sizes *(target_length) x (emb_dim).* 

After a **row-wise linear layer** (a linear layer in the form of matrix product on the right) and a **Softmax** per row this will result in a matrix for which **the maximum element per row indicates the next word**.

we don‚Äôt have problems in the linear layers because they are defined to be token-wise/row-wise in the form of a matrix multiplication through the right

The problem will be in Multi-Head Attention and the input will need to be masked

At training time, the prediction of all rows matter. Given that at prediction time we are doing an iterative process we are just going to care about the prediction of the next word of the last token in the target/output sequence.

##### Masked Multi-Head Attention

This will work exactly as the Multi-Head Attention mechanism but **adding masking to our input**.

T**he only Multi-Head Attention block where masking is required is the first one of each decoder block.** 

the one in the middle is used to combine information between the encoded inputs and the outputs inherited from the previous layers. There is no problem in combining every target token‚Äôs representation with any of the input token‚Äôs representations (since we will have all of them at test time).

The modification will take place after computing the QK/sqrt(d) ratio matrix

the masking step is just going to set to minus infinity all the entries in the strictly upper triangular part of the matrix

* if those entries are relative attention measures per each row, the larger they are, the more attention we need to pay to that token.
* softmax output: the relative attention of those tokens that we were trying to ignore has indeed gone to zero.
* When multiplying this matrix with V_i the only elements that will be accounted for to predict the next word are the ones into its right, i.e. the ones that the model will have access to during test time.

the output of the modified Multi-Head Attention layer will be a matrix *(target_length) x (emb_dim)* because the sequence from which it has been calculated has a sequence length of target_length.

The rest of the process is identical as described in the Multi-Head Attention for the encoder.

##### Multi-Head Attention ‚Äî Encoder output and target

Observe that in this case we are using different inputs for that layer. More specifically, instead of deriving Q_i, K_i and V_i from X as we have been doing in previous Multi-Head Attention layers, this layer will use both the Encoder‚Äôs final output E (final result of all encoder blocks) and the Decoder‚Äôs previous layer output D (the masked Multi-Head Attention after going through the Dropout, Add & Norm layer).

[...]

**every token in the target sequence is represented in every head as a combination of encoded input tokens**. Moreover, this will happen for multiple heads and just as before, that is going to **allow each token of the target sequence to be represented by multiple relationships with the tokens in the input sequence**.



##### Linear and Softmax

This is the final step before being able to get the predicted token for every position in the target sequence. 

 The output from the last Add & Norm layer of the last Decoder block is a matrix X *(target_length)x(emb_dim)*.

linear layer: for every row in x of X compute xW_1

where W_1 is a matrix of learned weights of dimensions *(emb_dim) x (vocab_size)* -> for a specific row the result will be a vector of length *vocab_size*.

a **softmax** is applied to this vector -> **vector describing the probability of the next token**. Therefore, **taking the position corresponding to the maximum probability returns the most likely next word according to the model**.

### [French NLP: entamez le CamemBERT avec les librairies fast-bert et transformers](https://medium.com/@vitalshchutski/french-nlp-entamez-le-camembert-avec-les-librairies-fast-bert-et-transformers-14e65f84c148)

Afin de r√©v√©ler le potentiel de CamemBERT il faudra adapter son mod√®le de langage √† nos donn√©es. En anglais on parle de fine-tuning. Si vous √™tes novice dans le NLP, on peut comparer cette op√©ration √† l‚Äôajustement d‚Äôun costume trop large √† votre taille. Attention, **on ne pr√©-entra√Æne pas CamemBERT** √† nouveau. Ce mod√®le ‚Äúma√Ætrise d√©j√† la grammaire de la langue fran√ßaise‚Äù. On l‚Äôaide simplement √† mieux comprendre la langue et la structure des commentaires. Lors du fine-tuning il faudra utiliser un learning rate tr√®s bas, car il s‚Äôagit des petits ajustements et non pas de r√©-apprentissage.

Les deux principales m√©thodes d'apprentissage automatique de Word2Vec sont Skip-gram et Continuous Bag of Words.

* Le mod√®le **Skip-gram** pr√©dit les mots (contexte) autour du mot cible (cible) (cible -> contexte)
* le mod√®le **Continuous Bag of Words** pr√©dit le mot cible √† partir des mots autour de la cible (contexte) (contexte -> cible)

Le mot cible ne doit pas n√©cessairement se trouver au centre de la ¬´**fen√™tre contextuelle**¬ª qui est compos√©e d'un nombre donn√© de mots environnants, mais peut se trouver √† gauche ou √† droite de la fen√™tre contextuelle.

Un point important √† noter est que **les fen√™tres contextuelles mobiles sont unidirectionnelles**. C'est-√†-dire que la fen√™tre se d√©place sur les mots dans une seule direction, de gauche √† droite ou de droite √† gauche.

en plus de ceux de son nom, BERT apporte d'autres d√©veloppements passionnants dans le domaine de la compr√©hension du langage naturel.

* Pr√©-formation √† partir d'un texte non √©tiquet√©
* Mod√®les contextuels bidirectionnels
* L'utilisation d'une architecture de transformateur
* Mod√©lisation du langage masqu√©
* Attention focalis√©e
* Implication textuelle (pr√©diction de la phrase suivante)
* D√©sambigu√Øsation gr√¢ce au contexte open source

La  magie  du BERT est sa mise en ≈ìuvre d'une **formation bidirectionnelle sur un corpus de texte non √©tiquet√©**, 



BERT a √©t√© **le premier framework / architecture de langage naturel √† √™tre pr√©-form√© en utilisant un apprentissage non supervis√© sur du texte brut** pur (2,5 milliards de mots + de Wikipedia anglais) plut√¥t que sur des corpus √©tiquet√©s.

Les anciens mod√®les de formation en langage naturel ont √©t√© form√©s de mani√®re **unidirectionnelle**. La signification du mot dans une fen√™tre contextuelle s'est d√©plac√©e de gauche √† droite ou de droite √† gauche avec un nombre donn√© de mots autour du mot cible (le contexte du mot ou ¬´c'est la soci√©t√©¬ª). Cela signifie que **les mots qui ne sont pas encore vus dans leur contexte ne peuvent pas √™tre pris en consid√©ration** dans une phrase et qu'ils pourraient en fait changer le sens d'autres mots en langage naturel. Les fen√™tres contextuelles mobiles unidirectionnelles peuvent donc manquer certains contextes changeants importants.

attention - Essentiellement, le BERT est capable de regarder tout le contexte dans la coh√©sion du texte en concentrant l'attention sur un mot donn√© dans une phrase tout en identifiant √©galement tout le contexte des autres mots par rapport au mot. Ceci est r√©alis√© simultan√©ment en utilisant des transformateurs combin√©s avec une pr√©-formation bidirectionnelle.

Cela contribue √† un certain nombre de d√©fis linguistiques de longue date pour la compr√©hension du langage naturel, y compris la r√©solution de cor√©f√©rence. Cela est d√ª au fait que les entit√©s peuvent √™tre cibl√©es dans une phrase en tant que mot cible et que leurs pronoms ou les phrases nominales les r√©f√©ren√ßant sont r√©solus de nouveau vers l'entit√© ou les entit√©s dans la phrase ou l'expression.

De plus, l'attention focalis√©e aide √©galement √† la d√©sambigu√Øsation des mots polys√©miques et des homonymes en utilisant **une pr√©diction / pond√©ration de probabilit√© bas√©e sur le contexte entier du mot en contexte avec tous les autres mots de la phrase**.  Les autres mots re√ßoivent un score d'attention pond√©r√© pour indiquer  combien chacun ajoute au contexte du mot cible en tant que  repr√©sentation du ¬´sens¬ª.

 L'encodeur est l'entr√©e de phrase traduite en repr√©sentations de sens  des mots et le d√©codeur est la sortie de texte trait√©e sous une forme  contextualis√©e.



**Mod√©lisation du langage masqu√©** (formation MLM)

√âgalement connue sous le nom de ¬´**proc√©dure Cloze**¬ª qui existe depuis tr√®s longtemps. L'architecture BERT analyse les phrases avec certains mots masqu√©s de mani√®re al√©atoire et tente de pr√©dire correctement ce qu'est le mot ¬´cach√©¬ª.

Le but de ceci est **d'emp√™cher les mots cibles dans le processus d'apprentissage passant par l'architecture du transformateur BERT de se voir par inadvertance pendant l'entra√Ænement bidirectionnel** lorsque tous les mots sont examin√©s ensemble pour un contexte combin√©. C'est √† dire. cela √©vite un type de boucle infinie erron√©e dans l'apprentissage automatique du langage naturel, qui fausserait le sens du mot.



Implication textuelle (**pr√©diction de la phrase suivante**)

L'une des principales innovations du BERT est qu'il est cens√© √™tre capable de pr√©dire ce que vous allez dire ensuite

form√© pour pr√©dire √† partir de paires de phrases si la deuxi√®me phrase fournie correspond bien √† un corpus de texte.

L'implication textuelle est un type de "qu'est-ce qui vient ensuite?" dans un corps de texte. En plus de l'implication textuelle, le concept est √©galement connu sous le nom de ¬´pr√©diction de la phrase suivante¬ª.

 L'implication textuelle est une t√¢che de traitement du langage naturel impliquant des paires de phrases. 

La premi√®re phrase est analys√©e, puis un niveau de confiance d√©termin√© pour pr√©dire si une deuxi√®me phrase hypoth√©tique donn√©e dans la paire ¬´correspond¬ª logiquement √† la phrase suivante appropri√©e, ou non, avec une pr√©diction positive, n√©gative ou neutre, √† partir d'un texte collection sous examen.



### [Apprentissage de Repr√©sentation dans les R√©seaux de Documents : Application √† la Litt√©rature Scientifique](https://tel.archives-ouvertes.fr/tel-02899422/document)



Le succ√®s des algorithmes d‚Äôapprentissage artificiel d√©pend principalement des repr√©-
sentations des donn√©es sur lesquelles ils sont appliqu√©s.

**L‚Äôapprentissage de repr√©sentation** (AR) [Bengio et al., 2013] s‚Äôoppose √† la construction manuelle de caract√©ristiques des
donn√©es en apprenant automatiquement des descriptions qui rendent leur analyse plus
efficace. En d‚Äôautres termes, plut√¥t que de construire √† la main des repr√©sentations des
donn√©es en utilisant des connaissances expertes, l‚ÄôAR d√©signe une approche diff√©rente o√π
l‚Äôon va construire ces repr√©sentations par un algorithme d‚Äôapprentissage optimisant un ou
plusieurs crit√®res sur les donn√©es.

Un crit√®re couramment utilis√© en apprentissage de repr√©sentation du texte s‚Äôappuie sur
**l‚Äôhypoth√®se distributionnelle**. Celle-ci stipule que **des mots apparaissant dans les m√™mes**
**contextes linguistiques partagent des significations similaire**



 partant d‚Äôune re-
pr√©sentation symbolique du texte (s√©quences de jetons) et transcrivant cette hypoth√®se
dans un espace euclidien muni d‚Äôune mesure de similarit√©, nous sommes en mesure de
construire des vecteurs denses associ√©s aux mots du vocabulaire en rapprochant les mots
apparaissant dans les m√™mes contextes linguistiques.

Cette m√©thodologie est √† l‚Äôorigine
des m√©thodes de plongement de mot (**word embedding**) [Mikolov et al., 2013a] qui a en-
suite √©t√© √©tendue au plongement de r√©seau

Les techniques d‚Äôapprentissage de repr√©sentation pour le texte et pour les r√©seaux sont
intimement li√©s. 

En effet, ces deux types de donn√©es sont naturellement repr√©sent√©es par
des ensembles fini d‚Äô√©l√©ments (ex : mots et sommets) dont on peut mesurer des simila-
rit√©s deux √† deux (ex : nombre de co-occurrences entre mots et nombre de marches o√π
apparaissent des sommets).

es m√™mes m√©thodes de repr√©-
sentations, tels que les plongements ou les m√©canismes d‚Äôattention, soient appliqu√©es √†
ces deux types de donn√©es. 

Un cas simple et largement utilis√© d‚Äôalgorithme de **plongement de r√©seau** est l**‚Äôalgo-**
**rithme de Fruchterman-Reingold** [Fruchterman and Reingold, 1991]. Celui-ci permet de
repr√©senter les sommets dans un espace vectoriel √† deux dimensions, rendant ainsi la vi-
sualisation d‚Äôun r√©seau plus digeste; C‚Äôest une m√©thode it√©rative qui simule, comme un
mod√®le physique, une attraction des sommets connect√©s et une r√©pulsion latente qui vise
√† s√©parer toute paire de sommets.

Originellement, le plongement de graphe est utilis√© comme une m√©thode g√©n√©rale de
r√©duction de dimension

L‚Äô√©mergence des algorithmes de plongement de r√©seau a suivi celle des algorithmes
de plongement de mot (word embedding).

Les m√©thodes d‚Äôapprentissage de repr√©sentation pour le texte et pour les r√©seaux sont
fortement li√©es. En effet, ces deux types de donn√©es peuvent √™tre d√©crits par des √©l√©ments
symboliques (par exemple les mots et les sommets) dont les relations sont mesurables
(par exemple en terme de co-occurrences des mots dans un corpus et de sommets dans
des marches al√©atoires).

L‚Äô**hypoth√®se distributionnelle** est une hypoth√®se fondamentale des algorithmes de plon-
gement de mots. Celle-ci stipule que **la similarit√© distributionnelle des mots est fortement**
**corr√©l√©e avec la similarit√© de leurs sens**.

*  pour construire des repr√©-
  sentations vectorielles captant le sens des mots, il suffit d‚Äô√©tudier le voisinage de ceux-ci,
  c‚Äôest-√†-dire le contexte dans lequel ils apparaissent.
*  si un mod√®le est
  capable de reconstruire les mots contextes d‚Äôun certain mot cible, il est capable d‚Äôen
  repr√©senter le sens

**Skip-Gram** [Mikolov et al., 2013a], l‚Äôune des variantes de la suite logicielle Word2vec est un mod√®le qui **construit deux repr√©sentations pour chaque mot** œâi : **un vecteur cible**
ui et **un vecteur contexte** hi. Ces deux vecteurs sont utilis√©s pour **calculer la probabilit√©**
**conditionnelle d‚Äôobserver un mot selon son contexte**, exprim√©e comme la fonction **softmax
du produit scalaire de leurs repr√©sentations** 

 Cet ensemble est construit en faisant glisser une fen√™tre de taille œÑ sur un corpus de
texte

 Skip-Gram mod√©lise les probabilit√©s d‚Äôoccurrence d‚Äôun mot cible conditionnellement
√† chaque mot contexte, de mani√®re ind√©pendante

a popularit√© de Skip-Gram vient aussi des propri√©t√©s g√©om√©triques des repr√©-
sentations qu‚Äôil construit. En effet, celles-ci semblent √™tre en lien direct avec les sens
s√©mantiques et syntaxiques des mots.

DeepWalk [Perozzi et al., 2014] est une m√©thode de plongement de r√©seau qui s‚Äôinspire
de Skip-Gram. 

‚Äôintuition centrale de cette approche est que les chemins g√©n√©r√©s par de
courtes marches al√©atoires dans un graphe sont similaires √† des phrases en langage naturel.

La fr√©quence d‚Äôapparition des sommets dans ces marches suit une loi puissance, similai-
rement aux fr√©quences des mots dans un corpus

l‚Äôexemple de la traduction
automatique, particuli√®rement telle qu‚Äôelle est abord√©e dans un mod√®le de type encodeur-
d√©codeur neuronal [Bahdanau et al., 2014]. La t√¢che consiste √† produire en sortie une
s√©quence de plongements de mots (y1,...,y_y) √©tant donn√©e une s√©quence de plongements
de mots d‚Äôentr√©e (x1,...,x_x).

Les mots en sortie correspondent par exemple √† de l‚Äôanglais
alors que les mots en entr√©e correspondent √† du fran√ßais.

Le processus est **auto-r√©gressif**, ce
qui signifie que l**a s√©quence de sortie est g√©n√©r√©e mot par mot**.

√Ä chaque √©tape i, l‚Äôencodeur
utilise la s√©quence d‚Äôentr√©e (x1,...,x`x) et le morceau de s√©quence de sortie pr√©c√©demment
g√©n√©r√© (y1,...,yi‚àí1) afin de pr√©dire le mot suivant yi.

Notons que l‚Äôon d√©finit le premier
vecteur de sortie comme une constante y1 = ystart ce qui permet de d√©finir la r√©currence
pour la premi√®re it√©ration. 

le probl√®me auquel cette approche fait face est la gestion de
la diversit√© d‚Äôinformation pr√©sente en entr√©e pour pr√©dire chaque sortie

 En traduction
automatique, pr√©dire un mot ne d√©pend souvent que d‚Äôune infime proportion des vecteurs
pr√©sent√©s √† l‚Äôencodeur-d√©codeur (x1,...,x`x,y1,...,yi‚àí1). 

. Lorsque cette s√©quence est longue,
il devient difficile pour le mod√®le d‚Äôen faire le tri. C‚Äôest pr√©cis√©ment pour faciliter ce tri que
les **m√©canismes d‚Äôattention** sont utilis√©s. Ils vont **permettre au mod√®le de ne s√©lectionner qu‚Äôun sous-ensemble pr√©cis de la s√©quence d‚Äôentr√©e afin de pr√©dire le prochain mot**.

Le premier m√©canisme d‚Äôattention pour les r√©seaux profonds est pr√©sent√© dans [Xu
et al., 2015] pour la g√©n√©ration automatique de l√©gendes pour images. Le mod√®le est d√©-
crit comme un **encodeur-d√©codeur**. L‚Äôencodeur extrait un ensemble de N repr√©sentations
vectorielles (a1,...,aN) d‚Äôune image via un **CNN**, appel√©es vecteurs d‚Äôannotation. Ces vec-
teurs sont g√©n√©r√©s de sorte √† capturer les diff√©rentes composantes de l‚Äôimage. Le d√©codeur,
un **LSTM**, produit de mani√®re auto-r√©gressive une s√©quence de mots en sortie. 

Pour ce
faire, il prend en entr√©e les mots pr√©c√©demment g√©n√©r√©s (y1,...,yi‚àí1). **L‚Äôattention est in-**
**t√©gr√©e pour conditionner cette g√©n√©ration au vecteurs d‚Äôannotation**. 

Le **vecteur contexte**
ci du LSTM est calcul√© non plus comme une fonction de ci‚àí1 et de hi‚àí1 mais comme une
**moyenne pond√©r√©e des vecteurs d‚Äôannotation issus de l‚Äôimage** 

Les poids
Œ±k sont calcul√©s par produit scalaire entre le vecteur cach√© hi‚àí1 de la pr√©c√©dente √©tape
(capturant l‚Äôhistorique des mots g√©n√©r√©s (y1,...,yi‚àí1)) avec chacun des vecteurs d‚Äôannota-
tion ak.

chaque mot est g√©n√©r√© en confrontant les caract√©ristiques de l‚Äôimage
et l‚Äôhistorique des mots pr√©c√©demment g√©n√©r√©s en l√©gende

 Ceci permet au mod√®le de se
concentrer alternativement sur les diff√©rents √©l√©ments de l‚Äôimage afin de construire une
description en langage naturelle de celle-ci

en √©tudiant les poids Œ±k, on peut
facilement identifier la r√©gion de l‚Äôimage ayant motiv√© le choix du mot g√©n√©r√© par le
mod√®le

Le **Transformer** [Vaswani et al., 2017] est **le premier mod√®le de traduction automa-**
**tique reposant uniquement sur un m√©canisme d‚Äôattention, sans RNN ni CNN**

**L‚Äôaspect**
**s√©quentiel de l‚Äôentra√Ænement d‚Äôun RNN (pr√©dire chaque mot l‚Äôun apr√®s l‚Äôautre) consti-**
**tue le v√©ritable point faible** de ces architectures car il rend difficile leur parall√©lisation.

Le Transformer reprend l‚Äôarchitecture **encodeur-d√©codeur** couramment utilis√©e en traduc-
tion automatique et introduit un **m√©canisme d‚Äôattention**, le **scaled dot-product attention**
(SDPA), **parall√©lisable** et reposant principalement sur des op√©rations matricielles.

Dans
sa version la plus simple, SDPA transforme une s√©quence de vecteurs d‚Äôentr√©e (x1,...,x_l)
en une s√©quence de vecteurs de sortie (y1,...,y_l) dont **chaque repr√©sentation yi repr√©-*
sente xi conditionnellement √† l‚Äôensemble des vecteurs (x1,...,x_l).** 

SDPA construit 3 repr√©sentations distinctes des xi par projections lin√©aires : 

* les **requ√™tes** Q = XWq,
* les **clefs** K = XWk et 
* les **valeurs** V = XWv. 

En notant œÅw la dimension de toutes les repr√©sentations X,Y,Q,K et V , le m√©canisme d‚Äôattention s‚Äô√©crit :
$ Y = softmax( \frac{QK^T}{‚àöœÅ_w})V$ 

le **nominateur** = une matrice dont chaque valeur contient le produit scalaire entre une requ√™te qi et une clef kj. 

le **d√©nominateur** permet de r√©duire le probl√®me de fuite du gradient qui
peut se produire lorsque le softmax prend des valeurs extr√™mes, en limitant la magnitude
du produit scalaire dont l‚Äô√©tendue des valeurs augmente naturellement avec la dimension.

Le **softmax**, op√©r√© sur chaque ligne de la matrice, permet d‚Äôobtenir une pond√©ration com-
pos√©e de valeurs positives sommant √† un, similaire √† des **probabilit√©s associ√©es aux n mots**
d‚Äôentr√©e, pour chaque requ√™te q

Les poids d‚Äôattention s‚Äô√©crivent $ Œ± = softmax( \frac{QK^T}{‚àöœÅ_w})$  et se somment √† 1

La multiplication matricielle entre ces poids d‚Äôattention et $V$ consiste √† r√©aliser des moyennes pond√©r√©es des v_j

 **Le produit scalaire entre requ√™tes et clefs g√©n√®re**
**des poids d‚Äôattention qui permettent de pond√©rer les valeurs, de sorte √† repr√©senter ce mot**
**contextuellement aux autres mots de la phrase.**

L'intuition derri√®re cette formule est que si Œ±ij est proche de 1, le vecteur yi sera
fortement influenc√© par la valeur vj (et donc √† une projection lin√©aire pr√®s par xj).

si Œ±ij est proche de 0, c‚Äôest que yi n‚Äôest pas li√© √† xj. 

les seuls param√®tres
de ce m√©canisme sont Wq,Wk et Wv de dimensions œÅw √óœÅw et leur r√¥le est de projeter
la s√©quence d‚Äôentr√©e dans trois espaces Q,K et V de sorte √† capturer les d√©pendances
entre les mots. 

 Les concepts de ¬´ clef ¬ª, ¬´ valeur ¬ª et ¬´ requ√™te ¬ª proviennent de syst√®mes
de stockage des donn√©es

. Par exemple, lorsque l‚Äôon saisit une **requ√™te** pour rechercher un
document dans une base de donn√©es, un moteur de recherche associe cette requ√™te √† un
ensemble de **clefs** enregistr√©s dans la base (titre du document, contenu, date, etc.), puis il
pr√©sente les meilleures correspondances de documents (**valeurs**)

Dans le Transformer, **le m√©canisme d‚Äôattention est utilis√© de deux fa√ßons diff√©rentes** :

1. dans des m√©canismes d‚Äô**auto-attention** (self-attention
   * de sorte √† **construire des repr√©-**
     **sentations contextuelles Xc des mots de la s√©quence d‚Äôentr√©e et Y c des mots de la s√©quence**
     **de sortie** 
2. dans le d√©codeur
   *  les **requ√™tes sont construites √† partir des sorties**
     **contextuelles** Y c
   * **les clefs et valeurs sont construites √† partir des entr√©es contextuelles**
     **Xc**
   * **Le d√©codeur fabrique ainsi des repr√©sentations de la s√©quence de sortie capturant ses**
     **d√©pendances avec la s√©quence d‚Äôentr√©e**, facilitant la pr√©diction du prochain mot

le
mod√®le dans son int√©gralit√© est en r√©alit√© compos√© de multiples m√©canismes d‚Äôattention
op√©r√©s en parall√®les, concat√©n√©s puis r√©utilis√©s sur de multiples couches. L‚Äôarchitecture
enti√®re fait intervenir un grand nombre de perceptrons √† plusieurs couches augmentant
significativement le nombres de param√®tres et rendant non trivial l‚Äôentra√Ænement du mo-
d√®le

Le Transformer, plus pr√©cis√©ment son **encodeur avec ses m√©canismes d‚Äôauto-attention**,
constitue la brique de base de **BERT** [

L‚Äôid√©e principale est de pr√©-entra√Æner ce m√©canisme d‚Äôattention sur
des t√¢ches tr√®s g√©n√©rales ne requ√©rant pas d‚Äôannotation particuli√®re avant d‚Äôaffiner les
param√®tres du mod√®le sur des t√¢ches sp√©cifiques, telles que la d√©tection d‚Äôentit√© nomm√©e et
l‚Äôanalyse de sentiment. 

Pour le **pr√©-entra√Ænement**, **deux t√¢ches** sont propos√©es : 

1. **la pr√©diction de mots masqu√©s** 
   * on
     pr√©sente au mod√®le des phrases dont 15% des mots ont √©t√© remplac√©s par un vecteur
     sp√©cial de masque et on optimise les param√®tres du mod√®le de sorte √† ce que les vecteurs
     de sortie associ√©s aux masques soient les plus proches possible des vecteurs des mots
     cach√©s. 
2. la **pr√©diction de phrases successives**.
   * on tire al√©atoirement 50% de paires de phrases qui se
     suivent dans un corpus et 50% de paires de phrases qui ne se suivent pas et on entra√Æne
     un classifieur √† pr√©dire si ces phrases se suivent √† partir d‚Äôune repr√©sentation de sortie
     produite √† partir d‚Äôun vecteur sp√©cial de classification ajout√© aux phrases d‚Äôentr√©e.



### [Comprendre le langage √† l'aide de XLNet avec pr√©-formation autor√©gressive](https://ichi.pro/fr/comprendre-le-langage-a-l-aide-de-xlnet-avec-pre-formation-autoregressive-172103194027075)                    

**XLNet** surpasse le BERT sur 20 t√¢ches de r√©f√©rence en PNL

XLNet exploite le meilleur de la mod√©lisation de **langage autor√©gressif** (AR) et de **l'autoencodage** (AE), les deux objectifs de pr√©-formation les plus connus, tout en √©vitant leurs limites

Consid√©r√© comme l'un des d√©veloppements les plus importants de 2019 en PNL, XLNet **combine le mod√®le de langage autor√©gressif**, Transformer-XL , **et la capacit√© bidirectionnelle** de BERT pour lib√©rer la puissance de cet important outil de mod√©lisation de langage

Pour la phase de pr√©-formation, les deux architectures les plus r√©ussies sont la mod√©lisation du langage autor√©gressif (AR) et l'autoencodage  (AE). 

1. ##### Mod√©lisation du langage autor√©gressif (AR)

Dans les mod√®les AR conventionnels, le contexte unidirectionnel dans le sens avant ou arri√®re dans une s√©quence de texte est cod√©

 Il est utile pour les t√¢ches NLP g√©n√©ratives qui g√©n√®rent un contexte dans le sens direct.

Cependant, AR √©choue dans le cas o√π le contexte bidirectionnel doit √™tre utilis√© simultan√©ment . Cela pourrait devenir probl√©matique, en particulier avec la t√¢che de compr√©hension du langage en aval o√π des informations de contexte bidirectionnelles sont requises.

2. ##### Mod√®le de langage de codage automatique (AE)

Un mod√®le bas√© sur AE a la capacit√© de mod√©liser des contextes bidirectionnels en reconstruisant le texte original √† partir d'une entr√©e corrompue ([MASK]). Le mod√®le AE est donc meilleur que le mod√®le AR lorsqu'il s'agit de mieux capturer le contexte bidirectionnel.

Un exemple notable d'AE est **[BERT](https://arxiv.org/pdf/1810.04805.pdf) qui est bas√© sur l'auto-encodage de d√©bruitage**. 

il souffre d'un **√©cart de pr√©-entra√Ænement-finetune** r√©sultant de la d√©pendance entre les jetons masqu√©s et ceux non masqu√©s.

[MASK] utilis√© dans l'√©tape de pr√©-formation est absent des donn√©es r√©elles utilis√©es aux t√¢ches en aval, y compris l'√©tape de r√©glage fin. 

Pour les caract√©ristiques de d√©pendance d'ordre √©lev√© et √† longue port√©e en langage naturel, BERT simplifie √† l'extr√™me le probl√®me en supposant que les jetons pr√©dits (masqu√©s dans l'entr√©e) sont ind√©pendants les uns des autres tant que les jetons non masqu√©s sont donn√©s.

Alors que AR peut estimer la probabilit√© d'un produit avant ou arri√®re sous la forme d'une distribution de probabilit√© conditionnelle, **BERT ne peut pas mod√©liser la probabilit√© conjointe en utilisant la r√®gle du produit en raison de son hypoth√®se d'ind√©pendance pour les jetons masqu√©s.**

3. ##### En quoi XLNet diff√®re-t-il des AR et AE conventionnels (BERT)?

Les auteurs de XLNet proposent de **conserver les avantages du mod√®le de langage AR tout en lui faisant apprendre du contexte bidirectionnel en tant que mod√®les AE (par exemple, BERT) pendant la phase de pr√©-formation**. 

**L'interd√©pendance entre les jetons sera pr√©serv√©e**, contrairement √† BERT. 

Le nouvel objectif propos√© est appel√© "**Mod√©lisation du langage de permutation**. "

Diff√©rent de BERT et d'autres transformateurs qui combinent l'incorporation de position et l'incorporation de contenu pour la pr√©diction, XLNet pr√©dit la distribution du prochain jeton en prenant en compte la position cible z_t comme entr√©e. 

L'architecture **d'auto-attention √† deux flux** est utilis√©e pour  r√©soudre les probl√®mes que pose le transformateur traditionnel; se compose de **deux types  d'attention personnelle**. 

1. la **repr√©sentation du flux de  contenu**
   * identique √† l'auto-attention standard de Transformer qui prend  en compte √† la fois le contenu (x_ {z_t}) et les informations de  position (z_t). 
2. la **repr√©sentation de requ√™te**, 
   * **remplace  essentiellement le [MASK]** de BERT, appris par l'attention du flux de  requ√™te pour pr√©dire x_ {z_t} **uniquement avec des informations de  position mais pas son contenu**.
   * seules les informations de position du  jeton cible et les informations de contexte avant le jeton sont  disponibles.

Le r√©sultat final de l'attention √† deux flux est une **distribution de  pr√©diction sensible √† la cible**. 

La principale diff√©rence entre XLNet et  BERT est que **XLNet n'est pas bas√© sur la corruption de donn√©es** comme le  fait BERT, il peut donc √©viter les limitations de BERT r√©sultant du  masquage

XLNet int√®gre un **sch√©ma de codage relatif** et un **m√©canisme de r√©currence de segment** de [Transformer-XL](https://arxiv.org/pdf/1901.02860.pdf) pour capturer les d√©pendances qui sont plus √©loign√©es que les RNN et Transformer. 

Le **codage  positionnel relatif** est appliqu√© en fonction de la s√©quence d'origine. 

Le **m√©canisme de r√©currence au niveau du segment** √©vite la fragmentation  de contexte repr√©sent√©e par le traitement de segment de longueur fixe.  Il permet de r√©utiliser des segments de phrase du pass√© avec le nouveau  segment. Le Transformer-XL r√©alise cela en incluant la r√©currence au  niveau du segment dans les √©tats masqu√©s.

le Transformer standard contient des **informations de position dans les codages de position**, matrice U , avec int√©gration de position absolue. 

Transformateur-XL **code pour la distance relative dynamiquement dans le score de l' attention** en introduisant la matrice R . 

Dans le score d'attention de Transformer-XL, les 4 termes repr√©sentent respectivement l'adressage bas√© sur le contenu, le biais de position d√©pendant du contenu, le biais de contenu global et le biais de position global. 

Avec Transformer-XL, des articles de texte coh√©rents peuvent √™tre g√©n√©r√©s et il y a √©galement une acc√©l√©ration substantielle lors de l'√©valuation par rapport aux RNN et au Transformer standard.

**Dans XLNet, Transformer-XL est inclus dans le cadre de pr√©-formation**. Le m√©canisme de r√©currence de Transformer-XL est ainsi incorpor√© dans le param√®tre de permutation propos√© dans XLNet pour r√©utiliser les √©tats cach√©s des segments pr√©c√©dents.

 L'ordre de factorisation dans la permutation des segments pr√©c√©dents ne sera pas mis en cache et r√©utilis√© √† l'avenir. 

Seule la repr√©sentation du contenu du segment est conserv√©e dans les √©tats masqu√©s.

**XLNet combine la capacit√© bidirectionnelle de BERT et la technologie autor√©gressive de Transformer-XL** pour r√©aliser une am√©lioration substantielle; il bat BERT dans plus d'une douzaine de t√¢ches. 



### [Review: BioBERT paper](https://medium.com/@raghudeep/biobert-insights-b4c66fde8fa7)

The major contribution is a pre-trained bio-medical language representation model for various bio-medical text mining tasks. Tasks such as NER from Bio-medical data, relation extraction, question & answer in the  biomedical field.

BioBERT is a **contextualized language representation model, based on BERT**, a pre-trained model which is trained on different combinations of general & biomedical domain corpora.

Approach

* Initialize BioBERT with BERT pre-trained model trained on Wikipedia 2.5 billion words & Books Corpus 0.8 billion words. 
  * rather than taking a random initialization of weights, pre-trained weights from BERT model are taken. 
  * Transferring representations learned from previous corpora, it's similar to the transfer learning used for image data problems.
* pre-training on the domain data, here BioBERT is pre-trained on PubMed Abstracts 4.5 billion words & PMC Full-text articles 13.5 billion words.
* the pre-trained model is used to fine-tune on various biomedical text mining tasks like NER, question & answer, relation extraction.

The interesting part was that the **pre-training was not just on biomedical corpora but rather took different combinations of general & biomedical corpora** since their research objective was to figure out the performance of BERT with different combinations of corpora & the amount of data needed to pre-train the model.

Authors have used Word piece tokenizer (read sec 4.1 of the paper) as used by the BERT paper to mitigate OOV (out of the vocabulary) problem.

### [Approaches to Biomedical Text Mining with BERT](https://medium.com/geekculture/approaches-to-biomedical-text-mining-dca3408397b0)

BERT includes a pre-training step and a fine-tuning step.

##### (1) A pre-training step builds a generic language model.

BERT builds a language model by learning the context of words in a  left-to-right and right-to-left manner (**bidirectional**). 

during training, 15% of all words are masked (blanked out)  in sequences at random. 

The training process predicts the masked-out  word using a standard back propagation of errors method

BERT **also  learns the relationships between two sentences** by predicting whether the next sentence following the previous sentence is the actual *next* sentence or a random sentence. 

when choosing two  sentences A and B for each training sample, 50% of the time, B is the  actual next sentence that follows A, and 50% of the time it‚Äôs a random  sentence from the text. 

expensive labeling is not required in  either case

##### (2) A fine-tuning step with labeled data after pre-training with unlabeled data.

example of one of several fine-tuning, answering questions. 

**Fine-tuning** a supervised downstream task has the advantage of having to **learn only a few additional parameters with a relatively small, labeled dataset**.

BERT (fine-tuned on SQuAD) **learns two extra vectors that mark the beginning and the end of the answer span**. They are the start-token classifier and the end-token classifier that demarcate the answer (from the paragraph).

##### Applying BERT to the Biomedical Domain ‚Äî BioBERT

The word distribution and context in the general text domain (WikiPedia and a large collection of books) is quite different from the biomedical domain (PubMed) and thus fine-tuning or adaptation is required (the architecture remains the same, however). Medical literature has a preponderance of medical terms; proper nouns (e.g., BRCA1, c.248T>C) and terms (e.g., transcriptional, antimicrobial), which are readily understood by biomedical researchers.

##### Pre-training BioBERT

Step 1, **initialize BioBERT with weights from BERT** (**transfer learning**).

Step 2, **BioBERT is pre-trained on biomedical domain text** (PubMed abstracts and PebMed Central full-text articles).



###  [ILLUSTRATION DE BERT](https://lbourdois.github.io/blog/nlp/BERT/) 

#####  1.1 R√©capitulatif sur le word embeddings 

La pratique a fait √©merg√©e que c‚Äô√©tait une excellente id√©e d‚Äôutiliser des **embeddings pr√©-entrain√©s sur de grandes quantit√©s de donn√©es textuelles** au lieu de les former avec le mod√®le sur ce qui √©tait souvent un petit jeu de donn√©es. Il est donc devenu possible de t√©l√©charger une liste de mots et leurs embeddings g√©n√©r√©es par le pr√©-entra√Ænement avec Word2Vec ou GloVe. V

La structure encodeur-d√©codeur du Transformer le rend tr√®s efficace pour la traduction automatique. 

#####  **1.5 L‚ÄôOpen AI Transformer (Pr√©-entra√Ænement d‚Äôun d√©codeur de Transformer pour la mod√©lisation du langage)** 

nous n‚Äôavons pas besoin d‚Äôun Transformer complet pour adopter l‚Äôapprentissage par transfert dans le cadre de taches de NLP. Nous pouvons nous contenter du decodeur du Transformer. 

Le d√©codeur est un bon choix parce que c‚Äôest un choix naturel pour la mod√©lisation du langage (pr√©dire le mot suivant). En effet il est construit pour masquer les futurs tokens ‚Äì une fonction pr√©cieuse lorsqu‚Äôil g√©n√®re une traduction mot √† mot.

Le mod√®le empile douze couches de d√©codeurs. Puisqu‚Äôil n‚Äôy a pas d‚Äôencodeur, les couches de d√©codeurs n‚Äôont pas la sous-couche d‚Äôattention encodeur-d√©codeur comme dans le Transformer classique. Ils ont cependant toujours la couche d‚Äôauto-attention.

nous pouvons proc√©der √† l‚Äôentra√Ænement du mod√®le sur la m√™me t√¢che de mod√©lisation du langage : pr√©dire le mot suivant en utilisant des ensembles de donn√©es massifs (sans label). 

L‚Äôentra√Ænement est r√©alis√© sur 7.000 livres car ils permettent au mod√®le d‚Äôapprendre √† associer des informations connexes

 l‚ÄôOpenAI Transformer est pr√©-entrain√© et que ses couches ont √©t√© ajust√©es pour g√©rer raisonnablement le langage, nous pouvons commencer √† l‚Äôutiliser pour des t√¢ches plus sp√©cialis√©es. 

GPT-2

2. ##### BERT : du d√©codeur √† l‚Äôencodeur 

#####  **2.1 Architecture du mod√®le** 

L‚Äôarticle original pr√©sente deux tailles de mod√®les pour BERT :

- BERT BASE de taille comparable √† celle de l‚ÄôOpenAI Transformer afin de comparer les performances.
- BERT LARGE, un mod√®le beaucoup plus grand qui a atteint l‚Äô√©tat de l‚Äôart des r√©sultats rapport√©s dans l‚Äôarticle.

Les deux mod√®les BERT ont un grand nombre de **couches d‚Äôencodeurs** (appell√©es **Transformer Block** dans l‚Äôarticle d‚Äôorigine) :

* 12 pour la version de base
* 24 pour la version large. 

Ils ont √©galement **des r√©seaux feedforward** plus grands (768 et 1024 unit√©s cach√©es respectivement) et plus de **t√™tes d‚Äôattention** (12 et 16 respectivement) que la configuration par d√©faut dans l‚Äôimpl√©mentation initial du Transformer 

#####  2.2 Entr√©es du mod√®le 

Le premier token d‚Äôentr√©e est un jeton sp√©cial [CLS]

Tout comme l‚Äôencodeur du Transformer, BERT prend une s√©quence de mots en entr√©e qui remonte dans la pile**. Chaque couche applique l‚Äôauto-attention et transmet ses r√©sultats √† un r√©seau feed-forward, puis les transmet √† l‚Äôencodeur suivant**

Trouver la bonne mani√®re d‚Äôentra√Æner une pile d‚Äôencodeurs est un  obstacle complexe que BERT r√©sout en adoptant un concept de ¬´ **mod√®le de  langage masqu√©** ¬ª (Masked LM en anglais) tir√© de la litt√©rature  ant√©rieure (il s‚Äôagit d‚Äôune **Cloze task**).

* prendre al√©atoirement 15% des tokens en entr√©e puis √† masquer 80% d‚Äôentre eux, en remplacer 10% par un autre token compl√®tement al√©atoire (un autre mot) et de ne rien faire dans le cas des 10% restant
* objectif: que le mod√®le pr√©dise correctement le token original modifi√© (via la perte d‚Äôentropie crois√©e)
* Le mod√®le est donc **oblig√© de conserver une repr√©sentation contextuelle** distributionnelle de chaque jeton d‚Äôentr√©e.

Afin d‚Äôam√©liorer BERT dans **la gestion des relations** existant **entre plusieurs phrases**, le processus de pr√©-entra√Ænement comprend une t√¢che suppl√©mentaire : √©tant donn√© deux phrases (A et B), B est-il susceptible d‚Äô√™tre la phrase qui suit A, ou non ?

#####  2.3 Sorties du mod√®le 

nous nous concentrons uniquement sur la sortie de la premi√®re position (√† laquelle nous avons pass√© le jeton sp√©cial [CLS]).

Ce vecteur peut maintenant √™tre utilis√© comme entr√©e pour un classifieur de notre choix. L‚Äôarticle obtient d‚Äôexcellents r√©sultats en utilisant simplement **un r√©seau neuronal √† une seule couche comme classifieur**.

en cas de plusieurs labels: modifier le r√©seau du classifieur pour avoir plus de neurones de sortie qui passent ensuite par la couche softmax.

#####  2.4 Mod√®les sp√©cifiques √† une t√¢che 

 les auteurs de BERT pr√©cisent les approches de fine-tuning appliqu√©es pour quatre t√¢ches de NLP diff√©rentes

1. classification
2. tests de logique
3. QA
4. NER

#####  2.5 BERT pour l‚Äôextraction de features 

L‚Äôapproche fine-tuning n‚Äôest pas l‚Äôunique mani√®re d‚Äôutiliser BERT. Tout comme ELMo, vous pouvez utiliser BERT pr√©-entrain√© pour cr√©er des word embeddings contextualis√©s. Vous pouvez ensuite int√©grer ces embeddings √† votre mod√®le existant.

##### remarques

BERT ne consid√®re pas les mots comme des tokens. Il regarde plut√¥t les WordPieces (par exemple : playing donne play + ##ing).



### [ILLUSTRATION DU WORD EMBEDDING ET DU WORD2VEC](https://lbourdois.github.io/blog/nlp/word_embedding/)               



Au lieu de regarder seulement deux mots avant le mot cible, nous pouvons aussi regarder deux mots apr√®s lui. C‚Äôest ce qu‚Äôon appelle une architecture de **continuous bag of words**.

 Au lieu de deviner un mot en fonction de son contexte (les mots avant et apr√®s), cette autre architecture essaie de deviner les mots voisins en utilisant le mot courant. Cette m√©thode s‚Äôappelle l‚Äôarchitecture **skipgram**.

Pour g√©n√©rer des embeddings de haute qualit√©, nous pouvons passer d‚Äôun mod√®le de la pr√©diction d‚Äôun mot voisin √† un mod√®le qui prend le mot d‚Äôentr√©e et le mot de sortie, et sort un score indiquant s‚Äôils sont voisins ou non (0 pour ¬´ non voisin ¬ª, 1 pour ¬´ voisin ¬ª).

Nous passons d‚Äôun r√©seau neuronal √† un mod√®le de r√©gression logistique qui est ainsi beaucoup plus simple et beaucoup plus rapide √† calculer.

 Mais il y a une faille √† combler. Si tous nos exemples sont positifs (cible : 1), nous nous ouvrons √† la possibilit√© d‚Äôun mod√®le qui renvoie toujours 1 ‚Äì atteignant 100% de pr√©cision, mais n‚Äôapprenant rien et g√©n√©rant des embeddings de d√©chets.

 nous devons introduire des √©chantillons n√©gatifs dans notre ensemble de donn√©es, c‚Äôest √† dire des √©chantillons de mots qui ne sont pas voisins. Notre mod√®le doit retourner 0 pour ces √©chantillons. (**negative sampling**) -> **Skipgram with Negative Sampling**

11. ##### Processus d‚Äôentra√Ænement de Word2vec 

Au d√©but de la phase d‚Äôentra√Ænement, nous cr√©ons deux matrices ‚Äì une **Embedding matrix** et une **Context matrix**. 

Ces deux matrices ont un embedding pour chaque mot de notre vocabulaire (*vocab_size* est donc une de leurs dimensions). 

La seconde dimension est la longueur que nous voulons que chaque vecteur d‚Äôembedding soit (une valeur g√©n√©ralement utilis√©e de *embedding_size* est 300, mais nous avons regard√© un exemple de 50 plus t√¥t dans ce post).

au d√©but de l‚Äôentra√Ænement, nous initialisons ces matrices avec des valeurs al√©atoires.

A chaque √©tape de l‚Äôentra√Ænement, nous prenons un exemple positif et les exemples n√©gatifs qui y sont associ√©s

Nous proc√©dons √† la recherche de leurs embeddings. Pour le mot d‚Äôentr√©e, nous regardons dans l‚ÄôEmbedding matrix. Pour les mots de contexte, nous regardons dans la Context matrix.

 nous effectuons le produit scalaire de l‚Äôembeddings d‚Äôentr√©e avec chacun des embeddings de contexte.

transformer ces scores en quelque chose qui ressemble √† des probabilit√©s. Nous avons besoin qu‚Äôils soient tous positifs et qu‚Äôils aient des valeurs entre z√©ro et un. Pour cela, nous utilisons la fonction sigmo√Øde.

La taille de la fen√™tre et le nombre d‚Äô√©chantillons n√©gatifs sont deux hyperparam√®tres cl√©s dans le processus d‚Äôentra√Ænement de word2vec.

Une heuristique est que des fen√™tres de petite taille (2-15) conduisent √† des embeddings avec des scores de similarit√© √©lev√©s entre deux embeddings. Cela signifie que les mots sont interchangeables 

Des fen√™tres de plus grande taille (15-50, ou m√™me plus) m√®nent √† des embeddings o√π la similarit√© donne une indication sur la parent√© des mots

###  [ILLUSTRATION DU TRANSFORMER](https://lbourdois.github.io/blog/nlp/Transformer/) 

Transformer, un mod√®le qui utilise l‚Äôattention pour augmenter la vitesse √† laquelle ces mod√®les peuvent √™tre entra√Æn√©s

 Le Transformer surpasse le mod√®le de traduction automatique de Google dans des t√¢ches sp√©cifiques

 **Le plus grand avantage, vient de la fa√ßon dont le Transformer se pr√™te √† la parall√©lisation.**

1. ##### apper√ßu haut niveau

un composant d‚Äôencodage, un composant de d√©codage et des connexions entre eux.

Le composant d‚Äôencodage est une pile **d‚Äôencodeurs** (l‚Äôarticle empile six encodeurs les uns sur les autres)

Le composant de d√©codage est une pile de **d√©codeurs** du m√™me nombre.

**Les encodeurs**

* tous **identiques mais ne partagent pas leurs poids**
* chacun divis√© en 2 sous-couches :
  1. Les entr√©es de l‚Äôencodeur passent d‚Äôabord par **une couche  d‚Äôauto-attention** 
     * **aide l‚Äôencodeur √† regarder les autres  mots dans la phrase d‚Äôentr√©e** lorsqu‚Äôil code un mot sp√©cifique
  2. Les sorties de la couche d‚Äôauto-attention sont transmises √† un **r√©seau feed-forward**. 
     * Le m√™me r√©seau feed-forward **appliqu√© ind√©pendamment √† chaque encodeur**.

**Le d√©codeur**

* poss√®de ces 2 couches, mais entre elles se trouve une **couche d‚Äôattention qui aide le d√©codeur √† se concentrer sur les parties pertinentes de la phrase d‚Äôentr√©e** **(encoder-decoder attention**; comme dans les mod√®les seq2seq).

#####  **2. Les tenseurs** 

 nous commen√ßons par transformer chaque mot d‚Äôentr√©e en vecteur √† l‚Äôaide d‚Äôun algorithme d‚Äôembedding.

**L‚Äôembedding n‚Äôa lieu que dans l‚Äôencoder inf√©rieur.** Le point commun √† **tous les encodeurs est qu‚Äôils re√ßoivent une liste de vecteurs de la taille 512.** Dans l‚Äôencoder du bas cela serait le word embeddings, mais dans les autres encodeurs, ce serait la sortie de l‚Äôencodeur qui serait juste en dessous.

La **taille de la liste** est un hyperparam√®tre que nous pouvons d√©finir. Il s‚Äôagirait essentiellement de **la longueur de la phrase la plus longue** dans notre ensemble de donn√©es d‚Äôentra√Ænement.

Apr√®s avoir enchass√© les mots dans notre s√©quence d‚Äôentr√©e, chacun d‚Äôentre eux traverse chacune des deux couches de l‚Äôencodeur.

 dans chacune des positions, le mot circule √† travers son propre chemin dans l‚Äôencodeur

Il y a des **d√©pendances entre ces chemins dans la couche d‚Äôauto-attention.**

**La couche feed-forward n‚Äôa pas ces d√©pendances** et donc les diff√©rents chemins peuvent √™tre ex√©cut√©s en parall√®le lors de cette couche.

#####  **3. L‚Äôencodage** 

un encodeur re√ßoit une liste de vecteurs en entr√©e. Il traite cette liste en passant ces vecteurs dans une couche d‚Äôauto-attention, puis dans un r√©seau feed-forward, et enfin envoie la sortie vers le haut au codeur suivant.

 Le mot √† chaque position passe par un processus d‚Äôauto-attention. Ensuite, chacun d‚Äôeux passe par un r√©seau feed-forward (le m√™me r√©seau feed-forward pour chaque vecteur mais chacun le traverse s√©par√©ment). 

#####  **4. Introduction √† l‚Äôauto-attention** 

Au fur et √† mesure que le mod√®le traite chaque mot (chaque position dans la s√©quence d‚Äôentr√©e, **l‚Äôauto-attention lui permet d‚Äôexaminer d‚Äôautres positions dans la s√©quence d‚Äôentr√©e √† la recherche d‚Äôindices qui peuvent aider √† un meilleur codage pour ce mot.**

L‚Äôauto-attention est la m√©thode que le Transformer utilise pour  **am√©liorer la compr√©hension du mot** qu‚Äôil est en train de traiter en  fonction des autres mots pertinents.

5. ##### L‚Äôauto-attention en d√©tail 

*1√®re √©tape*: **cr√©er 3 vecteurs √† partir de chacun des vecteurs d‚Äôentr√©e** xi de l‚Äôencodeur (dans ce cas, l‚Äôembedding de chaque mot).

Chaque vecteur d‚Äôentr√©e xi est utilis√© de 3 mani√®res diff√©rentes dans l‚Äôop√©ration d‚Äôauto-attention :

1. Il est **compar√© √† tous les autres vecteurs** pour √©tablir les **pond√©rations pour sa propre production** yi ->  forme le **vecteur de requ√™te** (**Query**)
2. Il est **compar√© √† tous les autres vecteurs** pour √©tablir les **pond√©rations pour la sortie** du j-√®me vecteur yj -> forme le **vecteur de cl√©** (**Key**).
3. Il est **utilis√© comme partie de la somme pond√©r√©e** pour **calculer chaque vecteur de sortie** une fois que les pond√©rations ont √©t√© √©tablies -> forme le **vecteur de valeur** (**Value**).

Ces vecteurs sont cr√©√©s en multipliant l‚Äôembedding par trois matrices que nous avons form√©es pendant le processus d‚Äôentra√Ænement

ces nouveaux vecteurs sont de plus petite dimension que le vecteur d‚Äôembedding; Ils n‚Äôont pas besoin d‚Äô√™tre plus petits. C‚Äôest un choix d‚Äôarchitecture pour rendre la computation des t√™tes d‚Äôattentions constante.

*2√®me √©tape*:  calculer  un score. Example: calcul de l‚Äôauto-attention pour le 1er mot; il faut noter chaque mot de la phrase d‚Äôentr√©e par rapport √† ce mot.  **Le score d√©termine le degr√© de concentration √† placer sur les autres  parties de la phrase d‚Äôentr√©e** au fur et √† mesure que nous codons un mot √† une certaine position.

score est calcul√© en prenant le **produit scalaire du vecteur de requ√™te avec le vecteur cl√©** du mot que nous √©valuons. Donc, si nous traitons l‚Äôauto-attention pour le mot en position #1, le premier score serait le produit scalaire de q1  et k1. Le deuxi√®me score serait le produit scalaire de q1 et k2.

*3√®me et 4√®me √©tape*: diviser les scores  par la racine carr√©e de la dimension des vecteurs cl√©s utilis√©s -> permet d‚Äôobtenir des gradients plus stables.

softmax peut √™tre sensible √† de tr√®s grandes valeurs d‚Äôentr√©e -> tue le gradient et ralentit l‚Äôapprentissage, ou l‚Äôarr√™te compl√®tement. 

la valeur moyenne du produit scalaire augmente avec la dimension de l‚Äôembedding, il est utile de redimensionner un peu le produit scalaire pour emp√™cher les entr√©es de la fonction softmax de devenir trop grandes.

Il pourrait y avoir d‚Äôautres valeurs possibles que la racine carr√©e de la dimension, mais c‚Äôest la valeur par d√©faut.

**Softmax** permet de normaliser les scores pour qu‚Äôils soient tous positifs et somment √† 1.

Ce score softmax d√©termine **√† quel point chaque mot sera exprim√© √† sa  position**. Il est donc logique que le mot √† sa position aura le score  softmax le plus √©lev√©, mais **le score des autres mots permet de  d√©terminer leur pertinence par rapport au mot trait√©**.

*5√®me √©tape*. **multiplier chaque vecteur de valeur par le score softmax** (en vue de les additionner) -> garder intactes les valeurs du ou des mots sur lesquels nous voulons nous concentrer, et de **noyer les mots non pertinents** (en les multipliant par de petits nombres comme 0,001, par exemple).

*6√®me √©tape*. **r√©sumer les vecteurs de valeurs pond√©r√©es**. Ceci **produit la sortie de la couche d‚Äôauto-attention √† cette position** 

Les vecteurs zi r√©sultants peuvent √™tre envoy√©s au r√©seau feed-forward. En pratique cependant, ce calcul est effectu√© sous forme de matrice pour un traitement plus rapide

6. ##### Les matrices de calcul de l‚Äôauto-attention 

*1√®re √©tape*. **calculer les matrices Requ√™te, Cl√© et Valeur**. concat√©ner les embeddings dans une matrice X et la multiplier par les matrices de poids que nous avons entra√Æn√©s 

*√©tapes 2 √† 6*. peuvent √™tre concat√©n√©es en 1 formule pour calculer les sorties de la couche  d‚Äôauto-attention.

 7. ##### La b√™te √† plusieurs t√™tes

**Au lieu d‚Äôex√©cuter une seule fonction d‚Äôattention** les auteurs de l‚Äôarticle ont trouv√© avantageux de **projeter lin√©airement les requ√™tes, les cl√©s et les valeurs h fois avec diff√©rentes projections lin√©aires** apprises sur les dimensions dk, dk et dv, respectivement.

Ce m√©canisme est appel√© ¬´ **attention multi-t√™tes** ¬ª. Cela **am√©liore les performances de la couche d‚Äôattention** de deux fa√ßons :

1. √©largit la capacit√© du mod√®le √† se concentrer sur diff√©rentes positions
   * ¬´ Marie a donn√© des roses √† Susane ¬ª: ¬´ donn√© ¬ª a des relations diff√©rentes aux diff√©rentes parties de la phrase. ¬´ Marie ¬ª exprime qui fait le don, ¬´ roses ¬ª exprime ce qui est donn√©, et ¬´ Susane ¬ª exprime qui est le destinataire. 
   * **En une seule op√©ration d‚Äôauto-attention, toutes ces informations ne font que s‚Äôadditionner** -> Si c‚Äô√©tait Suzanne qui avait donn√© les roses plut√¥t que Marie, le vecteur de sortie zdonn√© serait le m√™me, m√™me si le sens a chang√©.
2. donne √† la couche d‚Äôattention de **multiples ¬´ sous-espaces de repr√©sentation ¬ª**. 
   * avec l‚Äôattention √† plusieurs t√™tes, nous n‚Äôavons pas seulement un, mais **plusieurs ensembles de matrices de poids** Query/Key/Value (le Transformer utilise huit t√™tes d‚Äôattention, donc nous obtenons huit ensembles pour chaque encodeur/d√©codeur)
   * chacun de ces ensembles est **initialis√© au hasard**. 
   * apr√®s l‚Äôentra√Ænement, chaque ensemble est utilis√© pour projeter les embedding d‚Äôentr√©e (ou les vecteurs des encodeurs/d√©codeurs inf√©rieurs) dans un **sous-espace de repr√©sentation diff√©rent.**

Si nous faisons le m√™me calcul d‚Äôauto-attention que nous avons d√©crit ci-dessus, huit fois avec des matrices de poids diff√©rentes, nous obtenons huit matrices Z diff√©rentes.

mais la couche de feed-forward attend une matrice unique (un vecteur pour chaque mot). 

pour condenser ces huit √©l√©ments en une seule matrice: **concat√©ner** les matrices puis les multiplier par **une matrice de poids suppl√©mentaire** WO.

#####  **8. Le codage positionnel** 

 fa√ßon de **rendre compte de l‚Äôordre des mots dans la s√©quence d‚Äôentr√©e.**

 le Transformer **ajoute un vecteur √† chaque embedding** d‚Äôentr√©e. Ces vecteurs suivent un mod√®le sp√©cifique que le mod√®le apprend ce qui l‚Äôaide √† d√©terminer la position de chaque mot (ou la distance entre les diff√©rents mots dans la s√©quence). L‚Äôintuition ici est que l‚Äôajout de ces valeurs √† l‚Äôembedding **fournit des distances significatives entre les vecteurs d‚Äôembedding** une fois qu‚Äôils sont projet√©s dans les vecteurs Q/K/V (puis pendant l‚Äôapplication du produit scalaire).

diff√©rentes m√©thodes possibles pour le codage positionnel; e.g. les valeurs de la moiti√© gauche sont g√©n√©r√©es par une fonction (qui utilise le sinus), et la moiti√© droite est g√©n√©r√©e par une autre fonction (qui utilise le cosinus). Ils sont ensuite concat√©n√©s pour former chacun des vecteurs d‚Äôencodage positionnel. 

#####  **9. Les connexions r√©siduelles** 

 **chaque sous-couche (auto-attention, feed-forward) dans chaque codeur a une *connexion r√©siduelle* autour de lui et est suivie d‚Äôune √©tape de *normalisation*.**

Cela vaut √©galement pour les sous-couches du d√©codeur.

10. ##### Le decodeur 

L‚Äôencoder commence par traiter la s√©quence d‚Äôentr√©e. 

La **sortie de l‚Äôencoder sup√©rieur** est ensuite transform√©e en un ensemble de **vecteurs d‚Äôattention K et V**. 

Ceux-ci doivent √™tre **utilis√©s par chaque d√©codeur dans sa couche ¬´ attention encodeur-d√©codeur ¬ª** qui permet au decodeur de se concentrer sur les endroits appropri√©s dans la s√©quence d‚Äôentr√©e 

Chaque √©tape de la phase de d√©codage produit un √©l√©ment de la s√©quence de sortie

Les √©tapes suivantes r√©p√®tent le processus jusqu‚Äô√† ce qu‚Äôun symbole sp√©cial indique au d√©codeur que le Transformer a compl√©t√© enti√®rement la sortie. 

**La sortie de chaque √©tape (mot ici) est envoy√©e au d√©codeur le plus bas pour le traitement du mot suivant**. 

 tout comme pour les entr√©es encodeur, nous ¬´ embeddons ¬ª et **ajoutons un codage positionnel √† ces entr√©es d√©codeur** pour indiquer la position de chaque mot.

Les couches **d‚Äôauto-attention du d√©codeur** fonctionnent d‚Äôune mani√®re l√©g√®rement diff√©rente de celle de l‚Äôencodeur.

* la couche d‚Äô**auto-attention** ne peut s‚Äôoccuper **que des positions ant√©rieures** dans la s√©quence de sortie. 
  * fait en masquant les positions futures (en les r√©glant sur -inf) avant l‚Äô√©tape softmax du calcul de l‚Äôauto-attention.

* la couche ¬´ **Attention encodeur-d√©codeur** ¬ª fonctionne comme une auto-attention √† plusieurs t√™tes, sauf qu‚Äôelle cr√©e sa **matrice de requ√™tes √† partir de la couche inf√©rieure**, et prend la **matrice des cl√©s et des valeurs √† la sortie de la pile encodeur**. 

 11. ##### Les couches finales : lin√©aire et sofmax

La pile de decodeurs d√©livre un vecteur de float. 

Comment le transformer en mots ? C‚Äôest le travail de la couche Lin√©aire qui est suivie d‚Äôune couche Softmax.

La **couche lin√©aire** est un simple r√©seau neuronal enti√®rement connect√© qui **projette le vecteur produit par la pile de decodeurs dans un vecteur beaucoup (beaucoup) plus grand appel√© vecteur logits**.

Supposons que notre mod√®le connaisse 10 000 mots anglais uniques (le ¬´ vocabulaire de sortie ¬ª de notre mod√®le) qu‚Äôil a appris de son ensemble de donn√©es d‚Äôentra√Ænement. Cela rendrait le vecteur logit large de 10 000 cellules, **chaque cellule correspondant au score d‚Äôun mot unique**. C‚Äôest ainsi que nous interpr√©tons la sortie du mod√®le suivie de la couche lin√©aire.

La **couche softmax** **transforme ensuite ces scores en probabilit√©s** (tous positifs dont la somme vaut 1). La cellule ayant la probabilit√© la plus √©lev√©e est choisie et le mot qui lui est associ√© est produit comme sortie pour ce pas de temps.

#####  **12. L‚Äôentra√Ænement** 

Pendant l‚Äôentra√Ænement, un mod√®le non entra√Æn√© passerait exactement par le m√™me processus. Mais puisque nous l‚Äôentra√Ænons sur un ensemble de donn√©es d‚Äôentra√Ænement labellis√©, nous pouvons comparer sa sortie avec la sortie correcte r√©elle.

Une fois que nous avons d√©fini notre vocabulaire de sortie, nous pouvons utiliser un vecteur de la m√™me largeur pour indiquer chaque mot de notre vocabulaire. C‚Äôest ce qu‚Äôon appelle aussi le **one-hot encoding**

#####  **13. La fonction de perte** 

Comment comparer deux distributions de probabilit√©s ? Nous  soustrayons simplement l‚Äôune √† l‚Äôautre. Pour plus de d√©tails, voir  l‚Äôentropie crois√©e et la divergence de Kullback-Leibler.

 Par exemple en entr√©e : ¬´ Je suis √©tudiant ¬ª et comme r√©sultat attendu : ¬´ I am a student ¬ª. Ce que cela signifie vraiment, c‚Äôest que nous voulons que notre mod√®le produise successivement des distributions de probabilit√©s o√π :

* Chaque distribution de probabilit√© est repr√©sent√©e par un vecteur de largeur vocab_size (6 dans notre exemple, mais de fa√ßon plus r√©aliste un nombre comme 3 000 ou 10 000)
* La premi√®re distribution de probabilit√©s a la probabilit√© la plus √©lev√©e √† la cellule associ√©e au mot ¬´ I ¬ª
* La deuxi√®me distribution de probabilit√© a la probabilit√© la plus √©lev√©e √† la cellule associ√©e au mot ¬´ am ¬ª
* Et ainsi de suite jusqu‚Äô√† ce que la cinqui√®me distribution de sortie indique ‚Äò‚Äô, auquel est √©galement associ√©e une cellule du vocabulaire √† 10 000 √©l√©ments



### [LE SEQ2SEQ ET LE PROCESSUS D‚ÄôATTENTION](https://lbourdois.github.io/blog/nlp/Seq2seq-et-attention/)               

Un sequence-to-sequence model est un mod√®le qui prend une s√©quence d‚Äô√©l√©ments (mots, lettres, caract√©ristiques d‚Äôune image‚Ä¶etc) et en sort une autre s√©quence. 

Sous le capot, le mod√®le est compos√© d‚Äôun encodeur et d‚Äôun d√©codeur.

L‚Äô **encodeur** traite chaque √©l√©ment de la s√©quence d‚Äôentr√©e. Il compile les informations qu‚Äôil capture dans un vecteur (appel√© **context**). Apr√®s avoir trait√© toute la s√©quence d‚Äôentr√©e, l‚Äôencodeur envoie le context au **d√©codeur**, qui commence √† produire la s√©quence de sortie item par item.

L‚Äôencodeur et le d√©codeur ont tendance √† √™tre tous deux des r√©seaux neuronaux r√©currents.

Le **vecteur de contexte s‚Äôest av√©r√© √™tre un goulot d‚Äô√©tranglement** pour ces types de mod√®les. Il √©tait donc difficile pour les mod√®les de composer avec de longues phrases. Une solution a √©t√© propos√©e dans Bahdanau et al., 2014 et Luong et al., 2015. Ces articles introduisirent et affin√®rent une technique appel√©e ¬´ **Attention** ¬ª, qui am√©liora consid√©rablement la qualit√© des syst√®mes de traduction automatique. L‚Äôattention **permet au mod√®le de se concentrer sur les parties pertinentes de la s√©quence d‚Äôentr√©e si n√©cessaire.**

Cette **capacit√© d‚Äôamplifier le signal de la partie pertinente** de la s√©quence d‚Äôentr√©e permet aux mod√®les d‚Äôattention de produire de meilleurs r√©sultats que les mod√®les sans attention.

Un mod√®le d‚Äôattention diff√®re d‚Äôun sequence-to-sequence model classique de deux fa√ßons principales :

1. **l‚Äôencodeur transmet beaucoup plus de donn√©es au decodeur**. 
   * Au lieu de passer le dernier √©tat cach√© de l‚Äô√©tape d‚Äôencodage, **l‚Äôencodeur passe tous les √©tats cach√©s au decodeur **
2.  un d√©codeur d‚Äôattention fait une √©tape suppl√©mentaire avant de produire sa sortie. Pour se concentrer sur les parties de l‚Äôentr√©e qui sont pertinentes, le d√©codeur
   * **regarde l‚Äôensemble des √©tats cach√©s de l‚Äôencodeur** qu‚Äôil a re√ßu (chaque √©tat cach√© de l‚Äôencoder est le plus souvent associ√© √† un certain mot dans la phrase d‚Äôentr√©e).
   * donne un **score √† chaque √©tat cach√©** 
   * **multiplie chaque √©tat cach√© par son score** attribu√© via softmax (amplifiant ainsi les √©tats cach√©s avec des scores √©lev√©s, et noyant les √©tats cach√©s avec des scores faibles)

 Le ¬´ scorage ¬ª se fait √† chaque pas de temps (nouveau mot) du c√¥t√© du d√©codeur.

 le mod√®le n‚Äôassocie pas seulement le premier mot de la sortie avec le premier mot de l‚Äôentr√©e. En fait, il a appris pendant la phase d‚Äôentrainement la fa√ßon dont sont li√©s les mots dans cette paire de langues (le fran√ßais et l‚Äôanglais dans notre exemple). 

###  [LES RNN, LES LSTM, LES GRU ET ELMO](https://lbourdois.github.io/blog/nlp/RNN-LSTM-GRU-ELMO/) 

Les RNN (recurrent neural network ou r√©seaux de neurones r√©currents en fran√ßais) sont des r√©seaux de neurones qui ont jusqu‚Äô√† encore 2017/2018, √©t√© majoritairement utilis√© dans le cadre de probl√®me de NLP.

Cette architecture poss√®de un probl√®me. **Lorsque la s√©quence √† traiter est trop longue, la r√©tropropagation du gradient de l‚Äôerreur peut soit devenir beaucoup trop grande et exploser, soit au contraire devenir beaucoup trop petite**. **Le r√©seau ne fait alors plus la diff√©rence entre une information qu‚Äôil doit prendre en compte ou non**. Il se trouve ainsi dans l‚Äôincapacit√© d‚Äôapprendre √† long terme. 

solution propos√©e par Les LSTM: prendre en compte un **vecteur m√©moire** **via un syst√®me de 3 portes (gates) et 2 √©tats**

- **Forget** **gate** (capacit√© √† oublier de l‚Äôinformation, quand celle-ci est inutile)
- **Input** **gate** (capacit√© √† prendre en compte de nouvelles informations utiles)
- **Output** **gate** (quel est l‚Äô√©tat de la cellule √† l‚Äôinstant t sachant la forget et la input gate)
- **Hidden** **state** (√©tat cach√©)
- **Cell** **state** (√©tat de la cellule)

**GRU** = une variante des LSTM; structure plus simple que les LSTM car moins de param√®tres entrent en jeu. **2 portes et 1 √©tat**:

* **Reset** **gate** (porte de reset)
* **Update** **gate** (porte de mise √† jour)
* **Cell** **state** (√©tat de la cellule)

En pratique, les GRU et les LSTM permettent d‚Äôobtenir des r√©sultats comparables. **L‚Äôint√©r√™t des GRU par rapport aux LSTM √©tant le temps d‚Äôex√©cution qui est plus rapide puisque moins de param√®tres doivent √™tre calcul√©s.**



#####  ELMo (l‚Äôimportance du contexte)

Embeddings from Language Models = bas√© sur un LSTM bidirectionnel

un mot peut avoir plusieurs sens selon la mani√®re dont o√π il est utilis√©; Pourquoi ne pas lui donner un embedding bas√© sur le contexte dans lequel il est utilis√© ? A la fois pour capturer le sens du mot dans ce contexte ainsi que d‚Äôautres informations contextuelles ->**contextualized word-embeddings**.

Au lieu d‚Äôutiliser un embedding fixe pour chaque mot, **ELMo examine l‚Äôensemble de la phrase avant d‚Äôassigner une embedding** √† chaque mot qu‚Äôelle contient. Il utilise un **LSTM bidirectionnel form√© sur une t√¢che sp√©cifique pour pouvoir cr√©er ces embedding**.

ELMo a constitu√© **un pas important vers le pr√©-entra√Ænement** dans le contexte du NLP. En effet, nous pouvons l‚Äôentra√Æner sur un ensemble massif de donn√©es dans la langue de notre ensemble de donn√©es, et ensuite nous pouvons **l‚Äôutiliser comme un composant dans d‚Äôautres mod√®les** qui ont besoin de traiter le langage.

Plus pr√©cis√©ment, **ELMo est entra√Æn√© √† pr√©dire le mot suivant dans une s√©quence de mots** ‚Äì une t√¢che appel√©e mod√©lisation du langage (Language Modeling). C‚Äôest pratique car nous disposons d‚Äôune grande quantit√© de donn√©es textuelles dont un tel mod√®le peut s‚Äôinspirer sans avoir besoin de labellisation.

‚Äã                                                                                                                              

ELMo va m√™me plus loin et forme un **LSTM bidirectionnel**

ELMo propose **l‚Äôembedding contextualis√© en regroupant les √©tats cach√©s (et l‚Äôembedding initial)** d‚Äôune certaine mani√®re (concat√©nation suivie d‚Äôune sommation pond√©r√©e).



### [ILLUSTRATION DU GPT2](https://lbourdois.github.io/blog/nlp/GPT2/)               

bas√© sur un Transformer entra√Æn√© sur un ensemble de donn√©es massif. 

le mod√®le de Transformer original est compos√© d‚Äôun encodeur et d‚Äôun  d√©codeur (chacun est une pile de ce que nous pouvons appeler des  transformer blocks). Cette architecture est appropri√©e parce que le  mod√®le s‚Äôattaque √† la traduction automatique

##### **Une diff√©rence par rapport √† BERT** 

**Le GPT-2 est construit √† l‚Äôaide de blocs d√©codeurs**. 

**BERT, pour sa  part, utilise des blocs d‚Äôencodeurs**. 

l‚Äôune des principales diff√©rences entre les  deux est que **le GPT-2, comme les mod√®les de langage traditionnels,  produit un seul token √† la fois**.

 Invitons par exemple un GPT-2 bien  entra√Æn√© √† r√©citer la premi√®re loi de la robotique :  ¬´ A robot may not  injure a human being or, through inaction, allow a human being to come  to harm ¬ª.

La fa√ßon dont fonctionnent r√©ellement ces mod√®les est qu‚Äô**apr√®s chaque token produit, le token est ajout√© √† la s√©quence des entr√©e**s. Cette nouvelle s√©quence devient l‚Äôentr√©e du mod√®le pour la prochaine √©tape. Cette id√©e est appel√©e ¬´ **autor√©gression** ¬ª et a permis aux RNN d‚Äô√™tre efficaces.

**Le GPT2 et certains mod√®les plus r√©cents comme TransformerXL et XLNet sont de nature autor√©gressive**; **BERT ne l‚Äôest pas.**

C‚Äôest un compromis. **En perdant l‚Äôautor√©gression, BERT a acquis la  capacit√© d‚Äôincorporer le contexte des deux c√¥t√©s d‚Äôun mot** pour obtenir  de meilleurs r√©sultats. 

**XLNet ram√®ne l‚Äôautor√©gression tout en trouvant  une autre fa√ßon d‚Äôint√©grer le contexte des deux c√¥t√©s.**

 Un bloc encodeur de l‚Äôarticle d‚Äôorigine peut recevoir des entr√©es jusqu‚Äô√† une certaine longueur maximale (512 tokens). Si une s√©quence d‚Äôentr√©e est plus courte que cette limite, nous avons simplement √† rembourrer le reste de la s√©quence. (ajouter <pad>)

**Une diff√©rence cl√© dans la couche d‚Äôauto-attention est qu‚Äôelle masque les futurs tokens ‚Äì non pas en changeant le mot en [mask] comme BERT,  mais en interf√©rant dans le calcul de l‚Äôauto-attention en bloquant les  informations des tokens qui sont √† la droite de la position √† calculer.**

Un bloc d‚Äôauto-attention normal permet √† une position d‚Äôatteindre le  sommet des tokens √† sa droite. L‚Äôauto-attention masqu√©e emp√™che que cela se produise 

Le mod√®le OpenAI GPT-2 utilise uniquement ces blocs d√©codeurs.  tr√®s similaires aux blocs d√©codeurs d‚Äôorigine, sauf qu‚Äôils suppriment la deuxi√®me couche d‚Äôauto-attention.

 Le GPT-2 peut traiter 1024 jetons. Chaque jeton parcourt tous les blocs d√©codeurs le long de son propre chemin. 

La fa√ßon la plus simple d‚Äôex√©cuter un GPT-2 entra√Æn√© est de lui  permettre de se promener de lui-m√™me (ce qui est techniquement appel√©  **generating unconditional samples**).   

Nous pouvons aussi le pousser √† ce  qu‚Äôil parle d‚Äôun certain sujet (**generating interactive conditional  samples**). Dans le premier cas, nous pouvons simplement lui donner le  token de d√©marrage et lui faire commencer √† g√©n√©rer des mots (le mod√®le  utilise *<|endoftext|>* comme token de d√©marrage. Appelons-le < s > √† la place pour simplifier les graphiques).

Le token est trait√© successivement √† travers toutes les couches, puis un vecteur est produit le long de ce chemin. 

GPT-2 a un param√®tre appel√© top-k que nous pouvons utiliser pour que le mod√®le consid√®re des mots d‚Äô√©chantillonnage autres que le top mot (ce qui est le cas lorsque top-k = 1).



##### encodage de l'entr√©e

Comme dans d‚Äôautres mod√®les de NLP, le GPT-2 recherche l‚Äô**embedding** du mot d‚Äôentr√©e dans son embedding matrix (obtenue apr√®s entra√Ænement).

Ainsi au d√©but nous recherchons l‚Äôembedding du token de d√©part < s > dans la matrice. Avant de transmettre cela au premier bloc du  mod√®le, nous devons incorporer le **codage positionnel** (un signal qui  indique aux blocs l‚Äôordre des mots dans la s√©quence). Une partie du  mod√®le entra√Æn√© contient une matrice ayant un vecteur de codage  positionnel pour chacune des 1024 positions de l‚Äôentr√©e.

 Envoyer un mot au premier bloc du Transformer, c‚Äôest rechercher son embedding et additionner le vecteur de codage positionnel pour la position #1.



##### voyage dans le bloc

Le premier bloc peut maintenant traiter le premier token en le  faisant passer d‚Äôabord par le processus d‚Äôauto-attention, puis par sa  couche feed forward. 

Une fois le traitement effectu√©, le bloc envoie le  vecteur r√©sultant pour qu‚Äôil soit trait√© par le bloc suivant. 

Le  processus est identique dans chaque bloc mais **chaque bloc a des poids  qui lui sont propres dans l‚Äôauto-attention et dans les sous-couches du  r√©seau neuronal**.



##### l'auto-attention

 on attribue des notes √† la pertinence de chaque mot du segment et additionne leur repr√©sentation vectorielle.

##### Le processus de l‚Äôauto-attention 

L‚Äôauto-attention est trait√©e le long du parcours de chaque token. Les composantes significatives sont **trois vecteurs** :

- **Query** : la requ√™te est une **repr√©sentation du mot  courant**. Elle est **utilis√©e pour scorer le mot vis-√†-vis des autres mots**  (en utilisant leurs cl√©s).
- **Key** : les vecteurs cl√©s sont comme des **labels**  pour tous les mots de la s√©quence. C‚Äôest **contre eux que nous nous  mesurons dans notre recherche de mots pertinents**.
- **Value** : les vecteurs de valeurs sont des  **repr√©sentations de mots r√©els**. Une fois que nous avons √©valu√© la  pertinence de chaque mot, ce sont les valeurs que nous **additionnons pour repr√©senter le mot courant**.

Une analogie grossi√®re est de penser √† la recherche dans un classeur. 

* La **requ√™te (query) est le sujet que vous recherchez.** 
* Les **cl√©s (key)  sont comme les √©tiquettes** des chemises √† l‚Äôint√©rieur de l‚Äôarmoire. 
* Lorsque vous faites correspondre la requ√™te et la cl√©, nous enlevons le  contenu du dossier. **Le contenu correspond au vecteur de valeur (value)**.  

Sauf que vous ne recherchez pas seulement une valeur, mais un m√©lange de valeurs √† partir d‚Äôun m√©lange de dossiers.

**Multiplier le vecteur de requ√™te par chaque vecteur cl√© produit un score** pour chaque dossier (techniquement : le produit scalaire suivi de softmax).

**Nous multiplions chaque valeur par son score et sommons. Cela donne le r√©sultat de notre auto-attention.**

Cette op√©ration permet d‚Äôobtenir un vecteur pond√©r√©



##### **Sortie du mod√®le**

Lorsque le bloc le plus haut du mod√®le produit son vecteur de sortie (le r√©sultat de sa propre auto-attention suivie de son propre r√©seau feed-forward), le mod√®le **multiplie ce vecteur par la matrice d‚Äôembedding**.

chaque ligne de la matrice d‚Äôembedding correspond √† un word embedding dans le vocabulaire du mod√®le. Le r√©sultat de cette multiplication est interpr√©t√© comme **un score pour chaque mot du vocabulaire** du mod√®le.

Nous pouvons simplement s√©lectionner le token avec le score le plus √©lev√© (top_k = 1). Mais de meilleurs r√©sultats sont obtenus si le mod√®le tient √©galement compte d‚Äôautres termes. Ainsi, une bonne strat√©gie consiste √† tirer au hasard un mot provenant du vocabulaire. **Chaque mot ayant comme probabilit√© d‚Äô√™tre s√©lectionner, le score qui lui a √©t√© attribu√©** (de sorte que les mots avec un score plus √©lev√© ont une plus grande chance d‚Äô√™tre s√©lectionn√©s). Un terrain d‚Äôentente consiste √† **fixer top_k √† 40,** et √† demander au mod√®le de prendre en compte les 40 mots ayant obtenu les scores les plus √©lev√©s.

Le mod√®le a alors termin√© une it√©ration aboutissant √† l‚Äô√©dition d‚Äôun  seul mot. **Le mod√®le it√®re alors jusqu‚Äô√† ce que le contexte entier soit  g√©n√©r√© (1024 tokens) ou qu‚Äôun token de fin de s√©quence soit produit.**

en r√©alit√©, le GPT2 utilise le **Byte Pair Encoding** pour cr√©er les tokens dans son vocabulaire = les tokens sont g√©n√©ralement des parties des mots



##### **Auto-attention (sans masking)**

 dans un bloc encoder. L‚Äôauto-attention s‚Äôapplique en **trois** **√©tapes** principales :

1. **Cr√©ation des vecteurs Query, Key et Value** pour chaque chemin.
2. Pour chaque token d‚Äôentr√©e, on utilise son **vecteur de requ√™te pour lui attribuer un score par rapport √† tous les autres vecteurs cl√©s.**
3. **Sommation des vecteurs de valeurs apr√®s les avoir multipli√©s par leurs scores associ√©s**. 



##### **Cr√©ation des vecteurs Query, Key et Value** 

1. Pour le premier token, nous prenons sa requ√™te et la comparons √†  toutes les cl√©s. Cela produit un score pour chaque cl√©. La **premi√®re  √©tape de l‚Äôauto-attention consiste √† calculer les trois vecteurs pour  chaque token**  (en multipliant par la matrice de poids) (ignorons les t√™tes d‚Äôattention pour le moment) 
2. **multiplions sa requ√™te par tous les autres vecteurs cl√©s** pour obtenir un score pour chacun des quatre tokens.
3. **multiplier les scores par les vecteurs de valeurs**. Une valeur avec un score √©lev√© constituera une grande partie du vecteur r√©sultant une fois que nous les aurons additionn√©s.

Si nous faisons **la m√™me op√©ration pour chaque token, nous obtenons** **un vecteur repr√©sentant et tenant compte du contexte pour chacun d‚Äôeux**. Ces vecteurs sont ensuite pr√©sent√©s √† la sous-couche suivante du bloc (le r√©seau de neurones feed-forward).



##### Auto-attention (avec masking)

identique √† l‚Äôauto-attention, sauf √† l‚Äô√©tape 2.

Supposons que le mod√®le n‚Äôa que deux tokens en entr√©e et que nous observons le deuxi√®me token. Dans ce cas, les deux derniers tokens sont masqu√©s. Le mod√®le attribue alors toujours aux futurs tokens un score de 0.

Ce ¬´ masquage ¬ª est souvent mis en ≈ìuvre sous la forme d‚Äôune matrice appel√©e **masque d‚Äôattention**.

A titre d‚Äôexemple, supposons avoir une s√©quence de quatre mots : ¬´ robot must obey orders ¬ª.

Sous forme matricielle, nous calculons les scores en multipliant une matrice de requ√™tes par une matrice cl√©s 

le masque d‚Äôattention **r√®gle les cellules que l‚Äôon veut **masquer sur **-inf ou un nombre n√©gatif tr√®s important** (par exemple -1 milliard pour le GPT2) 

l‚Äôapplication de softmax produit les scores r√©els que nous utilisons pour l‚Äôauto-attention

#####  L‚Äôauto-attention masqu√©e du GPT-2

**Le GPT-2 conserve les vecteurs cl√© et valeur des tokens qu‚Äôil a d√©j√† trait√© afin de ne peut √† avoir √† les recalculer √† chaque fois** qu‚Äôun nouveau token est trait√©. 

*Etape 1 : Cr√©ation des vecteurs Query, Key et Value*

Chaque bloc d‚Äôun Transformer a ses propres poids. Nous nous servons de **la matrice de poids pour cr√©er les vecteurs des requ√™tes, des cl√©s et des valeurs**. Cela consiste en pratique √† une **simple multiplication**.

 En multipliant le vecteur d‚Äôentr√©e par le vecteur de poids d‚Äôattention (et en ajoutant un vecteur de biais non repr√©sent√© ici), on obtient les vecteurs cl√©, valeur et requ√™te pour ce token. 

*Les t√™tes d'attention*

**L‚Äôauto-attention est men√©e plusieurs fois sur diff√©rentes parties des vecteurs Q,K,V.**

S√©parer les t√™tes d‚Äôattention, c‚Äôest simplement **reconstruire le vecteur long sous forme de matrice.**

Le plus petit GPT2 poss√®de 12 t√™tes d‚Äôattention. Il s‚Äôagit donc de la premi√®re dimension de la matrice remodel√©e 

 *Etape 2 : Scoring*

 *Etape 3 : Somme*

nous multiplions maintenant chaque valeur par son score, puis nous les additionnons pour obtenir le r√©sultat de l‚Äôattention port√©e √† la t√™te d‚Äôattention n¬∞1 

 *Fusion des t√™tes d‚Äôattention*

Nous concat√©nons les t√™tes d‚Äôattention.

 *Etape 4 : Projection*

la 2√®me grande matrice de poids qui projette les r√©sultats des t√™tes d‚Äôattention dans le vecteur de sortie de la sous-couche d‚Äôauto-attention 

 *Etape 5 : Fully Connected Neural Network*

Le r√©seau neuronal enti√®rement connect√© est l‚Äôendroit o√π le bloc traite son token d‚Äôentr√©e apr√®s que l‚Äôauto-attention a inclus le contexte appropri√© dans sa repr√©sentation. Il est compos√© de **deux couches**.

La premi√®re couche est quatre fois plus grande que le mod√®le (768x4 =  3072). Cela semble donner aux mod√®les  de Transformer une capacit√© de repr√©sentation suffisante pour faire face aux t√¢ches qui leur ont √©t√© confi√©es jusqu‚Äô√† pr√©sent.

La deuxi√®me couche projette le r√©sultat de la premi√®re couche dans la dimension du mod√®le (768 pour le petit GPT2). **Le r√©sultat de cette multiplication est le r√©sultat du bloc Transformer pour ce token.**

##### r√©sum√©

Chaque bloc a **son propre jeu de ces poids**. D‚Äôautre part, le mod√®le n‚Äôa qu‚Äô**une seule matrice d‚Äôembedding** de token et **une seule matrice de codage positionnel** 



### [bert-base-uncased](https://huggingface.co/bert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France)



BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion.

it was pretrained with two objectives:

- **Masked language modeling** (MLM)
- **Next sentence prediction** (NSP)

This way, the model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard classifier using the features produced by the BERT model as inputs.

 **this model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering. For tasks such as text generation you should look at model like GPT2.**

The inputs of the model are then of the form:

[CLS] Sentence A [SEP] Sentence B [SEP]

With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that **what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two "sentences" has a combined length of less than 512 tokens.**

The details of the masking procedure for each sentence are the following:

15% of the tokens are masked.
In 80% of the cases, the masked tokens are replaced by [MASK].
In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.
In the 10% remaining cases, the masked tokens are left as is.



### [Utilisez les r√©seaux neuronaux de transformateurs](https://www.wolfram.com/language/12/neural-network-framework/use-transformer-neural-nets.html.fr?product=language)

[AttentionLayer](http://reference.wolfram.com/language/ref/AttentionLayer.html) peut exploiter les d√©pendances √† long terme au sein des s√©quences d'une mani√®re beaucoup plus directe que les couches r√©currentes telles que [LongShortTermMemoryLayer](http://reference.wolfram.com/language/ref/LongShortTermMemoryLayer.html) et [GatedRecurrentLayer](http://reference.wolfram.com/language/ref/GatedRecurrentLayer.html).

**GTP** a une architecture similaire √† celle de BERT. Sa principale diff√©rence r√©side dans le fait qu'il utilise une **architecture d'auto-attention causale**, au lieu d'une architecture d'auto-attention simple.

**L'attention causale est moins efficace dans le traitement de texte  car un token particulier ne peut pas obtenir d'informations sur de  futurs tokens.** 

D'autre part, **une attention causale est n√©cessaire pour la g√©n√©ration de texte** : **GPT est capable de g√©n√©rer des phrases, alors que BERT ne peut que les traiter.** 



### [Apprentissage par transfert pour l‚Äôextraction de relations pharmacog√©nomiques √† partir de textes](https://hal.inria.fr/hal-02939161/document)



 BERT atteint ce haut
niveau de performance gr√¢ce √† l‚Äôutilisation des diff√©rentes techniques de pointe dans le
domaine de l‚Äôapprentissage profond combin√©es avec une architecture de transformateur
originale (le premier transformateur qui capture le contexte bidirectionnel) sans oublier le
m√©canisme d‚Äôattention et l‚Äôapprentissage par transfert qui constituent les piliers du succ√®s
de BERT. 

. Avant les transformateurs, les r√©seaux de neurones r√©cur-
rents (RNN) et les r√©seaux de neurones convolutionnels (CNN) √©taient les architectures
d‚Äôapprentissage profond les plus utilis√©es

##### RNN

peuvent traiter l‚Äôinformation contextuelle √† travers
des liens de r√©currence et peuvent par cette m√©thode m√©moriser l‚Äôinformation dans les s√©-
quence

. LSTM (Long short-term memory) et GRU (Gated recurrent unit) sont les types
de RNN les plus standards gr√¢ce √† leur gestion du flux par des portes, proc√©d√© qui per-
met aux LSTM et GRU de pallier le probl√®me de la disparition du gradient (vanishing
gradient) et d‚Äôaugmenter ainsi la longueur des s√©quences m√©moris√©e

. Les RNN ont la
capacit√© de capturer le contexte, 

: les blocs LSTM peuvent √™tre utilis√©s s√©quentiellement
sous forme d‚Äôune cha√Æne ou sous forme d‚Äôun arbre de LSTM[36]qui forme un r√©seau

. Les arbres LSTM peuvent notamment
servir √† apprendre des structures sous forme d‚Äôarbre comme par exemple les graphes de
d√©pendances qui sont des arbres binaires qui repr√©sente la structure lexicale d‚Äôune phrase

Pour
obtenir l‚Äôarbre de d√©pendance on a souvent besoin d‚Äôun parseur qui d√©finit les liens de
d√©pendance entre les mots dans la phrase

. La structure des LSTM correspond √† cet arbre

une architecture parall√®le compos√©e d‚Äôune LSTM s√©quentielle
et d‚Äôun arbre LSTM qui sert √† capturer les caract√©ristiques structurelles ; la partie LSTM
s√©quentielle est suivie par l‚Äôattention et fournit une information sur la pertinence entre
les mots √† la deuxi√®me partie (l‚Äôarbre LSTM). 

##### CNN

Le traitement automatique des langues les a adopt√©es en raison de leur grande capacit√©
d‚Äôextraction des caract√©ristiques locales √† l‚Äôaide des filtres de convolution

 L‚Äôextraction
des caract√©ristiques s‚Äôop√®re hi√©rarchiquement, partant des caract√©ristiques tr√®s locales et
tr√®s simples, apprises dans les premi√®res couches et allant jusqu‚Äô√† des **caract√©ristiques**
**compliqu√©es apprises dans les couches profondes**

avantage de
poss√©der un nombre r√©duit de param√®tres gr√¢ce au partage des param√®tres (les filtres
de convolution)

une architec-
ture mixte qui utilise en parall√®le CNN et RNN, la partie RNN permet d‚Äôextraire des
vecteurs de repr√©sentation en exploitant l‚Äôinformation contextuelle √† travers un BiLSTM
combin√© avec les caract√©ristiques locales extraites par un CNN, un plongement de posi-
tion relative fournit une information structurelle au mod√®le et le m√©canisme d‚Äôattention
(2.1) a √©t√© √©galement utilis√© avec un LSTM pour mieux capturer le contexte

##### M√©canisme d‚Äôattention

capacit√© de focalisation sur l‚Äôinformation pertinent

<u>S√©quence √† s√©quence</u>

introduit pour la 1√®re fois avec RNN pour architecture s√©quence √† s√©quence

 LSTM et GRU sont plus adapt√©s aux longues phrases que le RNN simple, mais le
probl√®me des longues phrases demeure.

 **architecture s√©quence √† s√©quence** = une
architecture compos√©e d‚Äôune partie encodeur et d‚Äôune partie d√©codeur.

*  **L‚Äôencodeur** prend
  une s√©quence en entr√©e et g√©n√®re un **vecteur contextuel** en sortie (le dernier vecteur d‚Äô√©tat).
* Ce vecteur r√©sume en quelque sorte la s√©quence en entr√©e, puis il deviendra l‚Äôentr√©e du
  **d√©codeur** qui va g√©n√©rer la s√©quence en sortie au long des it√©rations 
*  dans chaque it√©ration
  le d√©codeur utilise le vecteur de contexte et le vecteur de l‚Äô√©tat pr√©c√©dent pour g√©n√©rer la
  sortie, associ√©e √† l‚Äôit√©ration en question. 

**s√©quence √† s√©quence avec l‚Äôatten-**
**tion,** le m√©canisme d‚Äôattention est appliqu√© dans la phase de d√©codage, 

* une fonction
  de similarit√© (souvent un produit scalaire) appliqu√©e entre les vecteurs de contexte et les vecteurs d‚Äô√©tat de la phase d‚Äôenco-
  dage
* fournit des scores de similarit√© associ√©s aux vecteurs
  d‚Äô√©tat d‚Äôencodeur. 
* scores normalis√©s par une fonction softmax afin d‚Äôobtenir
  un **vecteur d‚Äôattention**
  * constitue avec le vecteur de contexte les entr√©es du
    d√©codeur
  * calcul√© √† chaque it√©ration de d√©codage afin d‚Äôobtenir
    le vecteur de contexte accord√© √† cette it√©ration. 

√Ä chaque it√©ration le m√©canisme d‚Äôatten-
tion permet donc d‚Äôavoir un **vecteur de contexte qui repr√©sente un r√©sum√© s√©lectif de la**
**s√©quence d‚Äôentr√©e**. 

**Plus un vecteur d‚Äô√©tat est similaire au vecteur de contexte associ√© √†**
**cette it√©ration, plus il contribue au vecteur en sortie du d√©codeur**

<u>L‚Äôauto-attention √† plusieurs t√™tes</u>

est un moyen de calculer la pertinence d‚Äôun ensemble
de valeurs en fonction de certaines cl√©s et requ√™tes selon plusieurs t√™tes

Le m√©canisme d‚Äôattention permet au mod√®le de se concentrer sur des informations
pertinentes en fonction de ce qu‚Äôil traite actuellement

 cette focalisation peut porter sur
plusieurs caract√©ristiques diff√©rentes -> n√©cessite de calculer l‚Äôattention selon plu-
sieurs t√™tes parall√®les

 **Chaque t√™te r√©alise une focalisation diff√©rente**, ce qui permet au
transformateur de calculer diverses repr√©sentations √† travers diff√©rentes transformations
lin√©aires, et donc de capturer plusieurs aspects sur les entr√©es

*Les vecteurs d‚Äôattention*

1√®re √©tape de l‚Äôattention multi-t√™tes: calculer 3 transforma-
tions sur chaque vecteur de repr√©sentation en entr√©e

transformations lin√©aires calcul√©es √† partir des **multiplications de vecteurs de repr√©sentation avec trois matrices**,
ces matrices √©tant des poids appris dans la phase d‚Äôentra√Ænement comme les autres poids
dans le r√©seau

matrices depoids associ√©es respectivement aux requ√™tes, cl√©s, valeurs.

*Attention selon une t√™te*

produit scalaire refl√®te le degr√© d‚Äôalignement des deux vecteurs, plus le produit sca-
laire entre deux vecteurs est grand plus les vecteurs sont align√©s et donc similaires

 **le produit scalaire est utilis√© comme une fonction de similarit√© qui sert √† calculer les scores de similarit√© entre les vecteurs requ√™tes et cl√©**

La dimension des vecteurs utilis√©s impacte la plage des valeurs, une **mise √† l'√©chelle**
est donc appliqu√©e sur le produit scalaire en divisant ce produit par la racine de la di-
mension

 la distribution des
scores est normalis√©e par une fonction **softmax**

 Le r√©sultat de
cette normalisation est le **vecteur d‚Äôattention**, ce vecteur sera par la suite multipli√© par le
vecteur des valeurs pour g√©n√©rer le vecteur de repr√©sentation

Ces **op√©rations d‚Äôattention permettent aux diff√©rents sous-mots de contribuer**
**√† la repr√©sentation de chaque sous-mot selon la similitude entre le sous-mot en question et**
**les autres sous-mots** 

(l‚Äô**attention** est calcul√©e **entre la cl√©** de chaque sous-mot **et la requ√™te**
du sous-mot dont on est en train de g√©n√©rer la repr√©sentation). 

Soit deux sous-mots A et
B. Plus le sous-mot A est similaire √† un sous-mot B, plus il contribue √† la repr√©sentation
de B, ce qui **permet au transformateur de capturer des relations de longueur arbitraire**
**entre les mots** dans la phrase contrairement au RNN.

*L‚Äôattention multi t√™tes*

Pour **apprendre diverses repr√©sentations**, l‚Äôattention multi-t√™tes applique des trans-
formations lin√©aires aux vecteurs, cl√©s et requ√™tes pour chaque t√™te d‚Äôattention.

Une fois que les vecteurs de repr√©sentation sont calcul√©s selon
les diff√©rentes t√™tes, ces vecteurs seront **concat√©n√©s** pour construire un vecteur de repr√©sen-
tation. 

Une transformation lin√©aire est appliqu√©e sur ce dernier pour **r√©duire sa dimension**,
comme le montrent l‚Äô√©quation

 Les
t√™tes √©tant ind√©pendantes, le calcul peut √™tre r√©alis√© **en parall√®le**.

##### Apprentissage par transfert

 l‚Äôapprentissage par transfert supervis√© se divise
en deux grandes cat√©gories : 

1. le **transfert inductif** = transf√©rer l‚Äôinformation entre des t√¢ches diff√©rentes mais dans le m√™me domaine
2.  le **transfert transductif**. = transf√©rer de l‚Äôinformation entre des t√¢ches similaires alors que les domaines correspon-
   dants sont diff√©rents (diff√©rentes probabilit√©s marginales);

##### Strat√©gies d‚Äôapprentissage par transfert

<u>Fine tuning</u>

* r√©cup√©rer un mod√®le pr√©-entra√Æn√© ou une partie d‚Äôun mod√®le
  pr√©-entra√Æn√© et 
*  l‚Äôutiliser comme un mod√®le initial en ajoutant un nombre r√©duit de
  param√®tres (couches) et en reprenant l‚Äôapprentissage.
*  Le mod√®le source (pr√©-entra√Æn√©)
  aide ainsi le mod√®le cible √† ajuster les poids en fournissant une bonne initialisation des
  poids.

* g√©n√©ralement les mod√®les pr√©-entra√Æn√©s sont entra√Æn√©s sur plusieurs t√¢ches et sur
  une grande quantit√© de donn√©es, le mod√®le transf√©r√© a donc d√©j√† une bonne capacit√© de
  repr√©sentation et ne n√©cessite que peu de donn√©es dans le domaine cible et moins de temps
  d‚Äôapprentissage pour adapter le mod√®le pr√©-entra√Æn√© √† la t√¢che cible. 
* largement utilis√©e avec les transformateurs (BERT par exemple).

<u>Frozen</u>

* Les mod√®les d‚Äôapprentissage profond sont des architectures en couches qui apprennent
  diff√©rentes caract√©ristiques √† diff√©rents niveaux (repr√©sentations hi√©rarchiques de caract√©-
  ristiques en couches).
*  Ces couches sont connect√©es √† une derni√®re couche (g√©n√©ralement
  une couche enti√®rement connect√©e dans le cas de l‚Äôapprentissage supervis√©) pour obtenir
  la sortie finale. 
* utiliser un r√©seau pr√©-entra√Æn√©
  (tel que BERT) comme un extracteur des caract√©ristiques si l‚Äôon enl√®ve la derni√®re couche.
* Apr√®s l‚Äôextraction des caract√©ristiques on peut utiliser n‚Äôimporte quel classifieur (SVM
  par exemple) pour classifier les objets d‚Äôint√©r√™t sur la base des caract√©ristiques fournirent
  par le mod√®le pr√©-entrain√©. 
* Une m√©thode similaire permet de faire un apprentissage en
  utilisant la strat√©gie qui consiste √† arr√™ter l‚Äôapprentissage sur certains poids ou couches au
  bout d‚Äôun moment, autrement dit √† figer une partie du r√©seau tandis que le reste continue
  l‚Äôapprentissage, ainsi la r√©tro-propagation du gradient s‚Äôapplique uniquement sur la partie
  non gel√©e du r√©seau.
*  On peut consid√©rer la partie fig√©e comme un extracteur des caract√©-
  ristiques et la partie non fig√©e comme un classifieur initialis√© √† l‚Äôaide du pr√©-entra√Ænement
  appliqu√© pr√©alablement.
* frozen = Un r√©seau dont une partie est fig√©e 
* temps d'ex√©cution: frozen plus avantageuse
  par rapport au fine-tuning car le nombre des poids √† ajuster dans le frozen est bien
  moins √©lev√© (temps d‚Äôune seule inf√©rence sur les poids fig√©s bien moindre que le temps n√©cessaire pour entra√Æner ces poids)

<u>Distillation</u>

* utilis√©e essentiellement pour la compression des mod√®les
* transfert des connaissances acquises par un
  grand mod√®le vers un petit mod√®le.
*  Le grand mod√®le (enseignant) doit en quelque sorte
  enseigner le petit mod√®le (√©l√®ve) sans perte de validit√©
* . M√™me si les deux mod√®les sont
  entra√Æn√©s sur les m√™mes donn√©es, le petit mod√®le est incapable d‚Äôapprendre une repr√©sen-
  tation concise des connaissances.
*  Cependant certaines informations sur une repr√©sentation
  concise des connaissances sont cod√©es dans les pseudo-vraisemblances affect√©es √† la sortie
  du mod√®le. 
* Les pseudo vraisemblances (ou **soft-label** en anglais) peut √™tre vu comme le
  processus suivant : 
  * apr√®s qu‚Äôun mod√®le pr√©dit correctement une classe, il attribue une
    valeur √©lev√©e √† la variable de sortie correspondant √† cette classe et des valeurs plus petites
    aux autres variables de sortie.
  *  La distribution des valeurs dans le vecteur des pseudo-
    vraisemblances en sortie de mod√®le (enseignant) fournit des informations sur la fa√ßon
    dont ce grand mod√®le repr√©sente les connaissances. 
  * Par cons√©quent, l‚Äôobjectif de facili-
    ter l‚Äôapprentissage du petit mod√®le (√©l√®ve) peut √™tre atteint en entra√Ænant uniquement le
    grand mod√®le sur les donn√©es o√π les √©tiquettes sont repr√©sent√©es par un vecteur ¬´one-hot
    encoding¬ª (hard-label) c‚Äôest-√†-dire en exploitant sa meilleure capacit√© √† apprendre des
    repr√©sentations de connaissances concises, puis en distillant ces connaissances dans un
    mod√®le plus petit, qui sera donc en mesure d‚Äôapprendre sur les pseudo-vraisemblances du
    grand mod√®le (soft-label)



##### Repr√©sentation des entr√©es

Le texte doit donc √™tre repr√©sent√© par
des vecteurs r√©els avant d‚Äô√™tre trait√© par le transformateur.

op√©ration r√©alis√©e via un plongement contextuel (**contextual embedding**)

Le **plongement lexical** classique (word2vec, GloVe) est en g√©n√©ral bas√© sur la
co-occurrence statistique des mots qui sont projet√©s **ind√©pendamment de leur contexte**.

* les relations s√©mantiques dans la phrase ne contribuent pas au plongement.

le **plongement contextuel** projette les mots **selon leur contexte** dans la phrase
et fournit en plus une repr√©sentation logique √† l‚Äô√©chelle de la phrase

BERT: plongement en plusieurs √©tapes

1. pr√©traitement des textes en ins√©rant des **symboles sp√©ciaux** dans le texte brut, pour indiquer au transformateur certaines informations sur les mots et sur la compo-
   sition du texte en entr√©e.

2. les mots sont d√©compos√©s en sous-mots (**token**) pr√©sents
   dans le vocabulaire et ces derniers seront repr√©sent√©s par leur identifiant dans le vocabu-
   laire
3.  Puis chaque token passera dans la couche du plongement lexical pour obtenir son
   **vecteur de repr√©sentation** (**token embedding**).

BERT poss√®de en plus 2 autres types de plongement : 

1. plongement de position (**position embedding**) = porte l‚Äôinformation structurelle de la phrase 
2. plongement de segmentation (**segmentation embedding**) = porte l‚Äôinformation de positionnement des phrases dans l‚Äôentr√©e

 La repr√©sentation finale de chaque sous-mot en
entr√©e est la **somme des trois vecteurs de repr√©sentation** qui lui sont associ√©s



##### Pr√©traitement

Des informations sont indiqu√©es au transformateur par le biais des **sous-mots sp√©ciaux**
ins√©r√©s dans le texte. Les principaux :

* [CLS] : indique le d√©but de la phrase
* [SEP] : indique la s√©paration entre les phrases ou la fin de la phrase.
* [PAD] : utilis√© pour compl√©ter les dimensions vides d‚Äôun vecteur dont la dimension
  est inf√©rieur √† celle du plongement (padding).
* [MASK] : utilis√© pour masquer des mots et pour indiquer au transformateur les
  mots √† pr√©dire dans la t√¢che de pr√©diction des mots masqu√©s.

strat√©gies d‚Äôindication des entit√©s nomm√©es au transformateur :

* **L‚Äôencapsulation** : les entit√©s nomm√©es sont entour√©es par des symboles sp√©ciaux pour
  indiquer le d√©but et la fin de chaque entit√©
  * avantage de bien repr√©senter les entit√©s imbriqu√©es ou celles qui se chevauchent. 
  * le mod√®le peut apprendre non seulement le contexte de la phrase mais aussi le contexte dans les
    sous-phrases qui forment les entit√©s.
  * tr√®s utile dans les approches qui prennent en consid√©ration les entit√©s
    nomm√©es en sortie
* l'**anonymysation**: remplacer les entit√©s nomm√©es par des
  sous-mots sp√©ciaux
  * permet
    au mod√®le d‚Äôidentifier les sous-mots qui appartiennent aux entit√©s nomm√©es sans conna√Ætre
    les entit√©s nomm√©es, 
  * assurent que le mod√®le a utilis√© uniquement le
    contexte dans sa d√©cision, ce qui permet d‚Äô√©viter l‚Äôapprentissage des co-occurrences qui
    pourrait √™tre une forme de sur-apprentissage.

##### Tok√©nisation

processus de partition des mots en sous-mots qui appartiennent au vo-
cabulaire de sorte que le mot sera remplac√© par le minimum de sous-mots (tokens).

##### La segmentation

Le r√¥le de **plongement de segmentation** consiste √† d√©signer les appartenances des
sous-mots aux phrases, par cons√©quent √† traiter des t√¢ches multi-entr√©es. 

La pr√©diction
de s√©quencement des phrases est une illustration d‚Äôune t√¢che de classification avec deux
phrases en entr√©es, ces derni√®res √©tant pr√©sent√©es par un vecteur de segmentation com-
pos√© d‚Äôune s√©quence de 0 (sous-mots de la premi√®re phrase) suivi par une s√©quence de 1
(sous-mots de la deuxi√®me)

##### Plongement de position

L‚Äôordre et la position des mots dans la phrase est une information utile pour la com-
pr√©hension, car la conception de la phrase dans les langages naturels respecte un ordre
d√©fini (une phrase n‚Äôest pas un sac de mots).

 Les CNN ou RNN exploitent bien les infor-
mations locales hi√©rarchiques et donc tiennent compte de cette information.

couche d‚Äôattention dans le transformateur: aucune r√®gle explicite
sur l‚Äôordre des √©l√©ments n‚Äôest impos√©e, ce qui n√©cessite de coder cette information dans
les entr√©es avant de les passer au transformateur ou bien d‚Äôintroduire cette information
dans le m√©canisme d‚Äôattention afin de l‚Äôexploiter. 

Ce plongement de position peut √™tre
**fix√© ou appris**.

##### Plongement contextuel

repr√©sentation des entr√©es par la **combinaison de**
**trois types diff√©rents de plongement**, 

Les trois types de plongement utilisent tous une couche linaire en sortie qui permet d‚Äôavoir trois vecteurs de plongement de la m√™me dimension (n,768) avec n qui d√©finit le nombre
de sous-mots en entr√©e. 

Une fois les trois plongements calcul√©s, le plongement
contextuel √©quivaut √† la somme des trois plongements

##### L‚Äôarchitecture de BERT

premi√®re architecture d‚Äôencodeur qui capture le contexte pass√© et futur
simultan√©ment

* OpenAI GPT capture uniquement le contexte pass√© 
* Elmo utilise une concat√©nation des contextes pass√© et future captur√©s ind√©pendamment.

**L‚Äôencodeur** est compos√© d‚Äôune pile de 12 blocs identiques. 

chaque bloc est compos√© de 2 sous-couches

1. m√©canisme d‚Äôauto-attention √† plusieurs t√™tes 
2. simple r√©seau de feed-forward enti√®rement connect√© (FFN). 

Des connexions
r√©siduelles sont utilis√©es autour de chacune des deux sous-couches

puis une normalisation appliqu√©e apr√®s chaque sous-couche.

Le **d√©codeur** est √©galement compos√© d‚Äôune pile de 12 (BERT base) blocs identiques. 

* en plus des deux sous-couches d‚Äôencodeur, 3√®me sous-couche, qui
  effectue une attention multi-t√™tes sur la sortie d‚Äôencodeur
  * les vecteurs cl√©s et valeurs viennent de la sortie de l‚Äôencodeur
  * la requ√™te
    est calcul√©e √† base d‚Äôattention sur les √©tiquettes dans le d√©codeur. 
* Les sous-couches de
  d√©codeur suivent la m√™me proc√©dure que celle du codeur

*Attention*
Ce processus it√©ratif √† travers les blocs aidera le r√©seau neuronal √† capturer des rela-
tions plus complexes entre les mots dans la s√©quence d‚Äôentr√©e. 

Les **sous-couches d‚Äôattention** utilis√©es dans BERT correspondent √† l‚Äôauto-attention √† plusieurs t√™tes d√©taill√©e

12 t√™tes d‚Äôattention -> peut apprendre jusqu‚Äô√† 12 types de relation entre les sous-mots dans chaque bloc

L‚Äôind√©pendance calculatoire √©lev√©e, un autre avantage, le calcul de l‚Äôattention selon chaque t√™te se fait ind√©pendamment
des autres t√™tes

*Feed-Forward*

sous-couche **Feed-Forward** compos√©es de 2 couches lin√©aires enti√®rement connect√©es avec une fonction d‚Äôactivation RELU entres ces deux couches. 

* permet de calculer les caract√©ristiques hi√©rarchiques non lin√©aires. 
* Pendant cette √©tape, les vecteurs de repr√©sentation des sous-mots n‚Äôinteragissent pas les uns avec les autres

*Liens r√©siduels*

Dans l‚Äôencodeur et le d√©codeur, 

* avant la normalisation des couches, **connexion r√©siduelle** (ou saut√©e) autour de chacune des 2 sous-couches
  * utilis√©es pour permettre aux gradients de circuler directement dans le
    r√©seau, sans passer par des fonctions d‚Äôactivation non lin√©aires
  * par leur nature non lin√©aire ces derni√®res font exploser ou dispara√Ætre les gradients. 
  * forment conceptuellement un bus qui circule tout au long du r√©seau, cela permet de r√©ali-
    ser un apprentissage sur une architecture assez profonde, telle que BERT, d‚Äôune mani√®re douce
  * les liens r√©siduels d√©forment la topologie de la fonction de perte, en appliquant une sorte de lissage sur cette derni√®re

*Normalisation* 

* permet de r√©soudre le probl√®me de d√©calage interne des covariables
* concerne la variation des distributions
  au niveau des couches
* d√ª √† 
  * l‚Äôinitialisation des poids ou
  * l‚Äô√©chelle des caract√©ris-tiques en entr√©e ou 
  * la d√©rive de distribution dans les diff√©rentes profondeurs.
* se manifeste quand le r√©seau apprend et que les poids
  sont mis √† jour, de sorte que la distribution des sorties d‚Äôune couche sp√©cifique dans le r√©-
  seau change
* oblige les couches sup√©rieures √† s‚Äôadapter √† cette d√©rive, ce qui ralentit
  l‚Äôapprentissage et impacte les performances et la stabilit√© de r√©seau.

* diff√©rents types
  * **Normalisation par lot** (**batch normalisation**)
    * consiste √† calculer la moyenne et la variance de chaque mini-lot et de normaliser chaque
      caract√©ristique en fonction des statistiques du mini-lot
    * la moyenne et la variance seront diff√©rentes pour chaque min-lot
    * cette d√©pendance pose des probl√®mes en
      fonction de la taille des lots et de la variation entre les lots
  * **normalisation par couche**
    * de la m√™me fa√ßon que la normalisation par lot, sauf que les
      statistiques sont calcul√©es sur l‚Äôaxe de caract√©ristique et non pas sur l‚Äôaxe des exemples.
    * pour chaque couche une moyenne et une variance sont calcul√©es pour chaque exemple
      en entr√©e ind√©pendamment des autres
    *  ces deux mesures sont utilis√©es par la suite dans
      la normalisation de la couche en question.
    * L‚Äôind√©pendance entre les entr√©es est l‚Äôavantage
      de cette m√©thode : chaque entr√©e a un fonctionnement de normalisation diff√©rent, ce
      qui permet d‚Äôutiliser des mini-lots de taille arbitraire
    * BERT adopte la **normalisation par couche comme m√©canisme de normalisation** et le **d√©crochage**
      **comme m√©canisme de r√©gularisation**

Le d√©crochage et l‚Äôaddition

* d√©crochage (**Dropout**) = m√©thode de **r√©gularisation** qui permet de r√©duire
  le sur-apprentissage dans les r√©seaux de neurones. 
  * co-adaptation entre les neurones
    peut conduire √† un d√©s√©quilibre des poids
  * caus√© par le fait que le neurone
    s‚Äôappuie uniquement sur quelques caract√©ristiques en entr√©e entra√Ænant une √©l√©vation des
    poids associ√©s √† ces caract√©ristiques
  * forme des relations de co-adaptations fortes
  * co-adaptation est une forme de sur-apprentissage 
* d√©crochage = abandon de neurones choisis al√©atoirement dans une couche, ce
  qui fait que la couche suivante n‚Äôa plus qu‚Äôune information partielle (uniquement les sorties
  des neurones conserv√©s). 
* emp√™che donc les neurones de s‚Äôappuyer toujours
  sur les m√™mes caract√©ristiques -> meilleure capacit√© de g√©n√©ralisation.
* A chaque it√©ration le neurone sera abandonn√© avec une probabilit√© p. 
  * p √©lev√©:
    * le nombre de neurones d√©croch√©s est important, 
    * la co-adaptation est donc
      moindre 
    *  risque de sous-apprentissage est plus √©lev√©,
  * p trop faible
    * la co-adaptation augmente 
    * le sur-apprentissage augmente aussi
* BERT: 
  * d√©crochage appliqu√© apr√®s chaque sous-couche avec une
    probabilit√© de 0.1.
  *  vecteurs r√©sultant de ce d√©crochage additionn√©s √† l‚Äôentr√©e de
    cette sous-couche. 
  *  vecteur-somme normalis√© utilisant une normalisation
    par couche

##### Phase de pr√©-entra√Ænement

* permet au transformateur d‚Äôavoir une certaine compr√©hension g√©n√©rale
  sous forme d‚Äôune capacit√© de repr√©sentation
* capacit√© de repr√©sentation bidirec-
  tionnelle profonde induite d‚Äôun entra√Ænement √† des t√¢ches non supervis√©es (pr√©diction
  des mots masqu√©s, pr√©diction de la phrase suivante) sur de larges corpus (Wikipedia
  (2,5B words), BookCorpus (800M words)). 
* capacit√© de repr√©sentation transf√©-
  r√©e √† la phase en amont = apprentissage par transfert
  *  **transfert inductif du type multit√¢che** au niveau de la phase de pr√©-
    entra√Ænement
  *  **transfert transductif du type d‚Äôadaptation de domaine** entre la phase
    de pr√©-entra√Ænement et la phase d‚Äôentra√Ænement

*La t√¢che de pr√©diction des mots masqu√©s*

* Contrairement √† Elmo et √† OpenAI GPT, BERTpeut capturer le contexte
  pass√© et futur simultan√©ment
  * ce qui a emp√™ch√© ses pr√©d√©cesseurs de le faire, c‚Äôest le biais engendr√© par le partage d‚Äôinformations
    entre les deux contextes, chaque contexte (le contexte pass√© et le contexte
    futur) a √† travers l‚Äôautre contexte un acc√®s direct √† l‚Äôinformation qu‚Äôil cherche √† g√©n√©raliser.
  * introduit un biais qui emp√™che le mod√®le d‚Äôapprendre les deux contextes simultan√©-
    ment
* solution de BERT: entra√Æner le mod√®le sur la g√©n√©ralisation du
  contexte bidirectionnel, sur une t√¢che o√π l‚Äôinformation que le mod√®le cherche √† pr√©dire
  est absente dans les deux contextes
  * le partage d‚Äôinformations entre contextes ne
    pose plus de probl√®me
  * masquage de mots al√©atoirement choisis
  * Le mod√®le va ensuite apprendre √† pr√©dire ces mots masqu√©s √† partir de leur contexte bidirectionnel.
  * permet d‚Äôacqu√©rir
    une bonne capacit√© de g√©n√©ralisation du contexte bidirectionnelle
  * t√¢che √©tiquet√©e automatiquement, permet d'entrainer le mod√®le sur une quantit√© beaucoup plus large de donn√©es non supervis√©es, donc facilement disponibles.

**Le but principal de la t√¢che de pr√©diction des mots masqu√©s n‚Äôest pas d‚Äôobtenir la**
**meilleure performance sur cette t√¢che, mais de transf√©rer une bonne g√©n√©ralisation du**
**contexte aux t√¢ches en aval o√π il n‚Äôy a pas de masquage de mots.** 

2 autres modes de remplacement propos√©s  pour aider le mod√®le √† s√©parer le vrai du faux selon le contexte bidirection-
nel

1. remplacement par un sous-mot al√©atoire 
2. ou par le
   mot original

* BERT:
  * masque 15% des sous-mots.
  *  80% des sous-mots masqu√©s seront
    remplac√©s par le sous-mot [MASK],
  * 10% par un sous-mot al√©atoire 
  * 10% conserveront
    le mot original.

 **La perte se d√©termine sur la fa√ßon dont BERT pr√©dit ou pas le mot man-**
**quant, et non sur l‚Äôerreur de reconstruction de la s√©quence enti√®re.**

*La t√¢che de pr√©diction de la phrase suivante*

compr√©hension de la relation entre deux phrases pas directement captur√©e par la mod√©lisation du langage utilisant la t√¢che de pr√©diction des mots
masqu√©s

pour former un mod√®le qui g√©n√©ralise la relation entre phrases, seconde t√¢che √©tiquet√©e automatiquement

**t√¢che de pr√©diction**
**de la phrase suivante**

*  t√¢che de classification binaire impliquant la pr√©diction pour
  dire si la deuxi√®me phrase succ√®de ou pas √† la premi√®re dans le corpus
* 2 s√©-
  quences en entr√©e s√©par√©es par le sous-mot [SEP] et les plongements de segmentation sont
  √©galement utilis√©s pour indiquer l‚Äôappartenance des sous-mots aux phrases. 
* Le corpus est
  construit d‚Äôune mani√®re √©quilibr√©e, dans 50% des cas, la phrase suivante est correctement
  utilis√©e comme phrase suivante, et dans 50% des cas, une phrase est tir√©e al√©atoirement
  du corpus
* garantit que le mod√®le s‚Äôadapte √† la g√©n√©ralisation des relations entre les
  s√©quences multiples

##### T√¢ches en aval

*Classification des s√©quences*

* mod√®le pr√©-entra√Æn√© entra√Æn√© sur un ensemble de donn√©es supervis√©es pour
  pr√©dire la classe d‚Äôune s√©quence donn√©e

* sous-mot de classification ([CLS]) utilis√©
  * sortie de ce sous-mot consid√©r√©e comme la sortie group√©e du classificateur et 
  * ensuite plac√©e dans une couche de classification enti√®rement connect√©e pour obtenir
    la sortie √©tiquet√©e.

*Reconnaissance des entit√©s nomm√©es*

* vecteurs de repr√©sentation en sortie du transformateur directement plac√©s
  dans une couche de classification
* nombre d‚Äô√©tiquettes comme unit√©s de sortie pour chacun des sous-mots
* softmax appliqu√©e sur les vecteurs en sortie de la couche de classification
* Les valeurs dans ces vecteurs interpr√©t√©es comme la
  probabilit√© de pr√©diction de chaque classe (types d‚Äôentit√©s nomm√©es)
  * utilis√©es pour obtenir la classe pr√©dite de chaque sous-mot en utilisant argmax.

*Inf√©rence en langage naturel*

* mod√®le pr√©-entra√Æn√© entra√Æn√© de la m√™me mani√®re que pour la t√¢che de pr√©dic-
  tion de la phrase suivante. 
* texte et hypoth√®se s√©par√©s √† l‚Äôaide du sous-mot [SEP]
  et identifi√©s √† l‚Äôaide du plongement de segmentation. 
* sous-mot [CLS] utilis√© pour
  obtenir le r√©sultat de la classification
* A partir d‚Äôune phrase, la t√¢che consiste √† choisir la suite la plus plausible parmi quatre
  choix. Donc il y 4 s√©quences d‚Äôentr√©e, chacune contenant la phrase originale et le choix correspondant concat√©n√© √† celle-ci. 
* vecteur en sortie associ√© au sous-mot [CLS] transmis √† une couche enti√®rement connect√©e pour obtenir les scores de chacun des choix
* scores ensuite normalis√©s √† l‚Äôaide d‚Äôune couche softmax

*R√©ponse aux questions*

* Un paragraphe est utilis√© comme une s√©quence 
* la question est utilis√©e comme une autre s√©quence.

2 cas de figure √† propos de cette t√¢che :

1. la r√©ponse se trouve dans le paragraphe

* la t√¢che
  consiste √† trouver le d√©but et la fin de la r√©ponse dans le paragraphe. 
* ntroduit un vecteur de d√©but S ‚àà RH et un vecteur de
  fin E ‚ààRH , soit H la taille des vecteurs en sortie du transformateur. 
* produit
  de chaque sous-mot Ti et du vecteur de d√©but S utilis√© pour obtenir la pro-
  babilit√© que le sous-mot i soit le d√©but de la r√©ponse
*  De m√™me, nous obtenons la
  probabilit√© du sous-mot de fin j. 
* score d‚Äôun intervalle candidat de la position i
  √† la position j est d√©fini comme S.Ti+ E.Tj, 
* l‚Äôintervalle de score maximum o√π
  j ‚â•i est utilis√© comme pr√©diction.

2. pas de r√©ponse courte √† la question pr√©sente dans le
   paragraphe

* utiliser les scores
  de probabilit√© pour le d√©but et la fin de la r√©ponse, calcul√©s avec le vecteur de
  repr√©sentation correspondant au [CLS] et les vecteurs de repr√©sentation des sous-
  mots. 
* scores (s_null) compar√©s avec le score maximum obtenu √† la
  meilleure plage de candidats (c‚Äôest-√†-dire le meilleur score pour le premier cas)
*  Si le
  score obtenu est sup√©rieur √† s_null d‚Äôun seuil suffisant œÑ, nous utilisons le meilleur
  score du candidat comme r√©ponse. Le seuil œÑ peut √™tre r√©gl√© pour maximiser le
  score F1 sur la base de d√©veloppement

##### Variante de BERT: Bio-BERT

reprend l‚Äô architecture de BERT et
l‚Äôentra√Æne sur des donn√©es biom√©dicales afin d‚Äôacqu√©rir une bonne capacit√© de repr√©senta-
tion dans le domaine biom√©dical.

Les donn√©es biom√©dicales utilis√©es sont essentiellement des ar-
ticles et des extraits d‚Äôarticles dans le domaine biom√©dical

deux larges corpus PubMed qui compte plus de 29 millions d‚Äôextraits d‚Äôarticles avec
environ 4.5 milliards de mots et MPC o√π les articles sont enti√®rement utilis√©s et contient
environ 13.5 milliards de mots.

Le **vocabulaire** utilis√© dans cette variante est le vocabulaire de BERT  sous l‚Äôhypoth√®se qu‚Äôil couvre le domaine biom√©dical.

plusieurs versions qui diff√®rent par les donn√©es utilis√©es et par le nombre
d‚Äôit√©rations utilis√© dans la phase de pr√©-entra√Ænement pour adapter BERT au domaine
biom√©dicale

Le **pr√©traitement** consiste √† anonymiser les entit√©s nomm√©es dans le texte par les sous-
mots (@GENE$) pour la premi√®re entit√© et par (@DISEASE$) pour la deuxi√®me.

Dans la phase de **pr√©-entra√Ænement**, les **poids du mod√®le sont initialis√©s par les poids de**
**BERT-base** avant d‚Äôentra√Æner ce mod√®le sur les t√¢ches non supervis√©es (pr√©diction de
la phrase suivante et pr√©diction des mots masqu√©s) en utilisant des donn√©es biom√©dicales
dans le but d‚Äôacqu√©rir une bonne g√©n√©ralisation de repr√©sentation des textes biom√©dicaux.

La strat√©gie de transfert utilis√©e dans BioBERT est le **fine-tuning**

* comme la r√©tro-propagation du gradient s‚Äôapplique √† tous les poids, on
  n‚Äôajoute qu‚Äôune seule couche pour les t√¢ches en aval, le r√©seau √©tant d√©j√† tr√®s profond.

* appliqu√© sur trois t√¢ches (la reconnaissance des entit√©s nomm√©es,
  l‚Äôextraction des relations et la r√©ponse aux questions) sur des corpus biom√©dicaux citant
  ChemProt pour l‚Äôextraction des relations.

  

Comme BERT, utilise la fonction cross-entropy comme une
fonction de perte, avec une optimisation r√©alise par l‚Äôalgorithme AdamWithDecay 
utilisant la technique de gestion de pas d‚Äôapprentissage adaptative appel√©e ¬´ linear war-
mup ¬ª

##### T-SNE

\- Dans la premi√®re √©tape : l‚Äôalgorithme construit une **distribution de probabilit√©s sur*
des paires d‚Äôobjets** de grande dimension de telle sorte que **les objets similaires se voient**
**attribuer une probabilit√© plus √©lev√©e** tandis que les points dissemblables se voient attribuer
une probabilit√© tr√®s faible.
\- Dans la deuxi√®me √©tape : l‚Äôalgorithme d√©finit une **distribution de probabilit√©s similaires**
**sur les vecteurs de dimension r√©duite**, en **minimisant la divergence de Kullback-Leibler**
**entre les deux distributions**. L‚Äôalgorithme utilise la distance euclidienne entre les objets
dans sa m√©trique de similarit√©.





###  [Tout ce que vous devez savoir sur google bert](https://www.hebergementwebs.com/nouvelles/tout-ce-que-vous-devez-savoir-sur-google-bert)

Le **bloc d'attention** multi-t√™tes peut √™tre consid√©r√© comme  une m√©thode sophistiqu√©e pour calculer, √† partir de l'int√©gration de  mots d'entr√©e, les valeurs d'auto-attention des mots, ce qui permet au  transformateur de **comprendre comment un mot est li√© √† tous les autres  mots**. 

contrairement au RNN, avec le r√©seau de transformateurs, nous ne suivons pas le m√©canisme de r√©currence.

Au lieu d'alimenter la phrase mot par mot, nous **alimentons tous les mots de la phrase parall√®lement au r√©seau**. Nourrir les mots en parall√®le permet de r√©duire le temps de formation et aide √©galement √† apprendre la d√©pendance √† long terme. 

l'ordre des mots pas conserv√©

encodage positionnel pour donner les informations sur l'ordre des mots au transformateur

la sortie (repr√©sentation du codeur) obtenue √† partir du codeur final (codeur le plus haut) sera la repr√©sentation de la phrase d'entr√©e donn√©e. 

BERT utilise le mod√®le de transformateur mais n'inclut que la partie encodeur

 L'encodeur du transformateur est de nature bidirectionnelle puisqu'il peut lire une phrase dans les deux sens. Ainsi, BERT est fondamentalement la repr√©sentation d'**encodeur bidirectionnel** obtenue √† partir du transformateur. 

nous alimentons cette phrase en tant qu'entr√©e dans l'encodeur du transformateur et obtenons la **repr√©sentation contextuelle (int√©gration) de chaque mot dans la phrase en tant que sortie**. 

Une fois que nous alimentons la phrase comme entr√©e de l'encodeur, l'encodeur comprend le contexte de chaque mot de la phrase en utilisant le m√©canisme d'attention multi-t√™tes (qui relie chaque mot de la phrase √† tous les mots de la phrase pour apprendre la relation et la signification contextuelle des mots) et renvoie la repr√©sentation contextuelle de chaque mot de la phrase comme sortie.

**nous alimentons la phrase en tant qu'entr√©e dans l'encodeur du transformateur et obtenons la repr√©sentation de chaque mot de la phrase en tant que sortie.**

 **avec le mod√®le BERT, pour une phrase donn√©e, on obtient la repr√©sentation contextuelle (embedding ) de chaque mot dans la phrase comme sortie**.

### [All You Need to know about BERT](https://www.analyticsvidhya.com/blog/2021/05/all-you-need-to-know-about-bert/)

. Language representations are of two types: 

(i) **Context-free** language  representation such as Glove and Word2vec where embeddings for each  token in the vocabulary are constant and it doesn‚Äôt depend on the  context of the word. 

(ii) **Contextual** language representation such as  Elmo and OpenAI-GPT where token representation depends on the context of the word where it is used.

With a contextual model, we get the representation of each token based  on the sentence in which that token is used resulting in a better  understanding of the language by the machine. The BERT model helps in  generating the contextual representation of each token. It is even able  to get the context of whole sentences, sentence pairs, or paragraphs.

BERT basically uses the concept of pre-training the model on a very  large dataset in an unsupervised manner for language modeling. 

After pre-training, the model can be fine-tuned on the task-specific supervised dataset to achieve good results

2 types of strategies can be applied namely 

1. features-based and  
   * e.g. Elmo 
   *  model architectures are task-specific. 
   * uses different models for  different tasks and uses a pre-trained model for language  representations.
2. fine-tuning.
   * e.g. BERT 
   * the final  model for any task is almost the same as BERT. 
   *  uses deep  bidirectional layers of transformers encoders for language understanding 

BERT‚Äôs model architecture is based on Transformers. It uses multilayer bidirectional transformer encoders for language representations. 

BERT uses the same model architecture for all the tasks be it NLI, classification, or Question-Answering with minimal change such as adding an output layer for classification.

##### **Input-Output Format**

The whole input to the BERT has to be given a single  sequence

special tokens [CLS] and [SEP] to understand input  properly

[SEP] token has to be inserted at the end of a single input

* when a task requires more than one input such as NLI and Q-A tasks,  [SEP] token helps the model to understand the end of one input and the  start of another input in the same sequence input. 

[CLS]  special  classification token 

* the last hidden state of BERT corresponding to  this token (h[CLS]) is used for classification tasks. 

BERT  uses **Wordpiece embeddings input for tokens**. 

Along with token embeddings, BERT uses **positional embeddings** and **segment embeddings** for each token.  

* Positional embeddings contain information about the position of tokens  in sequence. 
* Segment embeddings help when model input has sentence  pairs. 
  * Tokens of the first sentence will have a pre-defined embedding of 0 
  * tokens of the second sentence will have a pre-defined  embedding of 1 as segment embeddings

**Final Embeddings** used by model architecture are the sum of  token embedding, positional embedding as well as segment embedding. The  final embeddings are then fed into the deep bidirectional layers to get  output. The output of the BERT is the hidden state vector of pre-defined hidden size corresponding to each token in the input sequence. These  hidden states from the last layer of the BERT are then used for various  NLP tasks.

##### **Pre-training and Fine-tuning**

BERT was pre-trained on unsupervised Wikipedia and Bookcorpus  datasets using language modeling. Two tasks were performed. 

1. Masked Language Model (MLM)
   * 15% of the tokens from the sequence were masked and then correct tokens  were predicted at the final hidden state. 
2. Next Sentence Prediction
   * To capture the relationship  between sentence pairs given as input
   * 50% of the  data is labeled as isNext where sentence B of the input sequence is just the next sentence of sentence A from the dataset corpus. 
   * Another 50% of data is labeled as notNext where sentence B is not next to sentence A  but any random sentence from the corpus dataset. 
   * Output hidden state  corresponding to [CLS] token is used to predict the correct label and  compute loss. 

After pre-training, BERT can be **fine-tuned** on the specific task-based dataset.

##### How to use BERT

For the implementation of BERT for any task on our dataset,  pre-trained weights are available and we can easily use those  pre-trained weights to fine-tune the model on our own dataset. 

need to use the same tokenizer and tokens index  mapping using which model has been pre-trained. We can get the tokenizer using the code given below.

### [Understanding BERT with Hugging Face](https://becominghuman.ai/understanding-bert-with-hugging-face-e041c08f3431)

What is a Question Answering Task?

In this task, we are given a question and a paragraph in which the answer lies to our BERT Architecture and the objective is to determine the start and end span for the answer in the paragraph.

 we provide two inputs to the BERT architecture. The paragraph and the question are separated by the <SEP> token. 

 two vectors S and E (which will be learned during fine-tuning) both  having shapes (1x768). 

take a dot product of these vectors with  the second sentence‚Äôs output vectors from BERT, giving us some scores.  

Softmax over these scores to get probabilities. 

training objective is the sum of the log-likelihoods of the correct  start and end positions.



### [Question Answering with a fine-tuned BERT](https://towardsdatascience.com/question-answering-with-a-fine-tuned-bert-bc4dafd45626)

For tasks like text classification, we need to fine-tune BERT on our  dataset. But for question answering tasks, we can even use the already  trained model and get decent results even when our text is from a  completely different domains.

 **[CLS]** token stands for classification and is there to **represent  sentence-level classification** and is used when we are classifying. 

**[SEP]** isused to separate the two pieces of text.

**Segment embeddings** help BERT in differentiating a question from the  text. In practice, we use a vector of 0's if embeddings are from  sentence 1 else a vector of 1‚Äôs if embeddings are from sentence 2.  

**Position embeddings** help in specifying the position of words in the  sequence. A

BERT uses **wordpiece tokenization**. In BERT, rare words get broken down into subwords/pieces. Wordpiece  tokenization uses ## to delimit tokens that have been split. 

The idea behind using wordpiece tokenization is to reduce the size of the vocabulary which improves training performance. 

###  [How to Fine-Tune Sentence-BERT for Question Answering](https://www.capitalone.com/tech/machine-learning/how-to-finetune-sbert-for-question-matching/) 

#####  What is Sentence-BERT?

Sentence-BERT is a word embedding model. 

* models used to numerically represent language by transforming  phrases, words, or word pieces (parts of words) into vectors. 
* can be pre-trained on a large background corpus (dataset) and  then later updated with a smaller corpus that is catered towards a  specific domain or task (**fine-tuning**)

The best word embedding models are able to represent text meaning,  including context. 

BERT, from which Sentence-BERT is  derived, is one of these high-performing models.

**Sentence-BERT** has been optimized for faster similarity computation **on the individual sentence level**, 



### [Question Answering with a Fine-Tuned BERT](https://mccormickml.com/2020/03/10/question-answering-with-a-fine-tuned-BERT/)

SQuAD benchmark, Given a question, and *a passage of text containing the answer*, BERT needs to highlight the ‚Äúspan‚Äù of text corresponding to the correct answer.

##### BERT Input Format

To feed a QA task into BERT, we pack both the question and the reference text into the input.

The two pieces of text are separated by the special `[SEP]` token.

BERT also uses ‚ÄúSegment Embeddings‚Äù to differentiate the question  from the reference text. These are simply two embeddings (for segments  ‚ÄúA‚Äù and ‚ÄúB‚Äù) that BERT learned, and which it adds to the token  embeddings before feeding them into the input layer.

##### Start & End Token Classifiers

BERT needs to highlight a ‚Äúspan‚Äù of text containing the answer‚Äìthis  is represented as simply **predicting which token marks the start of the  answer, and which token marks the end.**

For every token in the text, we feed its final embedding into the  start token classifier. **The start token classifier only has a single set of weights which it applies to every word.**

After taking the **dot product** between the output embeddings and the  ‚Äòstart‚Äô weights, we apply the **softmax** activation to produce a  probability distribution over all of the words. Whichever word has the  highest probability of being the start token is the one that we pick.

We repeat this process for the end token‚Äìwe have a separate weight vector this.

### [How to Train A Question-Answering Machine Learning Model (BERT)](https://blog.paperspace.com/how-to-train-question-answering-machine-learning-models/)

 a completely new neural network architecture based on attention, specifically self-attention, called Transformer, has been the real game-changer in NLP

A **language model** is a probabilistic model that learns the probability of the occurrence of a sentence, or sequence of tokens, based on the examples of text it has seen during training. For

Traditionally **RNNs** were used to train such models due to the sequential structure of language, but they are **slow to train** (due to sequential processing of each token) and sometimes **difficult to converge** (due to vanishing/exploding gradients)

different variants of **Transformers**, with their ability to process tokens in **parallel** and impressive performance due to **self-attention** mechanism and **different pre-training objectives**, have made training large models (and sometimes really really large models), which understand natural language really well, possible

**BERT** has been trained using the Transformer Encoder architecture, with Masked Language Modelling (MLM) and the Next Sentence Prediction (NSP) pre-training objective.

BERT uses Transformer Encoder from the original Transformer paper

An **Encoder** has a **stack of encoder blocks** (where the output of one block is fed as the input to the next block), and each encoder block is composed of **two neural network layers**. 

1. **self-attention layer** (which is the magic operation that makes transformers so powerful)
2.  a simple **feed-forward layer**. 

After each layer, there is a **residual connection** and a **layer normalization** operation 

for each encoder layer, **the number (with a maximum limit of 512) of input vectors and output vectors is always the same**

before the first encoder layer, the **input vector for each token is obtained by adding token embedding, positional embedding, and segment embedding**. 

**These vectors are processed in parallel** inside each encoder layer using matrix multiplications, and the obtained output vectors are fed to the next encoder block

After being processed sequentially through N such blocks, the obtained output vectors start understanding natural language very well.

A **pre-training objective** is a task on which a model is trained before being fine-tuned for the end task. 

* **GPT** models are trained on a  Generative Pre-Training task i.e. generating the  next token given previous tokens, before being fine-tuned 
* **BERT uses MLM and NSP as its pre-training objectives**.
  * a few special tokens like CLS, SEP, and MASK to complete these objectives. 
  * each tokenized sample fed to BERT is **appended with a CLS token** **in the  beginning** and **the output vector of CLS from BERT is used for different  classification tasks.** 
  * **MLM objective**, a percentage of tokens are masked i.e. replaced with  special token MASK, and the model is asked to **predict the correct token  in place of MASK**. 
    * To accomplish this a masked language model head is  added **over the final encoder block, which calculates a probability**  distribution over the vocabulary only for the output vectors (output  from the final encoder block) of MASK tokens.
  * in **NSP**, the **2  sentences tokenized and the SEP token appended at their end are  concatenated** and fed to BERT. The **output vector of the CLS token is then used to calculate the probability** of whether the second sentence in the pair is the subsequent sentence in the original document. 
  * For both the  objectives, standard cross-entropy loss with AdamW optimizer is used to  train the weights.
  * powerful pre-training objectives in capturing the  semantics of the natural language in comparison to other pre-training  objectives
  * After being trained on such pre-training objectives, these models are  **fine-tuned** on special tasks like question answering, name entity  recognition, etc. 

**SQuAD** is a popular dataset for this task which contains many paragraphs  of text, different questions related to the paragraphs, their answers,  and the start index of answers in the paragraph.

SQuAD2.0 contains over 50,000 unanswerable questions that look similar to the answerable ones

to perform the QA task we add a new question-answering head on top of BERT, 

The purpose of this question-answering head is to **find the start token and end token** of an answer for a given paragraph

Everything that comes in between, including the start and end token, is considered an answer.	

**Inside the question answering head are two sets of weights**, one for the  start token and another for the end token, which have the same  dimensions as the output embeddings.

The output embeddings of all the tokens are fed to this head, and a dot  product is calculated between them and the set of weights for the start  and end token, separately.

the **dot product** between the  start token weight and output embeddings is taken, and the dot product  between the end token weight and output embeddings is also taken. 

Then a **softmax** activation is applied to produce a probability distribution  over all the tokens for the start and end token set (each set also  separately). T

he tokens with the maximum probability are chosen as the  start and end token, respectively. 

 it may so happen  that the end token could appear before the start token. In that case an  empty string is output as the predicted answer. 

In popular implementations, this head is implemented as a  feed-forward layer that takes the input of the same dimension as the  BERT output embeddings and returns a two-dimensional vector, which is  then fed to the softmax layer. 

The complete BERT SQuAD model is  finetuned using cross-entropy loss for the start and end tokens.

### [BERT for question answering (Part 1)](https://dida.do/blog/bert-for-question-answering-part-1)

the terms "encoder" and "decoder" are interpreted in a slightly different way compared to for example commonly used convolution neural networks: we **do not have the typical "encoding" in the sense of layers getting narrower and the typical "decoding" in the sense of layers getting wider** (like for example in an autoencoder network). 

**the decoder consumes model outputs of previous sequence components as an input**: this distinguishes both components of the network.

 **residual connections** **which skip the attention layers** and feed the output of a previous layer directly into an **addition layer** including a layer **normalization** .

Transformers make use of the so-called **attention mechanism**, . The idea of the attention mechanism is to **obtain every decoder output as a weighted combination of all the input tokens**. 

Prior to this approach, most NLP tasks based on **RNNs usually obtained an output from a single aggregated value of all the previous objects** in the input sequence. This is quite a big problem especially for long input sequences, since information at the end of the input sequences **compresses all prior sequence components, thereby potentially introducing a lot of noise**.

The idea is that the encoding-decoding process based on the **attention mechanism** now performs the particular task (such as translation) **in combination with an "alignment search"**, i.e. it additionally learns how each one of individual input sequence components is involved in the resulting output sequence instead of just going through a prescribed output order via "classical" end-to-end RNN decoding. ; in terms of the **weights** , which determine the influence of the i-th input component on the j-th output component

For the case of the transformer, **multiple attention layers** are stacked in order to obtain the encoder and decoder structures.

All layers contained in the model have the same size - note that this is also needed for the residual connections.

The raw BERT model can take **either a single sentence or two sentences as a token sequence input**, which makes BERT flexible for a variety of downstream tasks. 

The ability to process two sentences can for example be used for question/answer pairs. BERT comes with is own tokenization facility.

As an input representation, BERT uses **WordPiece embeddings**,. Given a vocabulary of ~30k word chunks, BERT breaks words up into components - resulting in a **tokenization**

**WordPiece is a language representation model on its own.** Given a desired vocabulary size, WordPiece tries to **find the optimal tokens** (= subwords, syllables, single characters etc.) **in order to describe a maximal amount of words in the text corpus**.

The important part here is that **WordPiece is trained separately from BERT and basically used as a black box to perform vectorization of input sequences in a fixed vocabulary.** This procedure will be **performed for every word** in the input after a general string sanitizing. 

The catch of the WordPiece idea is that BERT can represent a relatively large catalog of words with a vector of fixed dimension corresponding to the vocabulary of chunks.

 There are several ways to deal with out-of-vocabulary tokens. 

The **tokens generated by WordPiece are usually mapped to IDs in the corresponding vocabulary** to obtain a numerical representation of the input sequence. These IDs are typically just the **number of the index in the vocabulary list (however, it is also possible to use hash functions** on the tokens).

 It is actually fairly easy to perform a manual WordPiece tokenization by using the vocabulary from the vocabulary file of one of the pretrained BERT models and the tokenizer module from the official BERT repository.

The characters **##** indicate that the token is associated  with the previous token (for example by breaking up a single word into  tokens). 

In BERTs case, the **numerical representation** of the tokens is  just the **mapping to their indices** in the vocabulary:

In case **two sentences** are passed to BERT, they are separated by using a special **[SEP]** token. 

All inputs start with a **[CLS]** token, which indicates the beginning of a token sequences and is later  **used as a representation for classification** tasks

The **token embeddings will be compared to word embedding lookup tables** and become the so-called **token input embedding** 



Assuming that we have now obtained the **token embeddings**  (that is,  word embeddings generated from either a mapping of tokens to IDs or a one-hot encoded version), a learned **sentence embedding**  is added, depending on the sentence which the token belongs to.

BERT uses an embedding of the token position in the input sequence: the **positional embedding**; vectors of the same size as the other two  embeddings. needed since BERT itself does  not have an intrinsic sequential ordering like for example a recurrent  neural network; embedding rule in terms of a  vector consisting of **sinusoids** was proposed

Learning the positional embeddings has also been proposed.

By propagating this input representation through the full model we will then obtain the final hidden embedding

 During the **pretraining** phase, BERT performs two particular tasks:
1. **Masked Language Modeling**:

tokens of the input sequences are chosen at random (15% of the original input sequence tokens). 

Among these selected tokens, a **word replacement** routine with a [MASK] token

. In order to not introduce a model bias towards the mask token, a small percentage of the selected tokens are replaced with a randomly chosen token or remain unchanged. 

The hidden representation of the input tokens will then be used in combination with a softmax classifier in order to predict the selected tokens from BERTs vocabulary under a cross entropy risk.

2. **Next Sentence Prediction**:

This task is performed in order to **learn connections between sentences**. 

sentence pairs are formed. 

Whenever the pairs are subsequent in the original text, a IsNext label is attached to them. 

Whenever the sentence pairs are not subsequent in the original texts, a NotNext label is attached. 

The training dataset is generated of 50% pairs with IsNext label and 50% NotNext label. 

BERT now **predicts the two labels as a binary classifier based on the hidden layer embedding of the [CLS] token** of the input sequence



**Fine-tuning** of BERT is always **associated with a  particular practical task** such as for example classification. 

The  **pretraining version of BERT (that is, the weights obtained from the  Masked Language Modeling and Next Sentence Prediction training routines  outlined above) are used as starting weights** for a supervised learning  phase.

Depending on the specific task, various components of BERTs input can be used. 

For a text sequence classification task, the  representation of the `[CLS]` token will be used. 

For tasks  involving two sentence inputs such as paraphrasing and question/answer  problems, we make use of the sentence A/B mechanism

Usually, **additional neurons/layers are added to the output layer** of BERT: in the classification case this could for example be a softmax output. 

 Typically, the fine-tuning phase is a **much faster** procedure than the  pretraining phase, since the transfer from Masked Language Modeling and  Next Sentence Classification to the particular fine-tuning task allows  to **start from a near-converged state.**



### [BERT Explained: State of the art language model for NLP](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)

BERT‚Äôs key technical innovation is **applying the bidirectional training of Transformer, a popular attention model, to language modelling**

a novel technique named Masked LM (MLM) which allows bidirectional training in models in which it was previously impossible.

In its vanilla form, Transformer includes two separate mechanisms ‚Äî an **encoder** that reads the text input and a **decoder** that produces a prediction for the task

**Since BERT‚Äôs goal is to generate a language model, only the encoder mechanism is necessary.** 

As opposed to directional models, which read the text input sequentially (left-to-right or right-to-left), **the Transformer encoder reads the entire sequence of words at once**. Therefore it is considered **bidirectional**, though it would be more accurate to say that it‚Äôs non-directional. This characteristic **allows the model to learn the context** of a word based on all of its surroundings (left and right of the word).

When training language models, there is a challenge of defining a prediction goal. Many models predict the next word in a sequence (e.g. ‚ÄúThe child came home from ___‚Äù), a directional approach which inherently limits context learning. To overcome this challenge, BERT uses two training strategies:

1. **Masked LM (MLM)**

Before feeding word sequences into BERT, 15% of the words in each sequence are replaced with a **[MASK]** token. 

The model then attempts to predict the original value of the masked words, based on the context provided by the other, non-masked, words in the sequence. 

the prediction of the output words requires:

1. Adding a **classification layer** on top of the encoder output.
2. **Multiplying the output vectors by the embedding matrix**, transforming them into the vocabulary dimension.
3. Calculating the probability of each word in the vocabulary with **softmax**.

The **BERT loss function takes into consideration only the prediction of the masked values** and ignores the prediction of the non-masked words. As a consequence, the model **converges slower** than directional models, a characteristic which is offset by its increased context awareness 

In practice, the BERT implementation is slightly more elaborate and doesn‚Äôt replace all of the 15% masked words

Training the language model in BERT is done by predicting 15% of the tokens in the input, that were randomly picked. These tokens are pre-processed as follows ‚Äî 80% are replaced with a ‚Äú[MASK]‚Äù token, 10% with a random word, and 10% use the original word. Intuition:

* If we used [MASK] 100% of the time the model wouldn‚Äôt necessarily produce good token representations for non-masked words. The non-masked tokens were still used for context, but the model was optimized for predicting masked words.
* If we used [MASK] 90% of the time and random words 10% of the time, this would teach the model that the observed word is never correct.
* If we used [MASK] 90% of the time and kept the same word 10% of the time, then the model could just trivially copy the non-contextual embedding.

2. **Next Sentence Prediction (NSP)**

the model receives pairs of sentences as input and learns to predict if the second sentence in the pair is the subsequent sentence in the original document. 

During training, 50% of the inputs are a pair in which the second sentence is the subsequent sentence in the original document, while in the other 50% a random sentence from the corpus is chosen as the second sentence. 

the input is processed in the following way before entering the model:

1. A **[CLS]** token is inserted at the beginning of the first sentence and a **[SEP]** token is inserted at the end of each sentence.
2. A **sentence embedding** indicating Sentence A or Sentence B is added to each token. Sentence embeddings are similar in concept to token embeddings with a vocabulary of 2.
3. A **positional embedding** is added to each token to indicate its position in the sequence. The concept and implementation of positional embedding are presented in the Transformer paper.

To predict if the second sentence is indeed connected to the first, the following steps are performed:

1. The **entire input sequence goes through the Transformer** model.
2. The **output of the [CLS] token** is transformed into a 2√ó1 shaped vector,  using a simple classification layer (learned matrices of weights and  biases).
3. Calculating the probability of IsNextSequence with **softmax**.

When training the BERT model, **Masked LM and Next Sentence Prediction are  trained together,** with the goal of minimizing the combined loss function of the two strategies.



BERT can be used for a wide variety of language tasks, while only **adding a small layer to the core model**:

1. **Classification** tasks such as sentiment analysis are done similarly to Next Sentence  classification, by adding a classification layer on top of the  Transformer output for the [CLS] token.
2. In **Question Answering** tasks (e.g. SQuAD v1.1), the software receives a  question regarding a text sequence and is required to mark the answer in the sequence. Using BERT, a Q&A model can be trained by **learning  two extra vectors that mark the beginning and the end of the answer.**
3. In **Named Entity Recognition** (NER), the software receives a text sequence  and is required to mark the various types of entities (Person,  Organization, Date, etc) that appear in the text. Using BERT, a NER  model can be trained by feeding the output vector of each token into a  **classification layer** that predicts the NER label.

In the fine-tuning training, most hyper-parameters stay the same as in  BERT training, and the paper gives specific guidance (Section 3.5) on  the hyper-parameters that require tuning. 

BERT‚Äôs **bidirectional approach (MLM) converges slower** than left-to-right approaches (because only 15% of words are predicted in each batch) but bidirectional training still outperforms left-to-right training after a small number of pre-training steps.

